\documentclass[12pt,a4paper]{article}

% Package to include code
\usepackage{listings}
\usepackage{color}
\usepackage{varioref}
\lstset{language=Python}
\lstset{numbers=none, basicstyle=\footnotesize,
  numberstyle=\tiny,keywordstyle=\color{blue},stringstyle=\ttfamily,showstringspaces=false}
\lstset{backgroundcolor=\color[rgb]{0.95 0.95 0.95}}
\lstdefinestyle{numbers}{numbers=left, stepnumber=1, numberstyle=\tiny, numbersep=10pt}
\lstdefinestyle{nonumbers}{numbers=none}


% Font selection: uncomment the next line to use the ``beton'' font
%\usepackage{beton}

% Font selection: uncomment the next line to use the ``times'' font
%\usepackage{times}

% Font for equations
\usepackage{euler}


%Package to define the headers and footers of the pages
\usepackage{fancyhdr}


%Package to include an index
\usepackage{index}

%Package to display boxes around texts. Used especially for the internal notes.
\usepackage{framed}

%PSTricks is a collection of PostScript-based TEX macros that is compatible
% with most TEX macro packages
\usepackage{pstricks}
\usepackage{pst-node}
\usepackage{pst-plot}
\usepackage{pst-tree}

%Package to display boxes around a minipage. Used especially to
%describe the biography of people.
\usepackage{boxedminipage}

%Package to include postscript figures
\usepackage{epsfig}

%Package for the bibliography
% \cite{XXX} produces Ben-Akiva et. al., 2010
% \citeasnoun{XXX} produces Ben-Akiva et al. (2010)
% \citeasnoun*{XXX} produces Ben-Akiva, Bierlaire, Bolduc and Walker (2010)
\usepackage[dcucite,abbr]{harvard}
\harvardparenthesis{none}\harvardyearparenthesis{round}

%Packages for advanced mathematics typesetting
\usepackage{amsmath,amsfonts,amssymb}

%Package to display trees easily
%\usepackage{xyling}

%Package to include smart references (on the next page, on the
%previous page, etc.) 
%%

%% Remove as it is not working when the book will be procesed by the
%% publisher.
%\usepackage{varioref}

%Package to display the euro sign
\usepackage[right,official]{eurosym}

%Rotate material, especially large table (defines sidewaystable)
\usepackage[figuresright]{rotating}

%Defines the subfigure environment, to obtain refs like Figure 1(a)
%and Figure 1(b). 
\usepackage{subfigure}

%Package for appendices. Allows subappendices, in particular
\usepackage{appendix}

%Package controling the fonts for the captions
\usepackage[font={small,sf}]{caption}

%Defines new types of columns for tabular ewnvironment
\usepackage{dcolumn}
\newcolumntype{d}{D{.}{.}{-1}}
\newcolumntype{P}[1]{>{#1\hspace{0pt}\arraybackslash}}
\newcolumntype{.}{D{.}{.}{9.3}}

%Allows multi-row cells in tables
\usepackage{multirow}

%Tables spaning more than one page
\usepackage{longtable}


%%
%%  Macros by Michel
%%

%Internal notes
\newcommand{\note}[1]{
\begin{framed}{}%
\textbf{\underline{Internal note}:} #1
\end{framed}}

%Use this version to turn off the notes
%\newcommand{\note}[1]{}


%Include a postscript figure . Note that the label is prefixed with
%``fig:''. Remember it when you refer to it.  
%Three arguments:
% #1 label
% #2 file (without extension)
% #3 Caption
\newcommand{\afigure}[3]{%
\begin{figure}[!tbp]%
\begin{center}%
\epsfig{figure=#2,width=0.8\textwidth}%
\end{center}
\caption{\label{fig:#1} #3}%
\end{figure}}






%Include two postscript figures side by side. 
% #1 label of the first figure
% #2 file for the first figure
% #3 Caption for the first figure
% #4 label of the second figure
% #5 file for the second figure
% #6 Caption for the first figure
% #7 Caption for the set of two figures
\newcommand{\twofigures}[7]{%
\begin{figure}[htb]%
\begin{center}%
\subfigure[\label{fig:#1}#3]{\epsfig{figure=#2,width=0.45\textwidth}}%
\hfill
\subfigure[\label{fig:#4}#6]{\epsfig{figure=#5,width=0.45\textwidth}}%
\end{center}
\caption{#7}%
\end{figure}}

%Include a figure generated by gnuplot using the epslatex output. Note that the label is prefixed with
%``fig:''. Remember it when you refer to it.  
 
%Three arguments:
% #1 label
% #2 file (without extension)
% #3 Caption
\newcommand{\agnuplotfigure}[3]{%
\begin{figure}[!tbp]%
\begin{center}%
\input{#2}%
\end{center}
\caption{\label{fig:#1} #3}%
\end{figure}}

%Three arguments:
% #1 label
% #2 file (without extension)
% #3 Caption
\newcommand{\asidewaysgnuplotfigure}[3]{%
\begin{sidewaysfigure}[!tbp]%
\begin{center}%
\input{#2}%
\end{center}
\caption{\label{fig:#1} #3}%
\end{sidewaysfigure}}


%Include two postscript figures side by side. 
% #1 label of the first figure
% #2 file for the first figure
% #3 Caption for the first figure
% #4 label of the second figure
% #5 file for the second figure
% #6 Caption for the second figure
% #7 Caption for the set of two figures
% #8 label for the whole figure
\newcommand{\twognuplotfigures}[7]{%
\begin{figure}[htb]%
\begin{center}%
\subfigure[\label{fig:#1}#3]{\input{#2}}%
\hfill
\subfigure[\label{fig:#4}#6]{\input{#5}}%
\end{center}
\caption{#7}%
\end{figure}}



%Include the description of somebody. Four arguments:
% #1 label
% #2 Name
% #3 file (without extension)
% #4 description
\newcommand{\people}[4]{
\begin{figure}[tbf]
\begin{boxedminipage}{\textwidth}
\parbox{0.40\textwidth}{\epsfig{figure=#3,width = 0.39\textwidth}}%\hfill
\parbox{0.59\textwidth}{%
#4% 
}%
\end{boxedminipage}
\caption{\label{fig:#1} #2}
\end{figure}
}

%Default command for a definition
% #1 label (prefix def:)
% #2 concept to be defined
% #3 definition
\newtheorem{definition}{Definition}
\newcommand{\mydef}[3]{%
\begin{definition}%
\index{#2|textbf}%
\label{def:#1}%
\textbf{#2} \slshape #3\end{definition}}

%Reference to a definitoin. Prefix 'def:' is assumed
\newcommand{\refdef}[1]{definition~\ref{def:#1}}


%Default command for a theorem, with proof
% #1: label (prefix thm:)
% #2: name of the theorem
% #3: statement
% #4: proof
\newtheorem{theorem}{Theorem}
\newcommand{\mytheorem}[4]{%
\begin{theorem}%
\index{#2|textbf}%
\index{Theorems!#2}%
\label{thm:#1}%
\textbf{#2} \sffamily \slshape #3
\end{theorem} \bpr #4 \epr \par}


%Default command for a theorem, without proof
% #1: label (prefix thm:)
% #2: name of the theorem
% #3: statement
\newcommand{\mytheoremsp}[3]{%
\begin{theorem}%
\index{#2|textbf}%
\index{Theorems!#2}%
\label{thm:#1}%
\textbf{#2} \sffamily \slshape #3
\end{theorem}}



%Put parentheses around the reference, as standard for equations
\newcommand{\req}[1]{(\ref{#1})}

%Short cut to make a column vector in math environment (centered)
\newcommand{\cvect}[1]{\left( \begin{array}{c} #1 \end{array} \right) }

%Short cut to make a column vector in math environment (right justified)
\newcommand{\rvect}[1]{\left( \begin{array}{r} #1 \end{array} \right) }

%A reference to a theorem. Prefix thm: is assumed for the label.
\newcommand{\refthm}[1]{theorem~\ref{thm:#1}}

%Reference to a figure. Prefix fig: is assumed for the label.
\newcommand{\reffig}[1]{Figure~\ref{fig:#1}}

%Smart reference to a figure. Prefix fig: is assumed for the label.
%\newcommand{\vreffig}[1]{Figure~\vref{fig:#1}}

%C in mathcal font for the choice set
\newcommand{\C}{\mathcal{C}}

%R in bold font for the set of real numbers
\newcommand{\R}{\mathbb{R}}

%N in bold font for the set of natural numbers
\newcommand{\N}{\mathbb{N}}

%C in mathcal font for the log likelihood
\renewcommand{\L}{\mathcal{L}}

%S in mathcal font for the subset S
\renewcommand{\S}{\mathcal{S}}

%To write an half in math envionment
\newcommand{\half}{\frac{1}{2}}

%Probability
\newcommand{\prob}{\operatorname{Pr}}

%Expectation
\newcommand{\expect}{\operatorname{E}}

%Variance
\newcommand{\var}{\operatorname{Var}}

%Covariance
\newcommand{\cov}{\operatorname{Cov}}

%Correlation
\newcommand{\corr}{\operatorname{Corr}}

%Span
\newcommand{\myspan}{\operatorname{span}}

%plim
\newcommand{\plim}{\operatorname{plim}}

%Displays n in bold (for the normal distribution?)
\newcommand{\n}{{\bf n}}

%Includes footnote in a table environment. Warning: the footmark is
%always 1.
\newcommand{\tablefootnote}[1]{\begin{flushright}
\rule{5cm}{1pt}\\
\footnotemark[1]{\footnotesize #1}
\end{flushright}
}

%Defines the ``th'' as in ``19th'' to be a superscript
\renewcommand{\th}{\textsuperscript{th}}

%Begin and end of a proof
\newcommand{\bpr}{{\bf Proof.} \hspace{1 em}}
\newcommand{\epr}{$\Box$}


\title{Monte-Carlo integration with PandasBiogeme}
\author{Michel Bierlaire} 
\date{December 31, 2019}

\newcommand{\PBIOGEME}{PandasBiogeme}


\begin{document}


\begin{titlepage}
\pagestyle{empty}

\maketitle
\vspace{2cm}

\begin{center}
\small Report TRANSP-OR 191231 \\ Transport and Mobility Laboratory \\ School of Architecture, Civil and Environmental Engineering \\ Ecole Polytechnique F\'ed\'erale de Lausanne \\ \verb+transp-or.epfl.ch+
\begin{center}
\textsc{Series on Biogeme}
\end{center}
\end{center}


\clearpage
\end{titlepage}

Biogeme is an open source Python package designed for the maximum
likelihood estimation of parametric models in general, with a special
emphasis on discrete choice models. It relies on the package Python
Data Analysis Library called Pandas.

 In this document, we investigate some aspects related to Monte-Carlo
 integration, which is particularly useful when estimating mixtures
 choice models, as well as choice models with latent variables. We
 assume that the reader is already familiar with discrete choice
 models, with \PBIOGEME, and with simulation methods, although a short
 summary is provided.  This document has been written using
 \PBIOGEME\ 3.2.5.

\section{Monte-Carlo integration}

Monte-Carlo integration consists in approximating an integral with the
sum of a
large number of terms. It comes from the definition of the expectation of
a continuous random variable. Consider the random variable $X$ with
probability density function (pdf) $f_X(x)$. Assuming that $X$ can take any
value in the interval $[a,b]$, where $a \in \R \cup \{ -\infty\}$ and $b
\in \R \cup \{+\infty \}$, the expected value of $X$ is given by
\begin{equation}
\label{eq:expect}
\expect[X] = \int_{a}^{b} x f_X(x) dx.
\end{equation}
Also, if $g:\R \to \R$ is a function, then 
\begin{equation}
\label{eq:expectg}
\expect[g(X)] = \int_{a}^{b} g(x) f_X(x) dx.
\end{equation}
The expectation of a random variable can be approximated using
simulation. The idea is simple: generate a sample of realizations of
$X$, that is generate $R$ draws $x_r$, $r=1,\ldots,
R$ from $X$, and calculate the sample mean:
\begin{equation}
\label{eq:meanmc}
\expect[g(X)] \approx \frac{1}{R} \sum_{r=1}^R g(x_r).
\end{equation}
Putting \req{eq:expectg} and \req{eq:meanmc} together, we obtain an
approximation of the integral:
\begin{equation}
 \int_{a}^{b} g(x) f_X(x) dx \approx  \frac{1}{R} \sum_{r=1}^R g(x_r).
\end{equation}
Also, we have 
\begin{equation}
 \int_{a}^{b} g(x) f_X(x) dx = \lim_{R\to\infty}  \frac{1}{R} \sum_{r=1}^R g(x_r).
\end{equation}
Therefore, the procedure to calculate the following integral
\begin{equation}
I = \int_a^b g(x) dx
\end{equation}
is the following:
\begin{enumerate}
\item select a random variable $X$ defined on the interval $[a,b]$ such that you can generate 
realizations of $X$, and such that the pdf $f_X$ is known,
\item generate $R$ draws $x_r$, $r=1,\ldots,R$ from $X$,
\item calculate
\begin{equation}
I \approx \frac{1}{R} \sum_{r=1}^R \frac{g(x_r)}{f_X(x_r)}.
\end{equation}
\end{enumerate}
In order to obtain an estimate of the approximation error, we must
calculate the variance of the random variable.
The sample variance is an unbiased estimate of the true variance:
\begin{equation}
V_R = \frac{1}{R-1} \sum_{r=1}^R (\frac{g(x_r)}{f_X(x_r)} - I)^2.
\end{equation}
Alternatively as 
\begin{equation}
\var[g(X)] = \expect[g(X)^2] - \expect[g(x)]^2,
\end{equation}
the variance can be approximated by simulation as well:
\begin{equation}
\label{eq:simulatedVariance}
V_R \approx \frac{1}{R} \sum_{r=1}^R \frac{g(x_r)^2}{f_X(x_r)} -
I^2.
\end{equation}
Note that, for the values of $R$ that we are using in this document,
dividing by $R$ or by $R-1$ does not make much difference in practice. The approximation error is then estimated as
\begin{equation}
\label{eq:stderr}
e_R = \sqrt{\frac{V_r}{R}}.
\end{equation}
We refer the reader to \citeasnoun{Ross12} for a comprehensive
introduction to simulation methods. 

\section{Uniform draws}
\label{sec:uniform}
There are many algorithms  to draw from various
distributions. All of them require at some point draws from the
uniform distribution. There are several techniques that generate such
uniform draws. 

Each programming language provides a routine to
draw a random number between 0 and 1. Such routines are
deterministic, but the sequences of numbers that they generate share
many properties with sequences of random numbers. Therefore, they are
often called ``pseudo random numbers''.

Researchers have proposed to use other types of sequences to perform
Monte-Carlo integration, called ``quasi-random sequences'' or
``low-discrepancy sequences''. \PBIOGEME\ implements the Halton draws,
from \citeasnoun{Halt60}.  They have been reported to perform well for
discrete choice models (\cite{Trai2000}, \cite{Bhat2001},
\cite{Bhat2003837}, \cite{Sandor2004313}).

An Halton sequence is defined by a prime number $p$, say $p=3$, in the
following way:
\begin{itemize}
\item divide the interval [0,1] into $p$ parts to obtain  the first
  $p-1$ elements: 1/3 and 2/3, for our example;
\item consider each new interval, and divide it again into $p$ parts:
  [0,1/3] generates 1/9 and 2/9, [1/3,2/3] generates 4/9 and 5/9, and
  [2/3,1] generates 7/9 and 8/9;
\item divide again each new interval into $p$ parts, etc.
\end{itemize}

Note that \citeasnoun{Trai2000} suggests that the first 10 elements of the
sequence are discarded, since the early elements have a tendency to be
correlated over Halton sequences with different primes.

The third method to generate uniform random numbers implemented in
\PBIOGEME\ is called ``Modified Latin Hypercube Sampling'', and has been
proposed by \citeasnoun{HessTraiPola05}. The general idea is to
generate a randomly perturbed uniform grid on the unit interval.

In the following, we are using these three options, and compare the
accuracy of the corresponding Monte-Carlo integration.


\section{Illustration with \PBIOGEME}
\label{sec:illustration}
We first illustrate the method on a simple integral.
Consider
\begin{equation}
I = \int_0^1 e^x dx.
\end{equation}
In this case, it can be solved analytically:
\begin{equation}
I = e - 1 = 1.718281828459045.
\end{equation}
In order to use Monte-Carlo integration, we consider the random
variable $X$ that is uniformly distributed on $[0,1]$, so that
\begin{equation}
f_X(x) = \left\{
\begin{array}{ll}
1 & \text{if } x \in [0,1], \\
0 & \text{otherwise.}
\end{array}
\right.
\end{equation}
Therefore, we can approximate $I$ by generating $R$ draws from $X$ and
\begin{equation}
\label{eq:simpleMC}
I = \expect[e^X] \approx \frac{1}{R} \sum_{r=1}^R \frac{e^{x_r}}{f_X(x_r)} = \frac{1}{R} \sum_{r=1}^R e^{x_r}.
\end{equation}
Moreover, as
\begin{equation}
\begin{aligned}
\var[e^X] &= \expect[e^{2X}] - \expect[e^X]^2 \\
 &= \int_0^1 e^{2x}dx - (e-1)^2 \\
 &= (e^2-1)/2 - (e-1)^2 \\
 &= 0.2420356075,
\end{aligned}
\end{equation}
 the standard error  is  $0.0034787613$ for $R=20'000$, and
 $0.0011000809$ for
$R=200'000$. These theoretical values are estimated also below using \PBIOGEME. 

We use \PBIOGEME\ to calculate \req{eq:simpleMC}. Note that
\PBIOGEME\ requires a database, which is not necessary in this simplistic
case. We
use the simulation mode of \PBIOGEME. It generates output for each
row of the data file. In our case, we just need one output, so that we
take any database.
For this specific example, the data included in the database is
irrelevant.

\begin{lstlisting}
pandas = pd.DataFrame()
pandas['FakeColumn'] = [1.0]
database = db.Database("fakeDatabase",pandas)
\end{lstlisting}


The generation of draws in \PBIOGEME\ is performed using
the command \lstinline$bioDraws('U','UNIFORM')$, where the first argument,
\lstinline$'U'$, provides the name of the random variable associated
with the draws, and the second argument, \lstinline$'UNIFORM'$,
provides the  distribution of the random variable.
Note that the valid keywords for the type of draws are 
\begin{itemize}
 \item \lstinline$UNIFORM$: pseudo-random draws from a uniform distribution
 U[0,1],
 \item \lstinline$UNIFORM_ANTI$: antithetic pseudo-random draws from a uniform distribution U[0,1],
 \item \lstinline$UNIFORM_HALTON2$: Halton draws with base 2, skipping the first 10,
 \item \lstinline$UNIFORM_HALTON3$: Halton draws with base 3, skipping the first 10,
 \item \lstinline$UNIFORM_HALTON5$: Halton draws with base 5, skipping the first 10,
 \item \lstinline$UNIFORM_MLHS$: Modified Latin Hypercube Sampling on [0,1],
 \item \lstinline$UNIFORM_MLHS_ANTI$: Antithetic Modified Latin Hypercube Sampling on [0,1],
 \item \lstinline$UNIFORMSYM$: Uniform U[-1,1],
 \item \lstinline$UNIFORMSYM_ANTI$: Antithetic uniform U[-1,1],
 \item \lstinline$UNIFORMSYM_HALTON2$: Halton draws on [-1,1] with base 2, skipping the first 10,
 \item \lstinline$UNIFORMSYM_HALTON3$: Halton draws on [-1,1] with base 3, skipping the first 10,
 \item \lstinline$UNIFORMSYM_HALTON5$: Halton draws on [-1,1] with base 5, skipping the first 10,
 \item \lstinline$UNIFORMSYM_MLHS$: Modified Latin Hypercube Sampling on [-1,1],
 \item \lstinline$UNIFORMSYM_MLHS_ANTI$: Antithetic Modified Latin Hypercube Sampling on [-1,1],
 \item \lstinline$NORMAL$: Normal N(0,1) draws,
 \item \lstinline$NORMAL_ANTI$: Antithetic normal draws,
 \item \lstinline$NORMAL_HALTON2$: Normal draws from Halton base 2 sequence,
 \item \lstinline$NORMAL_HALTON3$: Normal draws from Halton base 3 sequence,
 \item \lstinline$NORMAL_HALTON5$: Normal draws from Halton base 5 sequence,
 \item \lstinline$NORMAL_MLHS$: Normal draws from Modified Latin Hypercube Sampling,
 \item \lstinline$NORMAL_MLHS_ANTI$: Antithetic normal draws from Modified
 Latin Hypercube Sampling
\end{itemize}

The integrand is defined by the following statement:
\begin{lstlisting}
integrand = exp(bioDraws('U','UNIFORM'))
\end{lstlisting}
and the Monte-Carlo integration is obtained as follows:
\begin{lstlisting}
simulatedI = MonteCarlo(integrand)
\end{lstlisting}
The number of draws is defined by the parameter \lstinline$NbrOfDraws$:
\begin{lstlisting}
R = 2000
biogeme = bio.BIOGEME(database,simulate,numberOfDraws=R)
\end{lstlisting}
We calculate as well the simulated variance, using
\req{eq:simulatedVariance}:
\begin{lstlisting}
sampleVariance = \
  MonteCarlo(integrand*integrand) - simulatedI * simulatedI
\end{lstlisting}
and the standard error \req{eq:stderr}:
\begin{lstlisting}
stderr = (sampleVariance / R)**0.5
\end{lstlisting}
Also, as we know the true value of the integral
\begin{lstlisting}
trueI = exp(1.0) - 1.0 
\end{lstlisting}
we can calculate the error:
\begin{lstlisting}
error = simulatedI - trueI
\end{lstlisting}
The calculation is obtained using the following statements:
\begin{lstlisting}
simulate = {'Analytical Integral': trueI,
            'Simulated Integral': simulatedI,
            'Sample variance   ': sampleVariance,
            'Std Error         ': stderr,
            'Error             ': error}

biogeme = bio.BIOGEME(database,simulate,numberOfDraws=R)
biogeme.modelName = f'01simpleIntegral_{R}'
results = biogeme.simulate()
\end{lstlisting}
We obtain the following results:
% 1.718282            1.            0.            0.           -0.
\begin{center}
\begin{tabular}{rr@{.}l}
Analytical Integral: & 1&718282, \\
Simulated Integral: & 1&695553, \\
Sample variance: & 0&230498, \\
Std Error: & 0&010735, \\
Error: &-0&022728.
\end{tabular}
\end{center}
Remember that the true variance is $0.2420356075$, and the true
standard error for 2'000 draws is
error is $0.011000809231598$.
If we use ten times more draws, that is 20'000 draws, we obtain a
more precise value:
%1.718282            1.721436            0.240849            0.010974            0.003155
\begin{center}
\begin{tabular}{rr@{.}l}
Analytical Integral: & 1&718282, \\
Simulated Integral: & 1&721436, \\
Sample variance: & 0&240849, \\
Std Error: & 0&010974, \\
Error: &0&003155.
\end{tabular}
\end{center}
Remember that the true variance is $0.2420356075$, and the true standard
error  for $R=20'000$  is $0.003478761327685$.
The complete specification file for \PBIOGEME\ is available in Appendix~\ref{sec:01simpleIntegral}.

We now illustrate the Monte-Carlo integration using Halton draws and
MLHS draws. By default, \PBIOGEME\ provides Halton sequences for bases
2, 3 and 5. But the user can also specify her own draws, by
implementing a Python function that takes two arguments:
the sample size, that is, the number of observations, and the number
of draws to generate per observation.  We illustrate
this with the Halton sequence for base 13. We implement a function
with the requested arguments, wrapping the Halton generation
function provided in the \lstinline$draws$ module.

\begin{lstlisting}
import biogeme.draws as draws
def halton13(sampleSize,numberOfDraws):
    return draws.getHaltonDraws(sampleSize,
                                numberOfDraws,
                                base=13,
                                skip=10)
\end{lstlisting}

This function needs next to be associated with a keyword, and a short
description.

\begin{lstlisting}
mydraws = {'HALTON13':(halton13,'Halton draws, base 13, skipping 10')}
\end{lstlisting}
And this must be communicated to the database.
\begin{lstlisting}
database.setRandomNumberGenerators(mydraws)
\end{lstlisting}

We recompute the integral using uniform draws, Halton draws in base 2,
Halton draws in base 13, and MLHS draws, using the following statements:
\begin{lstlisting}
integrand = exp(bioDraws('U','UNIFORM'))
integrand_halton = exp(bioDraws('U_halton','UNIFORM_HALTON2'))
integrand_halton13 = exp(bioDraws('U_halton13','HALTON13'))
integrand_mlhs = exp(bioDraws('U_mlhs','UNIFORM_MLHS'))
\end{lstlisting}

The results are reported in Table~\vref{tab:simple}.

\begin{table}[htb]
  \begin{center}
    \begin{tabular}{l|r@{.}lr@{.}lr@{.}lr@{.}l}
	&	\multicolumn{2}{l}{Uniform} &
      \multicolumn{2}{l}{Halton} &
      \multicolumn{2}{l}{Halton13} & \multicolumn{2}{l}{MLHS} \\
      \hline
Simulated &	1&71612 &	1&71815 &	1&718 &	1&71828 \\
Sample var. &	0&239978 &	0&241995 &	0&241998 &
0&242035 \\
Std error &	0&00346394 &	0&00347847 &	0&00347849 &
0&00347876 \\
Error	&	-0&00216237 &	-0&000132989 &	-0&000286219 &
9&41272e-08
    \end{tabular}
  \end{center}
  \caption{\label{tab:simple}Estimation with different types of draws,
  with 20'000 draws}
\end{table}

The complete specification file for \PBIOGEME\ is available in Appendix~\ref{sec:02simpleIntegral}.

\section{Variance reduction: antithetic draws}

There are several techniques to reduce the variance of the draws used
for the Monte-Carlo integration. Reducing the variance improves the
precision of the approximation for the same number of
draws. Equivalently, they allow to use less draws to achieve the same
precision. In this document, we introduce the concept of antithetic
draws. As the focus of this document is on
\PBIOGEME, we urge the reader to read an introduction to variance
reduction methods in simulation, for instance in \citeasnoun{Ross12}.

Instead of drawing from $X$, consider two random variables $X_1$ and $X_2$, identically distributed
with pdf $f_X=f_{X_1}=f_{X_2}$, and define a new random variable
\begin{equation}
Y = \frac{X_1+X_2}{2}.
\end{equation}
 Then, as
$\expect[Y]=\expect[X_1]=\expect[X_2]=\expect[X]$, we can rewrite \req{eq:expect}
as follows:
\begin{equation}
\expect[Y] = \frac{1}{2} \expect[X_1] +
\frac{1}{2} \expect[X_2] = \expect[X] = \int_{a}^{b} x f_X(x) dx.
\end{equation}
The variance of this quantity is 
\begin{equation}
\label{eq:varY}
\var[Y] = \frac{1}{4}(\var(X_1)+\var(X_2)+2\cov(X_1,X_2)).
\end{equation}
If $X_1$ and $X_2$  are independent, this variance is equal to
\begin{equation}
\var[Y] = \frac{1}{2}\var[X].
\end{equation}
Therefore, using $Y$ for Monte-Carlo integration is associated with a
variance divided by two, but requires twice more draws ($R$ draws for $X_1$
and $R$ draws for $X_2$). It has no advantage on drawing directly $R$
draws from $X$. Formally, we can compare the standard errors of the
two methods for the same number of draws. Drawing $2R$ draws from $X$,
we obtain the following standard error:
\begin{equation}
\sqrt{\frac{\var[X]}{2R}}.
\end{equation}
Drawing $R$ draws from $X_1$ and $R$ draws from $X_2$ to generate $R$
draws from $Y$, we obtain the
same standard error
\begin{equation}
\sqrt{\frac{\var[Y]}{R}} = \sqrt{\frac{\var[X]}{2R}}. 
\end{equation}

 However, if the variables $X_1$ and
$X_2$ happen to be negatively correlated, that is if $\cov(X_1,X_2) <
0$, then $\var[Y] < \var[X] / 2$, and drawing from $Y$ reduces the
standard error. For instance, if $X_1$ is uniformly
distributed on $[0,1]$, then $X_2=1-X_1$ is also uniformly
distributed on $[0,1]$, and 
\begin{equation}
\cov(X_1,X_2)=\expect[X_1(1-X_1)]-\expect[X_1]\expect[1-X_1] =
-\frac{1}{12} < 0.
\end{equation}
If $X_1$ has a standard normal distribution, that is such that
$\expect[X_1]=0$ and $\var[X_1]=1$, then $X_2=-X_1$ has also a
standard normal distribution, and is 
negatively
correlated with $X_1$, as
\begin{equation}
\cov(X_1,X_2) = \expect[-X_1^2] - \expect[X_1]\expect[-X_1] = -1 < 0.
\end{equation}
The other advantage of this method is that we can recycle the
draws. Once we have generated the draws $x_r$ from $X_1$, the draws from $X_2$
are obtained using $1-x_r$ and $-x_r$, respectively. 

Now, we have to be careful when this technique is used for the general
case \req{eq:expectg}. Indeed, it must be verified first that $g(X_1)$
and $g(X_2)$ are indeed negatively correlated. And it is not
guaranteed by the fact that $X_1$ and $X_2$ are negatively
correlated. Consider two examples.

First, consider $g(X)=\left(x-\frac{1}{2} \right)^2$. Applying the
antithetic method with 
\begin{equation}
X_1 = \left(X-\frac{1}{2}\right)^2 \text{ and } X_2 = \left((1-X)-\frac{1}{2}\right)^2 
\end{equation}
does not work, as 
\begin{equation}
\cov(X_1,X_2) = \frac{1}{180} > 0.
\end{equation}
Actually, applying the antithetic method would \emph{increase} the
variance here, which is not desirable.

Second, consider $g(X)=e^X$, as in the example presented in
Section~\ref{sec:illustration}. We apply the antithetic method using 
\begin{equation}
Y = \frac{e^{X}+e^{1-X}}{2}.
\end{equation}
Here, the two transformed random variables are negatively correlated:
\begin{equation}
\begin{aligned}
\cov(e^X,e^{1-X}) &= \expect[e^X e^{1-X}] -
\expect[e^X]\expect[e^{1-X}] \\
 &= e - (e-1)^2 \\
 &= -0.2342106136.
\end{aligned}
\end{equation}
Therefore, the variance of $Y$ given by \req{eq:varY} is
$0.0039124969$, as opposed to $0.2420356075 / 2 = 0.1210178037$ if the
two sets of draws were independent. It means that for 10'000 draws from
$Y$, the standard error decreases from $0.0034787613$ down to
$0.0006254996$. Moreover, as we use recycled draws, we need only 10'000
draws instead of 20'000. 

There are two ways to apply this technique in \PBIOGEME.
First, it is possible to explicitly write the integrand  as
follows:
\begin{lstlisting}
U = bioDraws('U','UNIFORM')
integrand = exp(U) + exp(1-U) 
simulatedI = MonteCarlo(integrand) / 2.0
\end{lstlisting}
and to divide the number of draws by two
\begin{lstlisting}
R = 10000
\end{lstlisting}

\begin{table}[htb]
  \begin{center}
    \begin{tabular}{lr@{.}lr@{.}lr@{.}l}
		& \multicolumn{2}{l}{Uniform (Anti)} &
      \multicolumn{2}{l}{Halton13 (Anti)} &
      \multicolumn{2}{l}{MLHS} 
      (Anti) \\
      \hline
Simulated	&1&71779 &	1&71829 &	1&71828 \\
Error	&	-0&000494215 &	1&02955e-05 &	1&9781e-07
    \end{tabular}
  \caption{Estimation with different types of draws,
  with 20'000 antithetic draws, using the explicit way}
  \end{center}
\end{table}

The complete specification file for \PBIOGEME\ is available in Appendix~\ref{sec:03antitheticExplicit}.
But the simplest way is to instruct \PBIOGEME\ to use antithetic draws

\begin{lstlisting}
integrand = exp(bioDraws('U','UNIFORM_ANTI'))
simulatedI = MonteCarlo(integrand)
\end{lstlisting}
In this case, if $R=20'000$ are requested, \PBIOGEME\ will actually
generate half of them, and complete them with their antithetic version.

\begin{table}[htb]
  \begin{center}
    \begin{tabular}{lr@{.}lr@{.}lr@{.}l}
		& \multicolumn{2}{l}{Uniform (Anti)} &
      \multicolumn{2}{l}{Halton13 (Anti)} &
      \multicolumn{2}{l}{MLHS} 
      (Anti) \\
      \hline
Simulated &	1&71894 &	1&71829 &	1&71828 \\
Error	&	0&000661971 &	1&02955e-05 &	1&27776e-07
    \end{tabular}
  \caption{Estimation with different types of draws,
  with 20'000 antithetic draws, using \PBIOGEME's draws}
  \end{center}
\end{table}

The complete specification file for \PBIOGEME\ is available in Appendix~\ref{sec:03antithetic}.


We encourage the reader to perform similar  tests for other simple
integrals. For instance,
\begin{equation}
\int_0^1 \left(x-\frac{1}{2} \right)^2 dx = \frac{1}{12}
\end{equation}
or
\begin{equation}
\int_{-2}^2 \left( e^{-x} + \frac{1}{2+\varepsilon-x}\right)dx= e^2 -
e^{-2} + \log \frac{4+\varepsilon}{\varepsilon},
\end{equation}
where $\varepsilon > 0$. Note that the domain of integration is not $[0,1]$.

\section{Mixtures of logit}

Consider an
individual $n$, a choice set $\C_n$, and an alternative $i\in\C_n$. The
probability to choose $i$ is given by the choice model:
\begin{equation}
P_n(i|x,\theta,\C_n), 
\end{equation}
where $x$ is a vector of explanatory variables and $\theta$ is a vector
of parameters to be estimated from data. In the random utility framework,
a utility function is defined for each individual $n$ and each
alternative $i\in\C_n$:
\begin{equation}
U_{in}(x,\theta)= V_{in}(x,\theta) + \varepsilon_{in}(\theta),
\end{equation}
where $V_{in}(x,\theta)$ is deterministic and $\varepsilon_{in}$ is a
random variable independent from $x$. The model is then written:
\begin{equation}
P_n(i|x,\theta,\C_n) = \prob(U_{in}(x,\theta) \geq U_{jn}(x,\theta), \forall j \in \C_n).
\end{equation}
Specific models are obtained from assumptions about the distribution
of $\varepsilon_{in}$. Namely, if $\varepsilon_{in}$ are
i.i.d. (across both $i$ and $n$) extreme value distributed, we obtain
the logit model: 
\begin{equation}
P_n(i|x,\theta,\C_n) = \frac{e^{V_{in}(x,\theta)}}{\sum_{j\in\C_n}e^{V_{jn}(x,\theta)}}.
\end{equation}
Mixtures of logit are obtained when some of the parameters $\theta$
are distributed instead of being fixed. Denote
$\theta=(\theta_f,\theta_d)$, where $\theta_f$ is the vector of fixed
parameters, while $\theta_d$ is the vector of distributed parameters,
so that the choice model, conditional on $\theta_d$, is 
\begin{equation}
P_n(i|x,\theta_f,\theta_d,\C_n).
\end{equation}
A distribution is to be assumed for $\theta_d$. We denote the pdf of
this distribution by $f_{\theta_d}(\xi;\gamma)$, where $\gamma$ contains the
parameters of the distribution. Parameters $\gamma$ are sometimes
called the \emph{deep parameters} of the model. Therefore, the choice model becomes:
\begin{equation}
\label{eq:mixedLogit}
P_n(i|x,\theta_f,\gamma,\C_n) = \int_\xi P_n(i|x,\theta_f,\xi,\C_n) f_{\theta_d}(\xi)d\xi,
\end{equation}
where $\theta_f$ and $\gamma$ must be estimated from data.
The above integral has no analytical expression, even when the kernel
$P_n(i|x,\theta_f,\xi,\C_n)$ is a logit model. Therefore, it must be
calculated with numerical integration or Monte-Carlo integration. We
do both here to investigate the precision of the variants of
Monte-Carlo integration.  

\subsection{Comparison of integration methods on one integral}
We
consider the Swissmetro example (\cite{BierAxhaAbay01}). The data file
is available from \verb+biogeme.epfl.ch+. At first, we keep only the first
observation, using the following statements:
\begin{lstlisting}
p = pd.read_csv("swissmetro.dat",sep='\t')
p = p.drop(p[p.index != 0].index)
database = db.Database("swissmetro",p)
\end{lstlisting}
Consider the
following specification: 
\begin{itemize}
\item Variables $x$: see variables in the data file and new variables
  defined in Section~\ref{sec:04normalMixtureNumerical}. 
\item Fixed parameters $\theta_f$
\begin{lstlisting}
ASC_CAR = 0.137
ASC_TRAIN = -0.402
ASC_SM = 0
B_COST = -1.29
\end{lstlisting}
\item Deep parameters $\gamma$:
\begin{lstlisting}
B_TIME = -2.26
B_TIME_S = 1.66
\end{lstlisting}
\item We define the coefficient of travel time to be distributed,
  using the random variable \lstinline$omega$, that is assumed to be
  normally distributed:
\begin{lstlisting}
B_TIME_RND = B_TIME + B_TIME_S * omega
\end{lstlisting}
The parameter \lstinline$B_TIME$ is the mean of \lstinline$B_TIME_RND$, and
\lstinline$B_TIME_S$$^2$ is its variance. Note that
\lstinline$B_TIME_S$ is just a parameter, and is \textbf{not} the
standard deviation. It can
be positive or negative. 
\item Utility functions $V_{in}$:
\begin{lstlisting}
V1 = ASC_TRAIN + \
     B_TIME_RND * TRAIN_TT_SCALED + \
     B_COST * TRAIN_COST_SCALED
V2 = ASC_SM + \
     B_TIME_RND * SM_TT_SCALED + \
     B_COST * SM_COST_SCALED
V3 = ASC_CAR + \
     B_TIME_RND * CAR_TT_SCALED + \
     B_COST * CAR_CO_SCALED
V = {1: V1, 2: V2, 3: V3}
\end{lstlisting}
\item Choice set $\C_n$, characterized by the availability conditions:
\begin{lstlisting}
CAR_AV_SP = \
  DefineVariable('CAR_AV_SP',CAR_AV  * (  SP   !=  0  ))
TRAIN_AV_SP = \
  DefineVariable('TRAIN_AV_SP',TRAIN_AV  * (  SP   !=  0  ))
av = {1: TRAIN_AV_SP,
      2: SM_AV,
      3: CAR_AV_SP}
\end{lstlisting}
\end{itemize}

As there is only one random parameter, the model \req{eq:mixedLogit}
can be calculated using numerical integration. It is done in
\PBIOGEME\ using the following procedure:
\begin{enumerate}
\item Mention that \lstinline$omega$ is a random variable:
\begin{lstlisting}
omega = RandomVariable('omega')
\end{lstlisting}
\item Define its pdf:
\begin{lstlisting}
import biogeme.distributions as dist
density = dist.normalpdf(omega).
\end{lstlisting}
\item Define the integrand from the logit model, where the probability
  of the alternative observed to be chosen is calculated (which is
  typical when calculating a likelihood function):
\begin{lstlisting}
import biogeme.models as models
integrand = models.logit(V,av,CHOICE)
\end{lstlisting}
\item Calculate the integral:
\begin{lstlisting}
analyticalI = Integrate(integrand*density,'omega')
\end{lstlisting}
\end{enumerate}

The complete specification file for \PBIOGEME\ is available in
Appendix~\ref{sec:04normalMixtureNumerical}. The value of the
choice model for first observation in the data file is 
\begin{equation}
I =  \int_\xi P_n(i|x,\theta_f,\xi,\C_n) f_{\theta_d}(\xi)d\xi = 0.6378498363459125.
\end{equation}
We then compare Monte-Carlo integration with various types of
draws. To do that, we implement a function that takes the parameter
$\beta$ as an argument, and returns the integrand (the logit model), using the
following syntax:
\begin{lstlisting}
def logit(THE_B_TIME_RND):
    V1 = ASC_TRAIN + \
         THE_B_TIME_RND * TRAIN_TT_SCALED + \
         B_COST * TRAIN_COST_SCALED
    V2 = ASC_SM + \
         THE_B_TIME_RND * SM_TT_SCALED + \
         B_COST * SM_COST_SCALED
    V3 = ASC_CAR + \
         THE_B_TIME_RND * CAR_TT_SCALED + \
         B_COST * CAR_CO_SCALED

    # Associate utility functions with the numbering of alternatives
    V = {1: V1,
         2: V2,
         3: V3}

    # Associate the availability conditions with the alternatives
    av = {1: TRAIN_AV_SP,
          2: SM_AV,
          3: CAR_AV_SP}

    # The choice model is a logit, with availability conditions
    integrand = models.logit(V,av,CHOICE)
    return integrand
\end{lstlisting}
Each version of the beta is specified with different types of draws,
as described earlier:
\begin{lstlisting}
B_TIME_RND_normal = B_TIME + B_TIME_S * \
                    bioDraws('B_NORMAL','NORMAL')
B_TIME_RND_anti = B_TIME + B_TIME_S * \
                  bioDraws('B_ANTI','NORMAL_ANTI')
B_TIME_RND_halton = B_TIME + B_TIME_S * \
                    bioDraws('B_HALTON','NORMAL_HALTON2')
B_TIME_RND_mlhs = B_TIME + B_TIME_S * bioDraws('B_MLHS','NORMAL_MLHS')
B_TIME_RND_antimlhs = B_TIME + B_TIME_S * \
                      bioDraws('B_ANTIMLHS','NORMAL_MLHS_ANTI')
\end{lstlisting}

The complete specification file for \PBIOGEME\ is available in
Appendix~\ref{sec:05normalMixtureMonteCarlo}. Using the result of the numerical integration as the ``true'' value of
the integral, We obtain the following results:
\begin{center}
\begin{tabular}{rr@{.}l}
Analytical integral & 0&63785 \\
Monte-Carlo integral & 0&637356 \\
Monte-Carlo integral, antithetic & 0&637549 \\
Monte-Carlo integral, Halton & 0&637908 \\
Monte-Carlo integral, MLHS & 0&637839 \\
Monte-Carlo integral, MLHS antithetic & 0&637861 \\
\end{tabular}
\end{center}

\subsection{Comparison of integration methods for maximum likelihood estimation}

We now estimate the parameters of the model using all observations
in the data set associated with work trips. Observations such that the dependent variable \lstinline$CHOICE$ is $0$ are also removed.
\begin{lstlisting}
exclude = (( PURPOSE != 1 ) * (  PURPOSE   !=  3  ) + \
          ( CHOICE == 0 )) > 0
database.remove(exclude)
\end{lstlisting}
The estimation using numerical integration is performed using the following statements:
\begin{lstlisting}
condprob = models.logit(V,av,CHOICE)
prob = Integrate(condprob * density,'omega')
logprob = log(prob)
biogeme = bio.BIOGEME(database,logprob)
biogeme.modelName = '06estimationIntegral'
results = biogeme.estimate()
\end{lstlisting}
The complete specification file for \PBIOGEME\ is available in
Appendix~\ref{sec:06estimationIntegral}.

\begin{table}[htb]
  \begin{center}
\begin{tabular}{ll}
Number of estimated parameters & 5 \\
Sample size & 6768 \\
Excluded observations & 3960 \\
Init log likelihood & -6879.406 \\
Final log likelihood & -5214.879 \\
Likelihood ratio test for the init. model & 3329.055 \\
Rho-square for the init. model & 0.242 \\
Rho-square-bar for the init. model & 0.241 \\
Akaike Information Criterion & 10439.76 \\
Bayesian Information Criterion & 10473.86 \\
Final gradient norm & 8.0436E-05 \\
Diagnostic & b'CONVERGENCE: NORM\_OF\_PROJECTED\_GRADIENT\_<=\_PGTOL' \\
Database readings & 16 \\
Iterations & 15 \\
Optimization time & 0:00:22.584674 \\
Nbr of threads & 8 \\
\end{tabular}
  \end{center}
  \caption{\label{sec:estimationIntegral}General statistics for the
    model estimation with analytical integral}
\end{table}

\begin{table}
  \begin{center}
\begin{tabular}{lrrrr}
{} &  Value &  Rob. Std err &  Rob. t-test &  Rob. p-value \\
\hline
ASC\_CAR   &  0.137 &        0.0517 &         2.65 &       0.00803 \\
ASC\_TRAIN & -0.401 &        0.0656 &        -6.12 &      9.64e-10 \\
B\_COST    &  -1.29 &        0.0863 &        -14.9 &           0.0 \\
B\_TIME    &  -2.26 &         0.117 &        -19.4 &           0.0 \\
B\_TIME\_S  &   1.65 &         0.125 &         13.3 &           0.0 \\
\end{tabular}
  \end{center}
  \caption{\label{sec:parametersIntegral}Parameter estimates for the
    model estimation with analytical integral}
\end{table}


For Monte-Carlo integration, we use the following statements:
\begin{lstlisting}
prob = models.logit(V,av,CHOICE)
logprob = log(MonteCarlo(prob))
R= 20000
biogeme = bio.BIOGEME(database,logprob,numberOfDraws=R)
\end{lstlisting}

The complete specification file for \PBIOGEME\ is available in
Appendix~\ref{sec:07estimationMonteCarlo}.

We now compare the estimation results for various types of draws: normal draws, antithetic,
Halton, MLHS and antithetic MLHS. We provide the results for 2'000 and 500 draws.
The final log likelihood in each case, as well as the estimation time
are summarized in Table~\ref{tab:estimSummary}.

It appears with that example that the use of pure random draws is the least precise way
to calculate Monte-Carlo integrals. The use of antithetic, Halton or
MLHS draws is therefore recommended.

\begin{table}[htb]
\begin{center}
\begin{tabular}{rrrrr}
Method & Draws &  Log likelihood & Run time \\
\hline
Numerical & --- & -5214.879 & 00:14 \\
Monte-Carlo  & 2000 & -5213.99 & 04:19 \\
Antithetic & 2000 & -5214.302 & 04:06 \\
Halton & 2000 & -5214.95 & 04:06 \\
MLHS & 2000 & -5214.637 & 04:14    \\
Antithetic MLHS  & 2000 & -5215.253 & 04:08    \\
Monte-Carlo  & 500 & -5219.581 & 00:57 \\
Antithetic & 500 & -5214.856 & 01:00 \\
Halton & 500 & -5215.069 & 01:01 \\
MLHS & 500 & -5215.013 & 01:01    \\
Antithetic MLHS & 500 & -5215.292 & 01:02    \\

\end{tabular}
\end{center}
\caption{\label{tab:estimSummary}Final log likelihood and run time for
each integration method}
\end{table}


\section{Conclusion}

This document describes the variants of Monte-Carlo integration.
It is recommended to perform some analysis using the simulation mode 
 of \PBIOGEME, in order to investigate the performance of
each type of draws  before starting a maximum likelihood  estimation, that may
take a while to converge. In the example provided in this document,
the antithetic draws method, combined with MLHS appeared to be the
most precise. This result is not universal. The analysis must be
performed on a case by case basis.

\clearpage

\appendix

\section{Complete specification files}

\subsection{\lstinline$01simpleIntegral.py$}
\label{sec:01simpleIntegral}
\lstinputlisting[style=numbers]{../../pandasbiogeme/biogeme_package/examples/montecarlo/01simpleIntegral.py}

\subsection{\lstinline$02simpleIntegral.py$}
\label{sec:02simpleIntegral}
\lstinputlisting[style=numbers]{../../pandasbiogeme/biogeme_package/examples/montecarlo/02simpleIntegral.py}

\subsection{\lstinline$03antitheticExplicit.py$}
\label{sec:03antitheticExplicit}
\lstinputlisting[style=numbers]{../../pandasbiogeme/biogeme_package/examples/montecarlo/03antitheticExplicit.py}

\subsection{\lstinline$03antithetic.py$}
\label{sec:03antithetic}
\lstinputlisting[style=numbers]{../../pandasbiogeme/biogeme_package/examples/montecarlo/03antithetic.py}

\subsection{\lstinline$04normalMixtureNumerical.py$}
\label{sec:04normalMixtureNumerical}
\lstinputlisting[style=numbers]{../../pandasbiogeme/biogeme_package/examples/montecarlo/04normalMixtureNumerical.py}

\subsection{\lstinline$05normalMixtureMonteCarlo.py$}
\label{sec:05normalMixtureMonteCarlo}
\lstinputlisting[style=numbers]{../../pandasbiogeme/biogeme_package/examples/montecarlo/05normalMixtureMonteCarlo.py}

\subsection{\lstinline$06estimationIntegral.py$}
\label{sec:06estimationIntegral}
\lstinputlisting[style=numbers]{../../pandasbiogeme/biogeme_package/examples/montecarlo/06estimationIntegral.py}

\subsection{\lstinline$07estimationMonteCarlo.py$}
\label{sec:07estimationMonteCarlo}
\lstinputlisting[style=numbers]{../../pandasbiogeme/biogeme_package/examples/montecarlo/07estimationMonteCarlo.py}




\clearpage 

\bibliographystyle{dcu}
\bibliography{../dca}





\end{document}





