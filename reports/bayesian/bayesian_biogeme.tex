\documentclass[12pt,a4paper]{article}
\PassOptionsToPackage{hyphens}{url}
\usepackage{michel}
\usetikzlibrary{arrows.meta,positioning,calc,decorations.pathmorphing}
\usepackage[dcucite,abbr]{harvard}
\harvardparenthesis{none}\harvardyearparenthesis{round}
\usepackage{varioref}
\usepackage{longtable}
\usepackage{siunitx}
\usepackage{pgfplots}
\providecommand{\mathdefault}[1]{#1}
\pgfplotsset{compat=newest}
\sisetup{
  parse-numbers=false,
  detect-inline-weight=math,
  tight-spacing=true
}

\usepackage{listings}
\usepackage{color}
\lstset{language=Python}
\lstset{numbers=none, basicstyle=\footnotesize,
  numberstyle=\tiny,keywordstyle=\color{blue},stringstyle=\ttfamily,showstringspaces=false}
\lstset{backgroundcolor=\color[rgb]{0.95 0.95 0.95}}
\lstdefinestyle{numbers}{numbers=left, stepnumber=1,
  numberstyle=\tiny,basicstyle=\tiny, numbersep=10pt}
\lstdefinestyle{nonumbers}{numbers=none}
\lstset{
  breaklines=true,
  breakatwhitespace=true,
}

\usepackage{geometry}
\geometry{left=2cm, top=1.5cm, right=2cm, bottom=1.5cm}

\title{Bayesian Estimation with Biogeme: User Guide}
\author{Michel Bierlaire}
\date{December 25, 2025}

\begin{document}

%======================================================================
% Title page
%======================================================================

\begin{titlepage}
\pagestyle{empty}

\maketitle
\vspace{2cm}

\begin{center}
\small Report TRANSP-OR 251225 \\
Transport and Mobility Laboratory \\
School of Architecture, Civil and Environmental Engineering \\
École Polytechnique Fédérale de Lausanne
\end{center}
    \begin{center}
      \textsc{Series on Biogeme}
    \end{center}


\clearpage
\end{titlepage}

\begin{titlepage}
\tableofcontents
\end{titlepage}

%======================================================================
\section{Introduction}
%======================================================================

Biogeme (\texttt{biogeme.epfl.ch}) is designed to estimate the parameters of
discrete choice models. Although originally built around maximum likelihood
estimation, Biogeme (since version~3.3.2) also supports Bayesian estimation based on Markov chain
Monte Carlo (MCMC), producing draws from the posterior distribution rather than
a single point estimate.

This user guide assumes that the reader is familiar with discrete choice models
and has successfully installed Biogeme. It focuses on \emph{how to run Bayesian
estimation in Biogeme, how to specify models appropriately, and how to interpret
the output}. Methodological background (MCMC, HMC, NUTS) is intentionally kept
to a minimum. A companion document (\cite{Bier25_bayes_a}) provides an informal introduction to the methodological foundations of Bayesian
estimation for discrete choice models.

Bayesian estimation is particularly useful for:
\begin{itemize}
  \item mixture models and random coefficients, where classical maximum
  likelihood requires Monte--Carlo integration;
  \item latent variable models and complex hierarchical structures;
  \item applications where full uncertainty propagation is needed
  (credible intervals, predictive distributions, Bayesian simulation).
\end{itemize}

Although maximum likelihood (ML) and Bayesian estimation rely on the same
underlying behavioral model, they differ in interpretation, numerical treatment,
and diagnostics. Table~\ref{tab:ml_vs_bayes} summarizes the practical
differences from a Biogeme user’s perspective.

\begin{table}[ht]
\centering
\caption{Comparison of maximum likelihood and Bayesian estimation in Biogeme}
\label{tab:ml_vs_bayes}
\begin{tabular}{p{4cm} p{5.5cm} p{5.5cm}}
\hline
 & \textbf{Maximum likelihood (ML)} & \textbf{Bayesian estimation} \\
\hline
Unknown parameters
&
Fixed but unknown constants
&
Random variables with prior distributions
\\[6pt]

Main output
&
Point estimate (MLE) and asymptotic covariance
&
Posterior distribution (draws)
\\[6pt]

Uncertainty interpretation
&
Asymptotic (large-sample) approximation
&
Finite-sample uncertainty (conditional on priors)
\\[6pt]

Estimation algorithm
&
Deterministic optimization (BFGS, Newton, etc.)
&
Stochastic simulation (MCMC via NUTS)
\\[6pt]


Random coefficients
&
Require Monte--Carlo integration
&
Treated as latent variables (no explicit integration)
\\[6pt]

Convergence diagnostics
&
Gradient norm, Hessian eigenvalues
&
$\widehat{R}$, ESS, divergences, trace plots
\\[6pt]

Model comparison
&
Likelihood ratio tests, AIC, BIC
&
WAIC, LOO, posterior predictive fit
\\[6pt]
\hline
\end{tabular}
\end{table}

\noindent


%======================================================================
\section{Quickstart}
%======================================================================

This section provides the minimal steps to run a Bayesian estimation in
Biogeme and explains key configuration options in
\texttt{biogeme.toml}. We assume that the model is already specified in Python
using standard Biogeme expressions.

\subsection{Minimal workflow}

Bayesian estimation in Biogeme follows the same high-level workflow as maximum
likelihood estimation:
\begin{enumerate}
  \item specify the model (utilities, likelihood or log-likelihood expression, parameters);
  \item configure the estimation algorithm in \texttt{biogeme.toml};
  \item run the estimation script;
  \item interpret the HTML report and inspect the \texttt{.nc} file containing
        posterior (and optional prior) draws.
\end{enumerate}
Model specification details (priors and mixture models) are discussed in
Section~\ref{sec:model_specification}.

In the Bayesian case, Biogeme relies on MCMC sampling (NUTS) to approximate the
posterior distribution. The estimation therefore produces \emph{draws} from the
posterior distribution rather than a single point estimate. Reported summaries
(means, medians, HDIs, diagnostics, information criteria) are computed from
these draws.

\subsection{Bayesian configuration in \texttt{biogeme.toml}}
\label{sec:bayesian_config_userguide}

Biogeme reads Bayesian settings from the \texttt{[Bayesian]} section of
\texttt{biogeme.toml}. The most important options control:
(i) which sampler backend is used (PyMC vs NumPyro/JAX),
(ii) the amount of MCMC computation (warm-up, draws, chains),
and (iii) which additional diagnostics and criteria are computed.

\paragraph{Sampler backend and sampling strategy.}
The option \texttt{mcmc\_sampling\_strategy} controls how sampling is performed:
\begin{itemize}
  \item \texttt{"automatic"}: Biogeme selects a suitable strategy depending on
  available hardware and installed libraries. 
  \item \texttt{"pymc"}: use the standard PyMC NUTS sampler on CPU.
  \item \texttt{"numpyro-parallel"}: use NumPyro/JAX and run one chain per
  device (multiple CPU devices, GPUs, or TPUs).
  \item \texttt{"numpyro-vectorized"}: use NumPyro/JAX and run all chains
  vectorized on a single device.
\end{itemize}
When JAX is available, NumPyro-based strategies can be significantly faster,
especially on accelerators or when multiple devices can be used. For most users, \lstinline+mcmc_sampling_strategy="automatic"+ is recommended.

\paragraph{Number of chains, warm-up, and posterior draws.}
\begin{description}
  \item[\texttt{chains}.]
  Number of independent MCMC chains. A common default is 4. Multiple chains are
  essential for convergence diagnostics (e.g.\ $\widehat{R}$).

  \item[\texttt{warmup}.]
  Number of warm-up iterations per chain. These iterations are used to adapt
  the sampler (step size, mass matrix) and are not used for posterior summaries.
  For difficult posteriors, increasing warm-up is often more effective than
  increasing the number of retained draws.

  \item[\texttt{bayesian\_draws}.]
  Number of post-warm-up draws \emph{per chain} retained for inference. Increasing
  this number reduces Monte Carlo error (once the chains mix well), but does not
  fix non-convergence.
\end{description}

\paragraph{Target acceptance rate.}
The option \texttt{target\_accept} is the target acceptance probability for NUTS.
Typical values are 0.8--0.9; values such as 0.9 or 0.95 often improve robustness
for challenging posteriors (at the cost of smaller step sizes and longer run
times). If divergences occur, increasing \texttt{target\_accept} is a common
first adjustment.

\paragraph{Saving prior draws (recommended for identification diagnostics).}
If \texttt{sample\_from\_prior = "True"}, Biogeme generates prior draws and saves
them alongside posterior draws. This is useful to diagnose weak identification,
because it enables direct comparisons between prior and posterior dispersion.

\paragraph{Likelihood-based summaries and model comparison criteria.}
\begin{description}
  \item[\texttt{calculate\_likelihood}.]
  If \texttt{"True"}, Biogeme computes likelihood-based statistics derived from
  posterior draws.

  \item[\texttt{calculate\_waic}.]
  If \texttt{"True"}, Biogeme computes WAIC (see Section~\ref{sec:information}).

  \item[\texttt{calculate\_loo}.]
  If \texttt{"True"}, Biogeme computes LOO (Pareto-smoothed leave-one-out, see Section~\ref{sec:information}).
\end{description}

\subsection{Recommended starting configuration}

A reasonable baseline configuration for many discrete choice models is:
\begin{itemize}
  \item \texttt{chains = 4},
  \item \texttt{warmup = 1000} to \texttt{2000},
  \item \texttt{bayesian\_draws = 1000} to \texttt{2000},
  \item \texttt{target\_accept = 0.9},
  \item \texttt{sample\_from\_prior = "True"} during model development.
\end{itemize}

\subsection{Example \texttt{[Bayesian]} section}

\begin{lstlisting}[language=,basicstyle=\ttfamily\footnotesize]
[Bayesian]
mcmc_sampling_strategy = "automatic"
sample_from_prior = "True"
bayesian_draws = 2000
warmup = 2000
chains = 4
target_accept = 0.9
calculate_waic = "True"
calculate_loo = "True"
calculate_likelihood = "True"
\end{lstlisting}


\noindent
\emph{Practical advice.}
If you are mainly interested in parameter inference, you may disable WAIC/LOO
initially to reduce computation and storage, as the calculation of these
quantities may take a significant amount of time. When comparing models, enable WAIC
and/or LOO and ensure that pointwise log-likelihood values are available in the
output.

%======================================================================
\section{Model specification}
\label{sec:model_specification}
%======================================================================

Model specification for Bayesian estimation is very similar to maximum
likelihood specification, with important differences regarding priors and (for
mixtures) the treatment of random coefficients.

\subsection{Prior distributions}

In Biogeme, all unknown model parameters that may be estimated from data are
represented by objects of class \texttt{Beta}. In the Bayesian context, a
\texttt{Beta} object plays two simultaneous roles:
(i) it defines an unknown quantity appearing in the model expressions (utilities,
log-likelihood, etc.), and
(ii) it defines how this quantity is treated by the Bayesian sampler through a
prior distribution and (optionally) bounds.

\paragraph{The \texttt{Beta} constructor.}
A parameter is created as
\begin{lstlisting}[language=Python]
b_cost = Beta(
    name='b_cost',
    value=-1.0,
    lowerbound=None,
    upperbound=None,
    status=0,
    sigma_prior=5.0,
    prior=None,
)
\end{lstlisting}
The arguments have the following meaning in Bayesian estimation:
\begin{description}
  \item[\texttt{name}] Identifier of the parameter, used in outputs and as the
  underlying PyMC variable name.

  \item[\texttt{value}] Default value.
  In Bayesian estimation, this value is used as an initial value for the sampler
  (via PyMC's \texttt{initval}) and as the center of the default prior when no
  user-defined prior is supplied.

  \item[\texttt{lowerbound}, \texttt{upperbound}] Optional bounds on the support
  of the parameter.
  In Bayesian estimation, these bounds  determine whether Biogeme uses a
  truncated default prior.

  \item[\texttt{status}] If different from 0, the parameter is fixed to its
  default value \texttt{value} and is not sampled. If \texttt{status=0}, the
  parameter is unknown and is estimated/sampled.

  \item[\texttt{sigma\_prior}] Scale used by Biogeme for the \emph{default} prior
  when \texttt{prior=None}. It controls how informative the default Normal (or
  truncated Normal) prior is. Larger values yield weaker regularization.

  \item[\texttt{prior}] User-supplied prior distribution, provided as a
  \emph{prior factory} (a Python callable). If \texttt{prior=None}, Biogeme builds
  a default prior using \texttt{value}, \texttt{sigma\_prior} and the bounds.
\end{description}

\paragraph{Default priors constructed by Biogeme.}
If no user-defined prior is provided (\texttt{prior=None}), Biogeme constructs:
\begin{itemize}
  \item an unbounded Normal prior centered at \texttt{value} when both bounds are
  \texttt{None};
  \item a truncated Normal prior whenever at least one bound is specified.
\end{itemize}
The prior scale is governed by \texttt{sigma\_prior}. This default behavior is
typically appropriate for most applications and provides mild regularization and
numerical stability.

\paragraph{Custom priors via a prior factory.}
A custom prior is specified by providing a callable that receives:
(i) the PyMC variable name,
(ii) the initial value,
(iii) the lower bound (if any),
(iv) the upper bound (if any),
and returns a valid PyMC distribution object.

The example below defines a prior factory that constructs a Student-\textit{t}
distribution and truncates it so that the parameter can only take negative
values:
\begin{lstlisting}
import pymc as pm
from pytensor.tensor import TensorVariable

def negative_student_prior(
    name: str,
    initial_value: float,
    lower_bound: float | None,
    upper_bound: float | None,
) -> TensorVariable:
    base = pm.StudentT.dist(mu=0.0, sigma=10.0, nu=5.0)
    # Enforce negativity through an upper bound at 0.
    upper = 0.0 if upper_bound is None else min(0.0, upper_bound)
    return pm.Truncated(
        name=name,
        dist=base,
        upper=upper,
        initval=initial_value,
    )
\end{lstlisting}

The prior is then passed to a Biogeme parameter as follows:
\begin{lstlisting}[language=Python]
b_cost = Beta(
    'b_cost',
    value=-1.0,
    lowerbound=None,
    upperbound=None,
    status=0,
    prior=negative_student_prior,
)
\end{lstlisting}

Note that, in this case, the values supplied for \texttt{value},
\texttt{lowerbound}, and \texttt{upperbound} are forwarded to the prior
factory, which determines how they are interpreted. In this example,
the \texttt{lowerbound} argument is not used by the factory and, therefore, ignored.


\paragraph{Practical recommendations.}
\begin{itemize}
  \item \textbf{Avoid uniform priors with NUTS.} Flat densities provide little
  curvature and can lead to poor exploration, divergences, and slow or unstable
  convergence.
  \item \textbf{Use bounds to encode identification constraints.} In particular,
  scale parameters (standard deviations) should be constrained to be positive
  (e.g.\ \texttt{lowerbound=1e-6}) to avoid artificial sign symmetry and
  spurious bimodal posteriors.
  \item \textbf{Tune informativeness via \texttt{sigma\_prior}.} When using the
  default priors, \texttt{sigma\_prior} controls the degree of regularization.
\end{itemize}


\subsection{Random coefficients and mixtures}

A major practical advantage of Bayesian estimation in Biogeme is that
mixture models do not require explicit numerical integration over random
parameters. Random coefficients are treated as latent variables and are sampled
jointly with structural parameters.

Because random coefficients are sampled jointly with the other unknowns, the
analyst specifies only the \emph{conditional} log-likelihood given a realization
of the random coefficients (typically the log of a logit probability). There is
no need to write an explicit mixture likelihood or to implement any separate
numerical averaging scheme over the mixing distribution.

In many applications, it is useful to retain simulated draws of random
coefficients in the estimation output (heterogeneity analysis,
post-estimation simulation, etc.). The following statement tells Biogeme to store
the draws of a random coefficient:
\begin{lstlisting}
b_time_rnd = DistributedParameter('b_time_rnd', b_time + b_time_s * b_time_eps)
\end{lstlisting}
Here, \lstinline+b_time_rnd+ is the individual-specific realization of the time
coefficient, stored for later inspection.

\subsection{Scale parameters and identification constraints}

In mixture models, scale parameters (e.g.\ standard deviations of normally
distributed random coefficients) must be constrained to avoid artificial sign
ambiguities. Without a positivity constraint, the sampler may explore both
positive and negative regions that correspond to the same model, producing a
spurious bimodal posterior and poor mixing.

In Biogeme, enforce positivity by specifying a small positive lower bound, for
example \texttt{lowerbound = 1e-6}, when defining the corresponding \texttt{Beta}
parameter.

\subsection{Mixtures with panel data}

With panel data, Biogeme assumes that random parameters are drawn \emph{per
individual}, not per observation, so all observations belonging to the same
person share the same realization of the random coefficients.

From the user's perspective, the model is specified at the observation level.
Conditional on random parameters, the contribution of a single observation is
simply the logit kernel:
\begin{lstlisting}[language=Python]
log_probability_one_observation = loglogit(v, av, CHOICE)
\end{lstlisting}
Biogeme automatically aggregates contributions over the observations belonging
to the same panel and handles sampling of random coefficients internally.

%======================================================================
\section{Running Bayesian estimation}
%======================================================================

In the Bayesian case, once the model is specified and the \texttt{[Bayesian]}
settings are defined in \texttt{biogeme.toml}
(Section~\ref{sec:bayesian_config_userguide}), you run exactly the same Python
estimation script as for maximum likelihood. Biogeme then produces an HTML
report and a \texttt{.nc} file with posterior draws. The next section explains
how to interpret these outputs.


%======================================================================
\section{Interpreting the output}
\label{sec:interpreting_output}
%======================================================================

Biogeme performs Bayesian estimation by relying internally on PyMC (and
optionally NumPyro/JAX). The output mirrors standard PyMC and ArviZ
diagnostics and is reported in a unified HTML report.
This section explains how to interpret the main numerical summaries,
diagnostics, and graphical outputs.

As a rule, interpret posterior summaries only after convergence diagnostics
($\widehat{R}$, ESS, divergences) and basic graphical checks (trace and energy
plots) are satisfactory.

We refer the reader to the online documentation of PyMC and ArviZ, as well
as \citeasnoun{Vehtari_2016} and
\citeasnoun{watanabe2010asymptoticequivalencebayescross}, for additional
background.

\subsection{General information about the estimation}

\begin{description}
\item[Sample size.] Number of observations used in the estimation.

\item[Sampler.] Biogeme selects an appropriate sampling method depending on
hardware and installed libraries. Users can also force a sampling strategy via
\texttt{mcmc\_sampling\_strategy}.

\item[Number of chains.] Number of independent MCMC chains (typically 4).

\item[Number of draws per chain.] Retained post-warm-up draws per chain.

\item[Acceptance rate target.] Target acceptance probability for NUTS.

\item[Run time.] Total wall-clock time used to obtain the posterior sample.
\end{description}

\subsection{Posterior log-likelihood and predictive fit}

\begin{description}
\item[Log-likelihood at posterior mean.]
Log-likelihood evaluated at the posterior mean.

\item[Expected log-likelihood.]
Average of the log-likelihood across posterior draws.

\item[Best-draw log-likelihood.]
Highest log-likelihood value encountered among posterior draws.
\end{description}

\noindent
\emph{Practical advice.}
If the log-likelihood at posterior mean and expected log-likelihood differ
substantially, the posterior may be wide, skewed, or the model may be sensitive
to specific parameter regions.

\subsection{Information criteria for model comparison}
\label{sec:information}
Biogeme reports two fully Bayesian criteria for model comparison:
the Widely Applicable Information Criterion (WAIC) and
Pareto-smoothed Leave-One-Out cross-validation (LOO).
Both criteria estimate a model’s expected predictive performance on new,
unobserved data, while accounting for model complexity.

\subsubsection*{Widely Applicable Information Criterion (WAIC)}

WAIC is a Bayesian generalization of the Akaike Information Criterion (AIC).
It is based on the \emph{pointwise} log-likelihood contributions and averages
predictive performance over the posterior distribution rather than evaluating
it at a single point estimate.

Let $\ell_n(\boldsymbol{\theta}) = \log p(y_n \mid \boldsymbol{\theta})$ denote
the log-likelihood contribution of observation $n$.
Given posterior draws $\boldsymbol{\theta}^{(s)}$, $s=1,\dots,S$, define:
\[
\text{lppd}
=
\sum_{n=1}^N
\log\!\left(
\frac{1}{S}
\sum_{s=1}^S
\exp\!\bigl(\ell_n(\boldsymbol{\theta}^{(s)})\bigr)
\right),
\]
the \emph{log pointwise predictive density}.
The effective number of parameters is estimated as
\[
p_{\text{WAIC}}
=
\sum_{n=1}^N
\mathrm{Var}_{s}
\!\left(
\ell_n(\boldsymbol{\theta}^{(s)})
\right),
\]
where the variance is taken across posterior draws.

WAIC is then defined as
\[
\text{WAIC}
=
-2 \left( \text{lppd} - p_{\text{WAIC}} \right).
\]

Lower WAIC values indicate better expected out-of-sample predictive
performance.  
Because WAIC relies on posterior draws, it is valid for a wide class of
models, including hierarchical and latent-variable models commonly
estimated in Biogeme.

\subsubsection*{Leave-One-Out Cross-Validation (LOO)}

LOO estimates predictive performance by repeatedly leaving out one
observation and evaluating how well the model predicts it.
Formally, it targets
\[
\sum_{n=1}^N \log p(y_n \mid \mathcal{D}_{-n}),
\]
where $\mathcal{D}_{-n}$ denotes the data set with observation $n$ removed.

Exact LOO would require refitting the model $N$ times, which is
computationally infeasible.  
Biogeme therefore relies on \emph{Pareto-smoothed importance sampling}
(PSIS-LOO), which approximates leave-one-out predictive densities using
the posterior draws from the full data set.

LOO is reported on the deviance scale:
\[
\text{LOO}
=
-2 \sum_{n=1}^N \log \widehat{p}(y_n \mid \mathcal{D}_{-n}),
\]
where $\widehat{p}$ denotes the PSIS approximation.

LOO also provides diagnostic measures (Pareto $k$ values) that assess the
reliability of the importance-sampling approximation. Large $k$ values
signal influential observations or model misspecification.

\subsubsection*{Practical interpretation}

\begin{itemize}
  \item Lower WAIC or LOO values indicate better predictive performance.
  \item Differences should be interpreted relative to their standard errors.
        As a rule of thumb, differences smaller than about twice the standard
        error are not decisive.
  \item LOO is generally more robust than WAIC, especially for complex
        models with latent variables or weak identification.
\end{itemize}

In Biogeme, both WAIC and LOO are computed from pointwise log-likelihood
values stored in the estimation output. When available, LOO is typically
recommended as the primary criterion for Bayesian model comparison.



\subsection{Posterior parameter summaries}

For each parameter, Biogeme reports statistics derived from posterior draws:
\begin{description}
\item[Name.] Identifier of the parameter.

\item[Value (posterior mean).] Expected value under the posterior distribution.

\item[Median.] Posterior median (robust measure of central tendency).

\item[Mode.] Posterior mode (kernel density estimation), useful for skewed or
multimodal posteriors.

\item[Std err.] Posterior standard deviation.

\item[$z$-value.] Mean divided by standard deviation.

\item[$p$-value.] Two-sided posterior probability that the parameter has the
opposite sign from its mean.

\item[HDI.] Highest Density Interval bounds (typically 95\%).
\end{description}

\subsection{Convergence diagnostics}

\begin{description}
\item[$\widehat{R}$.] Values close to 1 indicate good mixing. A common threshold
is $\widehat{R} \le 1.01$.

\item[ESS (bulk).] Effective sample size for the main mass of the posterior.
Values above 400 support reliable estimation of means and variances.

\item[ESS (tail).] Effective sample size for tail behavior. Values above 100 help
stabilize extreme quantiles.
\end{description}

\subsection{Identification diagnostics}

These diagnostics detect non-identification or weak identification. Intuitively,
identification problems arise when some linear combinations of parameters can vary
substantially without affecting the likelihood, leading to very wide posterior
directions. Diagnostics use posterior draws and (when available) prior draws.

\paragraph{Posterior covariance diagnostics.}
\begin{itemize}
\item \emph{Minimum and maximum eigenvalues.}
Eigenvalues measure posterior variance along orthogonal directions.
A very \emph{large} eigenvalue corresponds to a very wide (nearly flat) direction,
indicating weak or non-identification along a linear combination of parameters.
A very small eigenvalue corresponds to a tightly concentrated direction.

\item \emph{Condition number.}
Ratio of largest to smallest eigenvalue; measures anisotropy of the posterior
covariance. Large values indicate near-dependencies among parameters. Values
around $10^3$ deserve attention; $10^5$ or more is a strong warning sign.

\item \emph{Effective rank.}
Effective dimensionality of posterior variability (between 0 and the number of
parameters). Values much smaller than the number of parameters suggest that
variability concentrates in a lower-dimensional subspace.
\end{itemize}

\paragraph{Prior covariance diagnostics.}
Same diagnostics for the prior. If the prior is well behaved but the posterior
becomes ill-conditioned, the issue typically originates from the likelihood/model
specification.

\paragraph{Identified by the prior.}
When prior draws are available, Biogeme reports per-parameter ratios of posterior
standard deviation to prior standard deviation:
\begin{itemize}
  \item ratios close to 1 suggest weak information in the likelihood (prior dominates);
  \item ratios well below 1 suggest that the data are informative.
\end{itemize}

\subsection{Simulated quantities (\texttt{.nc} output)}

The \texttt{.nc} file stores posterior draws (and optional prior draws) in a PyMC
\texttt{InferenceData} structure. The explicit list is provided in the HTML file.   Key groups include:
\begin{description}
\item[constant\_data.] Observed data treated as fixed (indexed by \texttt{Dimension.OBS}).

\item[posterior.] Posterior draws of parameters and derived quantities (typically
\texttt{(chain, draw)}; observation-level quantities include \texttt{Dimension.OBS}).

\item[prior.] Prior draws, if enabled.

\item[log\_likelihood.] Pointwise log-likelihood contributions used for WAIC/LOO.

\item[sample\_stats.] Sampler diagnostics (acceptance rate, divergences, step size,
tree depth, energy, etc.).
\end{description}

\subsection{Graphical diagnostics}

Biogeme includes plots such as trace plots, rank plots, energy plots, and
autocorrelation plots, generated by ArviZ. Even when numerical diagnostics look good, graphical
inspection may help detect multimodality, slow transitions, or other issues. If the plots are not visible in the report, consider using ArviZ directly to
regenerate them.

\subsection{Troubleshooting common issues}
\label{sec:troubleshooting}

This subsection summarizes common warning signs encountered in Bayesian
estimation with \texttt{Biogeme} and provides guidance on how to address
them. It should be read in conjunction with the convergence,
identification, and graphical diagnostics described above.

\paragraph{Chains do not converge ($\widehat{R} > 1.01$).}
Values of $\widehat{R}$ significantly above one indicate that different
chains explore different regions of the posterior distribution.
\begin{itemize}
  \item Increase the number of warm-up iterations to allow the sampler to
  adapt more thoroughly.
  \item Inspect trace plots to detect poorly mixing chains or slow drift.
  \item Revisit model identification: near-linear dependencies or
  redundant parameters often prevent convergence.
  \item Reparameterize the model by removing redundant constants,
  centering variables, or rescaling explanatory variables.
\end{itemize}

\paragraph{Low effective sample size (ESS).}
Low ESS values indicate strong autocorrelation in the simulated draws,
even when $\widehat{R}$ is satisfactory. This reduces the precision of
posterior summaries.
\begin{itemize}
  \item Increase the number of posterior draws
  (\texttt{bayesian\_draws}).
  \item Increase \texttt{target\_accept} to improve exploration of the
  posterior geometry.
  \item Use pair plots to identify strong posterior correlations and
  consider reparameterization.
\end{itemize}

\paragraph{Divergent transitions reported.}
Divergent transitions signal numerical instability in the simulation of
Hamiltonian dynamics and usually indicate problematic posterior
geometry. They should not be ignored.
\begin{itemize}
  \item Increase \texttt{target\_accept} to reduce the integration step
  size.
  \item Ensure that all scale parameters are strictly positive and
  parameterized in a numerically stable way.
  \item Inspect pair plots and energy plots to detect funnel-shaped
  geometries, extreme curvature, or heavy-tailed behavior induced by
  diffuse priors.
\end{itemize}

\paragraph{Very slow sampling or excessive run time.}
Long run times may result from complex model specifications, expensive
likelihood evaluations, or post-processing computations.
\begin{itemize}
  \item Simplify the model during development and add complexity
  incrementally.
  \item Temporarily disable WAIC and LOO, which require storing and
  processing pointwise log-likelihood values.
  \item When available, use the NumPyro/JAX backend to benefit from
  just-in-time compilation and automatic differentiation.
\end{itemize}

\paragraph{Posterior resembles the prior.}
When posterior and prior distributions are very similar, the data provide
little additional information about the corresponding parameters.
\begin{itemize}
  \item Compare posterior and prior dispersion measures to assess the
  amount of learning.
  \item Verify through trace plots that the chains explore the posterior
  rather than merely reproducing the prior.
  \item Reconsider the model specification and verify that the data
  contain sufficient identifying variation.
\end{itemize}
%======================================================================
\section{Simulation and prediction}
\label{sec:simulation_prediction}
%======================================================================

Once posterior draws are available, Biogeme supports simulation either at a
single representative parameter vector (posterior means) or by propagating full
posterior uncertainty.

\subsection{Simulation using posterior means}

Replace each parameter by its posterior mean and run a deterministic simulation:
\begin{lstlisting}[language=Python]
results = biosim.simulate(the_beta_values=betas)
\end{lstlisting}
This is fast and interpretable, but ignores posterior uncertainty.

\subsection{Bayesian simulation using posterior draws}

Propagate uncertainty by simulating the model for a subset of posterior draws:
\begin{lstlisting}[language=Python]
bayesian_results = biosim.simulate_bayesian(
    bayesian_estimation_results=estimation_results,
    percentage_of_draws_to_use=3
)
\end{lstlisting}
The output becomes a distribution of simulated quantities, enabling credible
regions and uncertainty-aware decision support.

%======================================================================
\section{Conclusion}
%======================================================================

This user guide has presented the practical workflow for Bayesian
estimation in \texttt{Biogeme}. We described how to enable Bayesian
inference through the \texttt{[Bayesian]} section of \texttt{biogeme.toml},
how to specify priors and identification constraints within the usual
\texttt{Beta} parameter objects, and how Bayesian estimation naturally
handles random coefficients and hierarchical structures by sampling latent
variables rather than requiring explicit numerical integration.

We then explained how to interpret the HTML report and the accompanying
\texttt{.nc} output file: posterior parameter summaries (mean, median,
mode, HDI), convergence diagnostics ($\widehat{R}$ and ESS), identification
diagnostics based on posterior (and optionally prior) draws, information
criteria for model comparison (WAIC and LOO), and the role of graphical
diagnostics such as trace plots, pair plots, and energy plots. Finally,
we provided troubleshooting guidance and illustrated how posterior draws
can be used for uncertainty-aware simulation and prediction.

While the concepts and options described here apply broadly, effective
Bayesian practice is best learned by working through concrete examples.
For this reason, readers are strongly encouraged to consult the online
examples provided with \texttt{Biogeme}, which illustrate recommended
configurations, typical diagnostic patterns, and complete end-to-end
workflows for estimation, model comparison, and Bayesian simulation.
%======================================================================
\clearpage
\bibliographystyle{dcudoi}
\bibliography{transpor}

\end{document}
