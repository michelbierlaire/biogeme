\documentclass[12pt,a4paper]{article}

% Package to include code
\usepackage{listings}
\usepackage{color}
\usepackage{varioref}
\lstset{language=Python}
\lstset{numbers=none, basicstyle=\footnotesize,
  numberstyle=\tiny,keywordstyle=\color{blue},stringstyle=\ttfamily,showstringspaces=false}
\lstset{backgroundcolor=\color[rgb]{0.95 0.95 0.95}}
\lstdefinestyle{numbers}{numbers=left, stepnumber=1, numberstyle=\tiny, numbersep=10pt}
\lstdefinestyle{nonumbers}{numbers=none}

\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}

% Font selection: uncomment the next line to use the ``beton'' font
%\usepackage{beton}

% Font selection: uncomment the next line to use the ``times'' font
%\usepackage{times}

% Font for equations
\usepackage{euler}


%Package to define the headers and footers of the pages
\usepackage{fancyhdr}


%Package to include an index
\usepackage{index}

%Package to display boxes around texts. Used especially for the internal notes.
\usepackage{framed}

%PSTricks is a collection of PostScript-based TEX macros that is compatible
% with most TEX macro packages
\usepackage{pstricks}
\usepackage{pst-node}
\usepackage{pst-plot}
\usepackage{pst-tree}

%Package to display boxes around a minipage. Used especially to
%describe the biography of people.
\usepackage{boxedminipage}

%Package to include postscript figures
\usepackage{epsfig}

%Package for the bibliography
% \cite{XXX} produces Ben-Akiva et. al., 2010
% \citeasnoun{XXX} produces Ben-Akiva et al. (2010)
% \citeasnoun*{XXX} produces Ben-Akiva, Bierlaire, Bolduc and Walker (2010)
\usepackage[dcucite,abbr]{harvard}
\harvardparenthesis{none}\harvardyearparenthesis{round}

%Packages for advanced mathematics typesetting
\usepackage{amsmath,amsfonts,amssymb}

%Package to display trees easily
%\usepackage{xyling}

%Package to include smart references (on the next page, on the
%previous page, etc.) 
%%

%% Remove as it is not working when the book will be procesed by the
%% publisher.
%\usepackage{varioref}

%Package to display the euro sign
\usepackage[right,official]{eurosym}

%Rotate material, especially large table (defines sidewaystable)
\usepackage[figuresright]{rotating}

%Defines the subfigure environment, to obtain refs like Figure 1(a)
%and Figure 1(b). 
\usepackage{subfigure}

%Package for appendices. Allows subappendices, in particular
\usepackage{appendix}

%Package controling the fonts for the captions
\usepackage[font={small,sf}]{caption}

%Defines new types of columns for tabular ewnvironment
\usepackage{dcolumn}
\newcolumntype{d}{D{.}{.}{-1}}
\newcolumntype{P}[1]{>{#1\hspace{0pt}\arraybackslash}}
\newcolumntype{.}{D{.}{.}{9.3}}

%Allows multi-row cells in tables
\usepackage{multirow}

%Tables spaning more than one page
\usepackage{longtable}


%%
%%  Macros by Michel
%%

%Internal notes
\newcommand{\note}[1]{
\begin{framed}{}%
\textbf{\underline{Internal note}:} #1
\end{framed}}

%Use this version to turn off the notes
%\newcommand{\note}[1]{}


%Include a postscript figure . Note that the label is prefixed with
%``fig:''. Remember it when you refer to it.  
%Three arguments:
% #1 label
% #2 file (without extension)
% #3 Caption
\newcommand{\afigure}[3]{%
\begin{figure}[!tbp]%
\begin{center}%
\epsfig{figure=#2,width=0.8\textwidth}%
\end{center}
\caption{\label{fig:#1} #3}%
\end{figure}}






%Include two postscript figures side by side. 
% #1 label of the first figure
% #2 file for the first figure
% #3 Caption for the first figure
% #4 label of the second figure
% #5 file for the second figure
% #6 Caption for the first figure
% #7 Caption for the set of two figures
\newcommand{\twofigures}[7]{%
\begin{figure}[htb]%
\begin{center}%
\subfigure[\label{fig:#1}#3]{\epsfig{figure=#2,width=0.45\textwidth}}%
\hfill
\subfigure[\label{fig:#4}#6]{\epsfig{figure=#5,width=0.45\textwidth}}%
\end{center}
\caption{#7}%
\end{figure}}

%Include a figure generated by gnuplot using the epslatex output. Note that the label is prefixed with
%``fig:''. Remember it when you refer to it.  
 
%Three arguments:
% #1 label
% #2 file (without extension)
% #3 Caption
\newcommand{\agnuplotfigure}[3]{%
\begin{figure}[!tbp]%
\begin{center}%
\input{#2}%
\end{center}
\caption{\label{fig:#1} #3}%
\end{figure}}

%Three arguments:
% #1 label
% #2 file (without extension)
% #3 Caption
\newcommand{\asidewaysgnuplotfigure}[3]{%
\begin{sidewaysfigure}[!tbp]%
\begin{center}%
\input{#2}%
\end{center}
\caption{\label{fig:#1} #3}%
\end{sidewaysfigure}}


%Include two postscript figures side by side. 
% #1 label of the first figure
% #2 file for the first figure
% #3 Caption for the first figure
% #4 label of the second figure
% #5 file for the second figure
% #6 Caption for the second figure
% #7 Caption for the set of two figures
% #8 label for the whole figure
\newcommand{\twognuplotfigures}[7]{%
\begin{figure}[htb]%
\begin{center}%
\subfigure[\label{fig:#1}#3]{\input{#2}}%
\hfill
\subfigure[\label{fig:#4}#6]{\input{#5}}%
\end{center}
\caption{#7}%
\end{figure}}



%Include the description of somebody. Four arguments:
% #1 label
% #2 Name
% #3 file (without extension)
% #4 description
\newcommand{\people}[4]{
\begin{figure}[tbf]
\begin{boxedminipage}{\textwidth}
\parbox{0.40\textwidth}{\epsfig{figure=#3,width = 0.39\textwidth}}%\hfill
\parbox{0.59\textwidth}{%
#4% 
}%
\end{boxedminipage}
\caption{\label{fig:#1} #2}
\end{figure}
}

%Default command for a definition
% #1 label (prefix def:)
% #2 concept to be defined
% #3 definition
\newtheorem{definition}{Definition}
\newcommand{\mydef}[3]{%
\begin{definition}%
\index{#2|textbf}%
\label{def:#1}%
\textbf{#2} \slshape #3\end{definition}}

%Reference to a definitoin. Prefix 'def:' is assumed
\newcommand{\refdef}[1]{definition~\ref{def:#1}}


%Default command for a theorem, with proof
% #1: label (prefix thm:)
% #2: name of the theorem
% #3: statement
% #4: proof
\newtheorem{theorem}{Theorem}
\newcommand{\mytheorem}[4]{%
\begin{theorem}%
\index{#2|textbf}%
\index{Theorems!#2}%
\label{thm:#1}%
\textbf{#2} \sffamily \slshape #3
\end{theorem} \bpr #4 \epr \par}


%Default command for a theorem, without proof
% #1: label (prefix thm:)
% #2: name of the theorem
% #3: statement
\newcommand{\mytheoremsp}[3]{%
\begin{theorem}%
\index{#2|textbf}%
\index{Theorems!#2}%
\label{thm:#1}%
\textbf{#2} \sffamily \slshape #3
\end{theorem}}



%Put parentheses around the reference, as standard for equations
\newcommand{\req}[1]{(\ref{#1})}

%Short cut to make a column vector in math environment (centered)
\newcommand{\cvect}[1]{\left( \begin{array}{c} #1 \end{array} \right) }

%Short cut to make a column vector in math environment (right justified)
\newcommand{\rvect}[1]{\left( \begin{array}{r} #1 \end{array} \right) }

%A reference to a theorem. Prefix thm: is assumed for the label.
\newcommand{\refthm}[1]{theorem~\ref{thm:#1}}

%Reference to a figure. Prefix fig: is assumed for the label.
\newcommand{\reffig}[1]{Figure~\ref{fig:#1}}

%Smart reference to a figure. Prefix fig: is assumed for the label.
%\newcommand{\vreffig}[1]{Figure~\vref{fig:#1}}

%C in mathcal font for the choice set
\newcommand{\C}{\mathcal{C}}

%R in bold font for the set of real numbers
\newcommand{\R}{\mathbb{R}}

%N in bold font for the set of natural numbers
\newcommand{\N}{\mathbb{N}}

%C in mathcal font for the log likelihood
\renewcommand{\L}{\mathcal{L}}

%S in mathcal font for the subset S
\renewcommand{\S}{\mathcal{S}}

%To write an half in math envionment
\newcommand{\half}{\frac{1}{2}}

%Probability
\newcommand{\prob}{\operatorname{Pr}}

%Expectation
\newcommand{\expect}{\operatorname{E}}

%Variance
\newcommand{\var}{\operatorname{Var}}

%Covariance
\newcommand{\cov}{\operatorname{Cov}}

%Correlation
\newcommand{\corr}{\operatorname{Corr}}

%Span
\newcommand{\myspan}{\operatorname{span}}

%plim
\newcommand{\plim}{\operatorname{plim}}

%Displays n in bold (for the normal distribution?)
\newcommand{\n}{{\bf n}}

%Includes footnote in a table environment. Warning: the footmark is
%always 1.
\newcommand{\tablefootnote}[1]{\begin{flushright}
\rule{5cm}{1pt}\\
\footnotemark[1]{\footnotesize #1}
\end{flushright}
}

%Defines the ``th'' as in ``19th'' to be a superscript
\renewcommand{\th}{\textsuperscript{th}}

%Begin and end of a proof
\newcommand{\bpr}{{\bf Proof.} \hspace{1 em}}
\newcommand{\epr}{$\Box$}


\title{A note on MEV models}
\author{Michel Bierlaire} 
\date{\today}

\newcommand{\PBIOGEME}{PandasBiogeme}


\begin{document}


\begin{titlepage}
\pagestyle{empty}

\maketitle
\vspace{2cm}

\begin{center}
\small Report TRANSP-OR xxxxxx \\ Transport and Mobility Laboratory \\ School of Architecture, Civil and Environmental Engineering \\ Ecole Polytechnique F\'ed\'erale de Lausanne \\ \verb+transp-or.epfl.ch+
\begin{center}
\textsc{Series on Biogeme}
\end{center}
\end{center}


\clearpage
\end{titlepage}



\clearpage 


This document contains lecture notes about Multivariate Extreme Value
models, introduced by \citeasnoun{McFa78}.

\section{Definition}

A vector $\varepsilon_n = (\varepsilon_{1n},\ldots,\varepsilon_{Jn})$
follows a \emph{multivariate extreme value}\footnote{There are several families of multivariate
  extreme value distributions. We refer the interested reader to
  \citeasnoun{Pick81},   \citeasnoun{Joe1997} or \citeasnoun{KotzNada01}, among others.}  distribution if it is characterized by the following cumulative distribution function:
\begin{equation}
\label{eq:mevCdf}
F_{\varepsilon_n}(\xi_{1},\ldots,\xi_{J}) =
e^{-G(e^{-\xi_{1}},\ldots,e^{-\xi_{J}})},
\end{equation}
where $G:\R_+^{J_n} \to \R_+$ is a positive function accepting
positive arguments, denoted by $y_{1},\ldots,y_{J}$ in the following. Note from  \req{eq:mevCdf} that, in our context, $y_{i} = \exp(-\xi_{i})$, guaranteeing the positivity of the arguments. 
To be a valid CDF, the function $F_{\varepsilon_n}$ must verify some
properties, and so does $G$:
\begin{enumerate}
\item $F_{\varepsilon_n}$ goes to zero when any  argument goes to $-\infty$, that is
\[
 F_{\varepsilon_n}(\xi_{1},\ldots,-\infty,\ldots,\xi_{J}) = 0.
\]
When $\xi_{i}$ goes to $-\infty$, then $y_{i}=\exp(-\xi_{i})$ goes to $+\infty$.
Consequently, the corresponding condition on $G$ is 
\begin{equation}
\label{eq:limitProperty}
 G(y_{1},\ldots,+\infty,\ldots,y_{J}) = +\infty,
\end{equation}
that is the function must go to infinity whenever one of its arguments does. It is called the \emph{limit} property.
\item  $F_{\varepsilon_n}$ goes to one when all of its arguments go to $+\infty$, that is
\[
F_{\varepsilon_n}(+\infty,\ldots,+\infty) = 1.
\]
When $\xi_{i}$ goes to $+\infty$, then $y_{i}=\exp(-\xi_{i})$ goes to $0$.
Consequently, the corresponding condition on $G$ is 
\begin{equation}
\label{eq:G0=0}
G(0,\ldots, 0) =  0.
\end{equation}
\item Any partial derivative of $F_{\varepsilon_n}$ defines a density
  function of a marginal distribution. To be a valid density function, it has to be non negative. More precisely, for any set of $\widehat{J}_n \leq J_n$
distinct indices $i_1,\dots,i_{\widehat{J}_n}$,
\[
 \frac{\partial^{\widehat{J}_n} F_{\varepsilon_n}}{\partial \varepsilon_{i_1 n} \cdots \partial \varepsilon_{i_{\widehat{J}_n}n}}(\varepsilon_{1n}, \ldots,\varepsilon_{{J_n}n}) \geq 0.
\]
In particular, if $\widehat{J}_n = J_n$, we obtain the density function of the entire distribution of $\varepsilon_n$, that is
\[
f_{\varepsilon_n}(\varepsilon_{1n}, \ldots,\varepsilon_{{J_n}n}) = \frac{\partial^{J_n} F_{\varepsilon_n}}{\partial \varepsilon_{1n} \cdots\partial  \varepsilon_{{J_n}n}}(\varepsilon_{1n}, \ldots,\varepsilon_{{J_n}n}) \geq 0.
\]

Considering \req{eq:mevCdf}, the above condition says that any level of differentiation must correspond to a non-negative result. 
It appears that the right-hand side of  \req{eq:mevCdf} changes sign each time it is differentiated, except the first time. Indeed, $\partial y_{in} / \partial \varepsilon_{in} = -\exp(-\varepsilon_{in}) = -y_{in}$. To compensate that and always obtain a non negative sign, the function $G$ must also change sign each time it is differentiated. This condition is called the \emph{strong alternating sign property}, and states  that the cross partial derivatives
  of $G$ have alternative signs\footnote{The \emph{weak alternating
  sign property} requires that $\ln G$ changes sign each time it is
  differentiated. See \citeasnoun{FosgMcFaBier13} for a detailed discussion.}. That is, at the first degree, 
\[
G_i = \partial G/ \partial y_i \geq 0,
\]
for $i=1,\ldots,J_n$. At the second degree, 
\[
G_{ij} = \partial G_i/\partial y_j = \partial^2 G/\partial y_i \partial y_j \leq 0,
\]
 for $i\neq j$. For higher degrees,  and for any set of $\widehat{J}_n$
distinct indices $i_1,\dots,i_{\widehat{J}_n}$, 
\begin{equation}
\label{eq:strongAltSignProp}
(-1)^{\widehat{J}_n-1} G_{i_1,\ldots,i_{\widehat{J}_n}} \geq 0.
\end{equation}
\end{enumerate}

If these properties are verified, \req{eq:mevCdf} is a valid CDF. 
We also need an additional condition on $G$: \emph{homogeneity}. A function $G$ is homogeneous
of degree $\mu$, or $\mu$-homogeneous, if
\begin{equation}
\label{eq:homogProp}
G(\alpha y) = \alpha^\mu G(y), \;\forall \alpha > 0 \text{ and } y\in \R_+^{J}.
\end{equation}
The homogeneity condition   implies two important properties of the model. We show below that, if $G$ is homogeneous, 
\begin{itemize}
\item the marginals of \req{eq:mevCdf} are univariate extreme value distributions, so that the valid CDF indeed corresponds to a multivariate extreme value distribution, and,
\item the corresponding choice model has a closed form.
\end{itemize}
The $i$th marginal
distribution of \req{eq:mevCdf} is given by
\begin{equation}
F_{\varepsilon_n}(+\infty,\ldots,+\infty,\varepsilon_{in},+\infty,\ldots,+\infty) =
e^{-G(0,\ldots,0,e^{-\varepsilon_{in}},0,\ldots,0)}.
\end{equation}
If $G$ is $\mu$-homogeneous, we have
\[
G(0,\ldots,0,e^{-\varepsilon_{in}},0,\ldots,0) = e^{-\mu\varepsilon_{in}}G(0,\ldots,0,1,0,\ldots,0),
\]
or equivalently,
\[
G(0,\ldots,0,e^{-\varepsilon_{in}},0,\ldots,0) = e^{-\mu\varepsilon_{in} + \ln G(0,\ldots,0,1,0,\ldots,0)},
\]
The quantity $\ln G(0,\ldots,0,1,0,\ldots,0)$ is a constant. Call it
$\mu \eta$, so that
the CDF of the $i$\th marginal
distribution of $\varepsilon_n$ is
\begin{equation}
F_{\varepsilon_n}(+\infty,\ldots,+\infty,\varepsilon_{in},+\infty,\ldots,+\infty)
=\exp\left(-e^{-\mu(\varepsilon_{in}-\eta)}\right),
\end{equation}
which is the CDF of a univariate extreme value distribution with
location parameter $\eta$ and scale parameter $\mu$.

We have now established that $F$ is the CDF of a multivariate extreme
value distribution if $G$ verifies a handful of properties:
\begin{description}
\item[M1:] the strong alternating sign property \req{eq:strongAltSignProp}, 
\item[M2:] the $\mu$-homogeneity property \req{eq:homogProp}, and,
\item[M3:] the limit property \req{eq:limitProperty}.
\end{description}
Note that the condition \req{eq:G0=0} is not included in the above list, as it is a direct consequence of the
homogeneity property. 


The corresponding choice model is:
\begin{equation}
  \label{eq:mev-orig}
P_n(i)=\frac{e^{V_{in}}G_i(e^V)}{\mu G(e^V)},
\end{equation}
or, equivalently,
\begin{equation}
\label{eq:mev-mnlLike}
P_n(i) = \frac{e^{V_{in} + \ln G_i\left(e^{V_{1n}},\ldots,e^{V_{J_n n}}\right)}}{\sum_j e^{V_{jn} + \ln G_j\left(e^{V_{1n}},\ldots,e^{V_{J_n n}}\right)}}.
\end{equation}
This is the \emph{multivariate extreme value} (MEV) model.  The function $G$
is called a \emph{choice probability generating function} (CPGF).

Formulation \req{eq:mev-mnlLike} is interesting because it has a
similar structure as the logit model. Indeed, it can be interpreted as
a logit model, where each systematic utility $V_i$ is shifted by  $
\ln G_i(\cdot)$. As a consequence, relaxing the independence
assumption associated with  the logit model can be accommodated by an
appropriate correction of the utility functions, while keeping the
functional form of the logit. However, it has to be remembered that the utility of an alternative $i$ depends on the variables of all alternatives in the MEV context. Moreover, if the $G$ function has a closed form, so has the choice model. 

\section{Properties of the MEV model}
\label{sec:MEVproperties}
We present here various comments, properties and features of the MEV model. 
\begin{itemize}
\item The multivariate extreme value model was first proposed by
\citeasnoun{McFa78}, under the name ``Generalized Extreme Value''
model. In order to avoid any confusion with the Generalized Extreme
Value distribution, and to
emphasize that we are dealing with multivariate distributions, we
 refer to this model as \emph{multivariate extreme value} (MEV).
\item In the context of random utility, a random vector $U_n=(U_{1n},\ldots,U_{J_nn})=(V_{1n}+\varepsilon_{1n},\ldots,V_{Jn}+\varepsilon_{Jn})$ with  a MEV distribution is such that its CDF is
\begin{equation}
\label{eq:mevCdfU}
 F_{U_n}(\xi_{1},\ldots,\xi_{J_n}) = \prob(U_n \leq \xi_n) =
e^{-G(e^{V_{1n}-\xi_1},\ldots,e^{V_{J_nn}-\xi_{J_n}})}.
\end{equation}
\item The marginal distributions of $(U_n)_j$ for $j=1,\ldots,J_n$ are extreme value distributed, with 
\begin{itemize}
\item means:
\begin{equation}
\label{eq:meanMarginals}
V_{jn} + \frac{\ln G(0,\ldots,1,\ldots,0) +\gamma}{\mu},
\end{equation}
where $\gamma$ is Euler's constant:
\begin{equation}
\label{eq:eulersConstant}
\gamma = - \int_0^{+\infty} e^{-x} \ln x dx \approx 0.5772,
\end{equation}
 \item  variances: $\pi^2/6 \mu^2$, for each $j$, and
\item  moment generating functions
\begin{equation}
e^{tV_{jn}} G(0,\ldots,1,\ldots,0)^{\frac{t}{\mu}} \Gamma\left(1-\frac{t}{\mu}\right),
\end{equation}
where $\Gamma(\cdot)$ is the Gamma function
\[
\Gamma(t) = \int_0^{+\infty}z^{t-1}e^{-z}dz.
\]
\end{itemize}
\item The variance covariance matrix of a MEV model is derived from its CDF \req{eq:mevCdf}.
The covariance between the error terms of two alternatives $i$ and $j$ is given by
\begin{equation}
\label{eq:covMev}
\begin{array}{rcl}
\cov(\varepsilon_{in},\varepsilon_{jn}) &=& \expect[\varepsilon_{in} \varepsilon_{jn}] - \expect[\varepsilon_{in}]  \expect[\varepsilon_{jn}] \\ && \\
&=& \displaystyle  \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}\xi_i \xi_j \frac{\partial^2 F_{\varepsilon_n} (\xi_i,\xi_j)}{\partial \xi_i \partial \xi_j } d\xi_i d\xi_j -  \gamma^2,
\end{array}
\end{equation}
where $\expect[\varepsilon_{in}]=\gamma$,
\begin{equation}
\label{eq:doubleMarginal}
F_{\varepsilon_n} (\xi_i,\xi_j) = F_{\varepsilon_n}(\ldots,+\infty, \xi_i,+\infty,\ldots,+\infty,\xi_j,+\infty,\ldots)
\end{equation}
is the bivariate marginal cumulative distribution, and
\begin{equation}
 \frac{\partial^2 F_{\varepsilon_{in},\varepsilon_{jn}} (\xi_i,\xi_j)}{\partial \xi_i \partial \xi_j } = F_{\varepsilon_{in},\varepsilon_{jn}} (\xi_i,\xi_j) e^{-\xi_i}e^{-\xi_j} (G^{ij}_i G^{ij}_j - G^{ij}_{ij})
\end{equation}
where 
\begin{equation}
\label{eq:giji}
G^{ij}_i = \frac{\partial G(\ldots,0,e^{-\xi_i},0,\ldots,0,e^{-\xi_j},0,\ldots)}{\partial y_i}
\end{equation}
and
\begin{equation}
\label{eq:gijij}
G^{ij}_{ij} = \frac{\partial^2 G(\ldots,0,e^{-\xi_i},0,\ldots,0,e^{-\xi_j},0,\ldots)}{\partial y_i \partial y_j }.
\end{equation}
In the general case, this double integral can only be computed
numerically. It is recommended to apply first the change of variables
$z_i=\exp(-\exp(-\xi_i))$ and $z_j=\exp(-\exp(-\xi_j))$.

\item Contrarily to the probit model, the variance-covariance
matrix does not characterize the distribution. Indeed, higher moments
of the MEV distribution exist, and different MEV models can share the
same variance-covariance (or the same correlation) matrix.
\item McFadden's original result was derived from the assumption that
  $G$ is a 1-homogeneous function. It is always possible through the normalization 
\begin{equation}
\label{eq:mev-normalization}
G(y_1,\ldots,y_{J})=G^*(y_1^{1/\mu},\ldots,y_{J}^{1/\mu})
\end{equation}
to convert a $\mu$-homogeneous function $G^*$ into a 1-homogeneous function $G$.
Indeed,
\[
\begin{array}{rcl}
G(\alpha y_1,\ldots,\alpha y_{J}) &=& G^*((\alpha y_1)^{1/\mu},\ldots,(\alpha y_{J})^{1/\mu})  \\
 &=& G^*(\alpha^{1/\mu} y_1^{1/\mu},\ldots,\alpha^{1/\mu} y_{J}^{1/\mu})   \\
 &=& (\alpha^{1/\mu})^\mu G^*(y_1^{1/\mu},\ldots,y_{J}^{1/\mu})  \\
 &=& \alpha G(y_1,\ldots,y_{J}), \\
\end{array}
\]
where the first and last equations use \req{eq:mev-normalization}, and the third is a consequence of the $\mu$-homogeneity of $G^*$.
In the choice model \req{eq:mev-mnlLike}, the arguments $y$ of the $G$ function (or its derivatives) are $e^V$. Therefore, the normalization $y^{1/\mu}$ is $e^{V/\mu}$, which amounts to rescale the $V$'s. 
 Like for the logit model, the $\mu$ parameter is not identified from
 data and can be normalized to one.

\item The logarithm of the CPGF  is the expected maximum utility
of the choice set for the  model, that is
\begin{equation}
\label{eq:cpgfExpMaxUtil}
\expect[\max_{j\in\C_n}U_{jn}] = \frac{1}{\mu}(\ln G(e^{V_{1n}},\ldots,e^{V_{J_nn}}) + \gamma),
\end{equation}
where $\gamma$ is Euler's constant. As utilities are defined up to a constant, it is common to ignore the $\gamma$. Also, the parameter $\mu$ is usually normalized to one.
Under this interpretation, the choice model can also be obtained from:
\[
P_n(i) = \frac{\partial \expect[\max_{j\in\C_n}U_{jn}]}{\partial V_{in}} , \; \forall i \in \C_n,
\]
which is
\begin{equation}
\label{eq:mev-form1}
P_n(i) = \frac{\partial  \ln G(e^{V_{1n}},\ldots,e^{V_{J_nn}})}{\partial V_{in}} = \frac{e^{V_{in}}G_i(e^{V_{1n}},\ldots,e^{V_{J_nn}})}{G(e^{V_{1n}},\ldots,e^{V_{J_nn}})},
\end{equation}
justifying the name ``choice probability generating function''.
Comparing \req{eq:mev-form1} with \req{eq:mev-mnlLike}, it is seen that $G$ must be such that
\begin{equation}
G(e^{V_{1n}},\ldots,e^{V_{J_nn}}) = \sum_j e^{V_{jn} + \ln G_j\left(e^{V_{1n}},\ldots,e^{V_{J_n n}}\right)}
\end{equation}
or, equivalently,
\begin{equation}
G(e^{V_{1n}},\ldots,e^{V_{J_nn}}) = \sum_j  e^{V_{jn} } G_j\left(e^{V_{1n}},\ldots,e^{V_{J_n n}}\right). 
\end{equation}
This latter condition actually characterizes homogeneous functions,
and is known as \emph{Euler's theorem}.
\item \label{item:9-networkMEV-inheritance} It can be shown
 that some operations maintain the properties
of  CPGF functions. Therefore, MEV functions can be constructed
from others, and they all correspond to valid choice models.  
The following results are adapted from the  \emph{inheritance theorem}
proposed by \citeasnoun{DalyBier06}. A MEV function which is
homogeneous of degree $\mu$ is called here a $\mu$-MEV function.


Consider a choice set $\C$ with $J$ alternatives. Consider also $M$
subsets of alternatives $\C_m$, $m=1,\ldots,M$, and let $J_m$ be the
number of alternatives in subset $m$. 
 Let $G^m:\mathbb{R}_+^{J_m}\longrightarrow \mathbb{R}$, $m=1,\ldots,M$ be $M$ $\mu_m$-MEV functions on $\C_m$.
Then, the function
\begin{equation}
\label{eq:rnev}
G:\mathbb{R}_+^{J}\longrightarrow \mathbb{R}: y \leadsto G(y) = \sum_{m=1}^M \left(\alpha_m G^m([y]_m)\right)^{\frac{\mu}{\mu_m}}
\end{equation}
is  a $\mu$-MEV function if $\alpha_m > 0$, $\mu > 0$ and $\mu_m \geq \mu$,
$m=1,\ldots,m,$ where  $[y]_m$ denotes a vector of dimension $J_m$ with entries  $y_i$, where the indices $i$ correspond to the elements in $\C_m$.

This result has some interesting corollaries. 
\begin{enumerate}
\item If $G(y)$ is a $\mu$-MEV function, so is $\alpha G(y)$, with $\alpha > 0$. The inheritance theorem can be invoked with $M=1$ and $\mu_m=\mu$.
\item If  $G(y)$ is a $\mu$-MEV function and $\widehat{\mu} \geq 1$, then $G(y^{\widehat{\mu}})^{1/\widehat{\mu}}$ is also a $\mu$-MEV function. Indeed, $G^*(y)=G(y^{\widehat{\mu}})$ is a $(\mu \widehat{\mu})$-MEV function. By the theorem,
\[
G^*(y)^{\frac{\mu}{\mu \widehat{\mu}}} = G(y^{\widehat{\mu}})^{\frac{1}{\widehat{\mu}}}
\]
is a $\mu$-MEV function, as $\mu \widehat{\mu} \geq \mu$.
\item Any linear combination of $\mu$-MEV functions is also a $\mu$-MEV function if the multipliers are non negative and at least one is strictly positive.
\item If $P_m(i)$ is the choice model derived from the $\mu_m$-MEV function $G^m$, then the choice model $P(i)$ derived from the $\mu$-MEV function $G$ defined by \req{eq:rnev} is
\begin{equation}
\label{eq:probaInheritance}
P(i) = \sum_{m=1}^M \frac{(\alpha_m G^m(e^V))^{\frac{\mu}{\mu_m}}}{\sum_{p=1}^M (\alpha_p G^p(e^V))^{\frac{\mu}{\mu_p}}} P_m(i).
\end{equation}
\end{enumerate}
\end{itemize}

\section{The logit model as  MEV}
\label{sec:9-mevMNL}

The  logit model is  a MEV model derived from the following choice probability generating function:
\begin{equation}
\label{eq:mev-mnl-mu}
G(y)= \sum_{i=1}^J y_i^\mu.
\end{equation}
Properties [M1]--[M3] are trivially verified:
\begin{description}
\item[M1] The strong alternating sign property  is a consequence of the fact that $\mu > 0$, $y_i > 0$ and 
\begin{equation}
G_i(y) = \frac{\partial G}{\partial y_i} = \mu y_i^{\mu-1}.
\end{equation}
Derivatives of higher orders are all zero:
\[
 G_{ij}(y) = \frac{\partial^2 G}{\partial y_i \partial y_j} =0, \text{ if } i \neq j.
\]
\item[M2] The function is $\mu$-homogeneous as
\[
G(\alpha y) = \sum_{i=1}^J (\alpha y_i)^\mu = \alpha^\mu \sum_{i=1}^J  y_i^\mu = \alpha^\mu G(y).
\]
\item[M3] The limit property is also verified, as
\[
\begin{array}{rcl}
G(y_1,\ldots,y_{j-1},+\infty,y_{j+1},\ldots,y_J) &=& \displaystyle\lim_{y_j\to+\infty} \sum_{i=1}^J y_i^\mu \\ \\&=& \displaystyle\sum_{i\neq j} y_i^\mu + \lim_{y_j\to+\infty} y_j^\mu = +\infty.
\end{array}
\]
 \end{description}

Consequently, \req{eq:mev-mnl-mu} defines a $\mu$-MEV function. From \req{eq:mevCdf}, the CDF is 
\[
\begin{array}{rcl}
F_{\varepsilon}(\xi_1,\ldots,\xi_J) &=& e^{-G(e^{-\xi_1},\ldots,e^{-\xi_J})} \\
                  &=& e^{-\sum_{i=1}^J e^{-\mu\xi_i}} \\
                  &=& \prod_{i=1}^J e^{-e^{-\mu\xi_i}}.
\end{array}
\]
Substituting
\[
e^{V_{i}+\ln G_i(e^{V_{1}},\ldots,e^{V_{J}})} = e^{V_{i}+\ln \mu + (\mu-1)\ln e^{V_i}}= e^{\ln \mu + \mu V_{i}}
\]
into \req{eq:mev-mnlLike}, we obtain the choice probability 
\[
P(i) =  \frac{e^{\ln \mu + \mu V_{in}}}{\sum_{j\in \C} e^{\ln \mu + \mu V_{jn}}}=\frac{e^{\mu V_{in}}}{\sum_{j\in \C} e^{\mu V_{jn}}},
\]
which is indeed the choice probability of a logit
model. 

From \req{eq:cpgfExpMaxUtil}, the expected maximum utility is
\[
\frac{1}{\mu}\ln G\left(e^{V_{1n}},\ldots,e^{V_{{J}n}}\right) = \frac{1}{\mu}
\ln\displaystyle\sum_{i=1}^{J} e^{\mu V_{in}},
\]
sometimes called the ``logsum'' formula.

We also illustrate the computation of the covariance from \req{eq:covMev}. We consider two distinct alternatives $i$ and $j$. The double integral in \req{eq:covMev} writes
\[
\begin{array}{l}
\displaystyle \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}\xi_i \xi_j \frac{\partial^2 F_{\varepsilon_i,\varepsilon_j} (\xi_i,\xi_j)}{\partial \xi_i \partial \xi_j } d\xi_i d\xi_j = \\ \\
\displaystyle \int_{-\infty}^{+\infty} \xi_i e^{-e^{-\xi_i}} e^{-\xi_i} d\xi_i\int_{-\infty}^{+\infty}\xi_j  e^{-e^{-\xi_j}}e^{-\xi_j} d\xi_j,
\end{array}
\]
where index $n$ has been dropped for notational convenience.
Applying the change of variable   $t_i=e^{-\xi_i}$, we have
\[
 \int_{-\infty}^{+\infty} \xi_i e^{-e^{-\xi_i}} e^{-\xi_i} d\xi_i = -\int_0^{+\infty} \ln t_i e^{-t_i} dt_i = \gamma,
\]
so that 
\begin{equation}
\label{eq:8.28}
\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}\xi_i \xi_j \frac{\partial^2 F_{\varepsilon_i,\varepsilon_j} (\xi_i,\xi_j)}{\partial \xi_i \partial \xi_j } d\xi_i d\xi_j = \gamma^2.
\end{equation}
 Using \req{eq:8.28} in \req{eq:covMev}, we obtain that the covariance between any pair of alternatives is 0, which is expected for the logit model.


\section{The nested logit model as  MEV}
\label{sec:9-mevNL}


We consider a nested logit model, where the choice set $\C$ is partitioned into
mutually exclusive nests $\C_m$, $m=\ldots, M$.
The nested logit model with $M$ nests is  a MEV model, with CPGF 
\begin{equation}
\label{eq:mev-nl}
G(y)= \sum_{m=1}^M \left( \sum_{\ell \in \C_m} y_\ell^{\mu_m} \right)^{\mu/\mu_m}.
\end{equation}
If $0 < \mu \leq \mu_m$, for all $m$, \req{eq:mev-nl} defines  a $\mu$-MEV function. Properties [M2] and [M3] are trivially verified.  In order to  check the strong alternating sign property [M1], we consider alternative $i$ in nest $m$. We have
\begin{equation}
\label{eq:gi-nl}
G_i(y) = \frac{\partial G}{\partial y_i}(y) = \mu y_i^{{\mu_{m}} - 1} \left(\sum_{\ell \in \C_m} y_\ell^{\mu_{m}}\right)^{\frac{\mu}{\mu_{m}}-1}.
\end{equation}
It is non-negative when $y\in\R^J_+$. Consider now another alternative $j\neq i$. If $j$ does not belong to the same nest as $i$, then
\begin{equation}
\label{eq:MEVnestedHessian2}
\frac{\partial^2 G}{\partial y_i y_j}(y) = 0.
\end{equation}
If $j$ belongs to $m$, we have
\begin{equation}
\label{eq:MEVnestedHessian}
\frac{\partial^2 G}{\partial y_i y_j}(y) = \mu \mu_m (\frac{\mu}{\mu_{m}}-1 )y_i^{{\mu_{m}} -1}y_j^{{\mu_{m}} -1}
\left(\sum_{\ell\in \C_m} y_\ell^{\mu_m}\right)^{\frac{\mu}{\mu_m}-2}.
\end{equation}
We need to verify that \req{eq:MEVnestedHessian} is non-positive. It is the case as $\mu > 0$ and $\mu \leq \mu_m$. Indeed, all terms in the product are
non-negative, except for the term $(\mu/\mu_m-1)$, which is
non-positive.  Each additional differentiation  involves an additional factor of the form $(\mu/\mu_m - k)$,  $k>1$, which is always negative. Therefore, each differentiation leads to a change of sign, and [M1] is verified.

We now derive the covariance between the error terms of two alternatives $i$ and $j$ in nest $m$. We first
normalize all utilities by $\mu$ is order to transform $G$ into a
1-homogenous function, without loss of generality, as suggested by \req{eq:mev-normalization}. Then we consider the bivariate marginal cumulative distribution 
\begin{align*}
F_{\varepsilon_n} (\xi_i,\xi_j) &= F_{\varepsilon_n}(\ldots,+\infty, \xi_i/\mu,+\infty,\ldots,+\infty,\xi_j/\mu,+\infty,\ldots)  \\
&= \exp\left(-G(\ldots,0, \exp(-\xi_i/\mu),0,\ldots,0,\exp(-\xi_j/\mu),0,\ldots)\right)  \\
&= \exp\left(-\left(\exp(-\frac{\mu_m}{\mu}\xi_i) + \exp(-\frac{\mu_m}{\mu}\xi_i) \right)^\frac{\mu}{\mu_m}\right).
\end{align*}
This is the CDF of a bivariave logistic model (see \cite[p. 628]{KotzBalaJohn2000}) with parameter $m=\mu_m/\mu$ and correlation
\begin{equation}
\rho = 1-m^{-2} = 1-\frac{\mu^2}{\mu_m^2}.
\end{equation}

Finally, we derive the choice model. 
 For each nest $m$, we denote
\[
S_m = \sum_{j \in \C_m} e^{\mu_m V_{jn}},
\]
so that
\[
G = \sum_m S_m^{\frac{\mu}{\mu_m}},
\]
and
\[
G_i = \mu e^{\mu_m V_{in}} e^{-V_{in}} S_m^{{\frac{\mu}{\mu_m}}-1}.
\]
We use \req{eq:mev-orig} to obtain
\[
\begin{aligned}
P_n(i)&= \frac{e^{V_{in}}\mu e^{\mu_m V_{in}} e^{-V_{in}}
  S_m^{{\frac{\mu}{\mu_m}}-1}}{\mu \sum_\ell S_\ell^{\frac{\mu}{\mu_\ell}}} \\
 &= \frac{e^{\mu_m V_{in}} 
  S_m^{{\frac{\mu}{\mu_m}}-1}}{ \sum_\ell S_\ell^{\frac{\mu}{\mu_\ell}}} \\
 &= \frac{e^{\mu_m V_{in}}}{S_m} \frac{S_m^{{\frac{\mu}{\mu_m}}}}{ \sum_\ell
  S_\ell^{\frac{\mu}{\mu_\ell}}} \\
&=\frac{e^{\mu_m V_{in}}}{\sum_{j \in \C_m} e^{\mu_m V_{jn}}} \frac{(\sum_{j \in \C_m} e^{\mu_m V_{jn}})^{{\frac{\mu}{\mu_m}}}}{ \sum_\ell
  (\sum_{j \in \C_\ell} e^{\mu_\ell V_{jn}})^{\frac{\mu}{\mu_\ell}}},
\end{aligned}
\]
which is the nested logit model.

The multiple level nested logit model is also a MEV model. Consider for instance a 3-level case, where the choice set is partitioned into $p$ groups, each of them partitioned into $M_p$ nests.  The model is derived from the following CPGF:
\begin{equation}
\label{eq:mev-mlnl}
G(y)= \sum_{p=1}^P \left(  \sum_{m=1}^{M_p} \left( \sum_{i=1}^{J_{mp}}  y_i^{\mu_{mp}} \right)^{\mu_p/\mu_{mp}}  \right)^{\mu/\mu_p},
\end{equation}
where $J_{mp}$ is the number of alternatives in the $m$\th\ nest within group $p$. It can be verified that the condition:
\[
0 \leq  \mu \leq \mu_m \leq \mu_{pm}, \text{ for all } m, p,
\]
 is sufficient for \req{eq:mev-mlnl} to define a CPGF function.



\section{The cross nested logit model}
\label{sec:9-mevCNL}


If we consider $M$ nests, the cross nested logit model is a MEV model based on the following CPGF:
\begin{equation}
\label{eq:mev-cnl}
G(y)= \sum_{m=1}^M \left( \sum_{j=1}^{J} \alpha_{jm}^{\frac{\mu_m}{\mu}} y_j^{\mu_m} \right)^{\mu/\mu_m},
\end{equation}
where $\mu_m$ is a parameter associated with nest $m$ (it plays a
similar role as the $\mu_m$ parameter in the nested logit model), and
$\alpha_{jm}$ are  parameters capturing the level of membership of alternative $j$ in nest $m$.  We immediately note that the nested logit model is a special instance of the cross nested logit model where $\alpha_{im} = 1$ if alternative $i$ belongs to nest $m$, and $\alpha_{im}=0$ otherwise. For this reason,  \citeasnoun{WenKopp01} prefer to call the model the \emph{Generalized Nested Logit Model}, although this terminology does not prevail in the literature.

\citeasnoun{Bier06-cnl} has shown that the conditions
\begin{enumerate}
\item\label{hyp1} $\alpha_{im} \geq 0$, $\forall i,m$,
\item\label{hyp2} $\sum_m \alpha_{im} > 0$, $\forall i$, and
\item\label{hyp3} $0 < \mu \leq \mu_m$, $\forall m$,
\end{enumerate}
 are sufficient for \req{eq:mev-cnl} to be a CPGF.
Note that condition \ref{hyp3} is the same as for the nested logit
model (see Section~\ref{sec:9-mevNL}). 

The CNL model must be normalized before being estimated. As any MEV model, the normalization $\mu=1$ is applicable. Moreover, if the $\alpha$ parameters are estimated, not all of them  are identified and the following normalization is
appropriate:
\begin{equation}
\label{eq:sumAlphaNormalized}
\sum_{m=1}^M \alpha_{im} = 1, \;\;\; \forall i=1,\ldots,J.
\end{equation}
This normalization is consistent with the interpretation of $\alpha_{im}$ as the level of membership of alternative $i$ in nest $m$. 
The derivative of \req{eq:mev-cnl} is
\begin{equation}
\label{eq:mev-cnl-deriv}
G_i(y) = \frac{\partial G(y)}{\partial y_i} = \mu  \sum_{m=1}^M \alpha_{im}^{\frac{\mu_m}{\mu}} y_i^{\mu_m-1}\left( \sum_{j=1}^{J} \alpha_{jm}^{\frac{\mu_m}{\mu}} y_j^{\mu_m} \right)^{\frac{\mu}{\mu_m}-1}.
\end{equation}


Substituting \req{eq:mev-cnl-deriv} in \req{eq:mev-mnlLike}, we obtain the
cross nested logit model:
\begin{equation}
\label{eq:CNLmodel}
P_n(i)  = \sum_{m=1}^M \frac{\left(\sum_{j\in\mathcal{C}_n}{\alpha_{jm}^{\mu_m/\mu} e^{\mu_m V_{jn}}}\right)^{\frac{\mu}{\mu_m}}}{\sum_{p=1}^M \left(\sum_{j\in\mathcal{C}_n}{\alpha_{jp}^{\mu_p/\mu} e^{\mu_p V_{jn}}}\right)^{\frac{\mu}{\mu_p}}} \; \frac{\alpha_{im}^{\mu_m/\mu}e^{\mu_m V_{in}}}{\sum_{j\in\mathcal{C}_n} \alpha_{jm}^{\mu_m/\mu}e^{\mu_m V_{jn}}},
\end{equation}
which can nicely be interpreted as
\begin{equation}
P_n(i) = \sum_{m=1}^M P_n(m|\mathcal{C}_n) P_n(i|m),
\end{equation}
where
\begin{equation}
P_n(i|m) = \frac{\alpha_{im}^{\mu_m/\mu}e^{\mu_m V_{in}}}{\sum_{j\in\mathcal{C}_n} \alpha_{jm}^{\mu_m/\mu}e^{\mu_m V_{jn}}},
\end{equation}
is the choice probability conditional to nest $m$, and
\begin{equation}
P_n(m|\mathcal{C}_n) = \frac{\left(\sum_{j\in\mathcal{C}_n}{\alpha_{jm}^{\mu_m/\mu} e^{\mu_m V_{jn}}}\right)^{\frac{\mu}{\mu_m}}}{\sum_{p=1}^M \left(\sum_{j\in\mathcal{C}_n}{\alpha_{jp}^{\mu_p/\mu} e^{\mu_p V_{jn}}}\right)^{\frac{\mu}{\mu_p}}},
\end{equation}
is the probability associated with nest $m$.

The choice model can also be written in a form where the utilities are shifted:
\begin{equation}
P_n(i)  = \sum_{m=1}^M \frac{\left(\sum_{j\in\mathcal{C}_n}{\exp(\mu_m (V_{jn}+\frac{1}{\mu}\ln \alpha_{jm}))}\right)^{\frac{\mu}{\mu_m}}}{\sum_{p=1}^M \left(\sum_{j\in\mathcal{C}_n}{\exp(\mu_p (V_{jn}+\frac{1}{\mu}\ln \alpha_{jp}))}\right)^{\frac{\mu}{\mu_p}}} \; \frac{e^{\mu_m (V_{in}+\frac{1}{\mu}\ln \alpha_{im})}}{\sum_{j\in\mathcal{C}_n} e^{\mu_m (V_{jn}+\frac{1}{\mu}\ln \alpha_{jm})}}.
\end{equation}

The correlation structure  must be computed using
\req{eq:covMev}, where
\begin{eqnarray}
\label{eq:Fijcnl}
F_{\varepsilon_i,\varepsilon_j}(\xi_i,\xi_j) =
\exp\left(-\sum_{m=1}^M \left( {(\alpha_{im}^{\frac{1}{\mu}}
e^{-\xi_i})}^{\mu_{m}} +  {(\alpha_{jm}^{\frac{1}{\mu}} e^{-\xi_j})}^{\mu_{m}}  \right)^\frac{1}{\mu_{m}}\right).
\end{eqnarray}

The cross nested logit model provides an intuitive way to capture
complex correlation structures. Indeed, any source of correlation
assumed by the analyst can be represented by a nest, and the
alternatives involved are associated with the nest. As each
alternative can potentially belong to more than one nest, a great deal
of flexibility is provided. Actually, \citeasnoun{FosgMcFaBier13} have
shown than any additive random utility model can be approximated by a
cross nested logit model.


\section{Derivation of the MEV model}
\label{sec:derivationMEV}

The  choice model is obtained by incorporating
\req{eq:mevCdf}
\[
F_{\varepsilon_n}(\varepsilon_{1n},\ldots,\varepsilon_{Jn}) =
e^{-G(e^{-\varepsilon_{1n}},\ldots,e^{-\varepsilon_{Jn}})},
\]
 into the general definition of a choice model from random utility theory:
\[
P_n(i) = \int_{\varepsilon=-\infty}^{+\infty} \frac{\partial F_{\varepsilon_{1n},\varepsilon_{2n},\ldots,\varepsilon_{J_n}}}{\partial \varepsilon_{i}}(\ldots,V_{in}-V_{(i-1)n}+\varepsilon,\varepsilon,V_{in}-V_{(i+1)n}+\varepsilon,\ldots)d\varepsilon.
\]
We have
\[
\begin{array}{l}
\displaystyle \frac{\partial F_{\varepsilon_{1n},\varepsilon_{2n},\ldots,\varepsilon_{J_n}}}{\partial \varepsilon_{i}}(\ldots,V_{in}-V_{(i-1)n}+\varepsilon,\varepsilon,V_{in}-V_{(i+1)n}+\varepsilon,\ldots) \\ \\ = e^{-\varepsilon} G_i(\ldots,e^{-V_{in}+V_{(i-1)n}-\varepsilon},e^{-\varepsilon},e^{-V_{in}+V_{(i+1)n}-\varepsilon},\ldots) \\
\multicolumn{1}{r}{\exp\left(-G(\ldots,e^{-V_{in}+V_{(i-1)n}-\varepsilon},e^{-\varepsilon},e^{-V_{in}+V_{(i+1)n}-\varepsilon},\ldots) \right)} \\ \\
 = e^{-\varepsilon} e^{-(\mu-1)\varepsilon} e^{-(\mu-1)V_{in}}G_i(\ldots,e^{V_{(i-1)n}},e^{V_in},e^{V_{(i+1)n}},\ldots) \\
\multicolumn{1}{r}{\exp\left(-e^{-\mu\varepsilon} e^{-\mu V_{in}}G(\ldots,e^{V_{(i-1)n}},e^{V_{in}},e^{V_{(i+1)n}},\ldots) \right)},
\end{array}
\]
because $G$ is $\mu$-homogeneous, which implies that $G_i$ is $(\mu-1)$-homogeneous.
We now denote 
\[
e^V = \left(\ldots,e^{V_{(i-1)n}},e^{V_{in}},e^{V_{(i+1)n}},\ldots\right),
\]
 and simplify the terms to obtain
\[
\begin{array}{l}
\displaystyle \frac{\partial F_{\varepsilon_{1n},\varepsilon_{2n},\ldots,\varepsilon_{J_n}}}{\partial \varepsilon_{i}}(\ldots,V_{in}-V_{(i-1)n}+\varepsilon,\varepsilon,V_{in}-V_{(i+1)n}+\varepsilon,\ldots) \\ \\ = e^{-\mu\varepsilon} e^{-\mu V_{in}} e^{V_{in}}G_i(e^V)
\exp\left(-e^{-\mu\varepsilon} e^{-\mu V_{in}}G(e^V) \right).
\end{array}
\]
Therefore,
\[
P_n(i) = e^{-\mu V_{in}}e^{V_{in}} G_i(e^V) \int_{\varepsilon=-\infty}^{+\infty} e^{-\mu\varepsilon} \exp\left(-e^{-\mu\varepsilon}e^{-\mu V_{in}}G(e^V)\right) d\varepsilon.
\]
Defining $t=-\exp(-\mu\varepsilon)$, so that $dt = \mu\exp(-\mu\varepsilon)d\varepsilon$, we write
\[
P_n(i) = e^{-\mu V_{in}}e^{V_{in}} G_i(e^V) \frac{1}{\mu}\int_{t=-\infty}^{0} \exp\left( t e^{-\mu V_{in}}G(e^V)\right) dt,
\]
which simplifies to
\[
P_n(i)=\frac{e^{V_{in}}G_i(e^V)}{\mu G(e^V)}.
\]
We finally invoke Euler's theorem that characterizes homogeneous functions to obtain \req{eq:mev-mnlLike}:
\[
P_n(i) = \frac{e^{V_{in} + \ln G_i\left(e^V\right)}}{\sum_j e^{V_{jn} + \ln G_j\left(e^V\right)}}.
\]



\section{Copulas}
\label{sec:copulas}


The concept of MEV functions is closely related to the concept of copulas
in statistics.  A copula is the CDF of a multivariate distribution such that
every marginal distribution is uniform in the interval $[0,1]$.  We
refer the reader to \citeasnoun{Nels2006} for an introduction to
copulas. A result by \citeasnoun{sklar1959fonctions} states that any multivariate distribution is entirely characterized by its marginals, and a copula. Intuitively, the copula captures the dependence among the various dimensions. More precisely, consider the CDF of a multivariate random vector $\varepsilon$
\[
F(\varepsilon_1,\ldots,\varepsilon_J)
\]
and denote $F_j(\varepsilon_j)$ the CDF of its univariate marginal distribution associated with dimension $j$. The copula of $F$ is defined as
\[
C: [0,1]^J \to \R : (u_1,\ldots,u_J) \to C(u_1,\ldots,u_J) = F\left(F_1^{-1}(u_1),\ldots, F_J^{-1}(u_J)\right),
\]
where $F_j^{-1}:[0,1]\to\R$ denotes the inverse function of the marginals.
Conversely, given a copula $C$, multivariate distributions can be constructed from the marginal distributions $F_j(\varepsilon_j)$:
\[
F(\varepsilon_1,\ldots,\varepsilon_J) = C\left(F_1(\varepsilon_1),\ldots,F_J(\varepsilon_J)\right).
\]
The link between copulas and MEV function is given by the following
result (\cite{Joe1997}): a multivariate random variable with CDF
$F(\varepsilon_1,\ldots,\varepsilon_J)$ has a MEV distribution if and
only if its copula satisfies the following condition:
\[
C(u_1,\ldots,u_J)^\alpha = C(u_1^\alpha,\ldots, u_J^\alpha),
\] 
for $u \in [0,1]^J$ and $\alpha > 0$. Actually, $\ln C$ plays the role of the choice
probability generating function defined above.


These results, although quite technical, can be exploited to generate
new MEV models from the theory of copulas. We refer the interested
reader to \citeasnoun{NikoKarl2008}, \citeasnoun{BhatSene2009} or
\citeasnoun{FosgMcFaBier13} for more details.

\section{Derivatives of the CDF of the cross-nested logit model}

We write \req{eq:mev-cnl} as
\[
G(y)= \sum_{m=1}^M \left( \sum_{k=1}^{J} t_{km} \right)^{\mu/\mu_m},
\]
where
\[
t_{km} = \alpha_{km}^{\frac{\mu_m}{\mu}} y_k^{\mu_m}.
\]
Therefore,
\begin{align*}
G_i = \frac{d G}{d y_i} &= \sum_m  \frac{\partial G}{\partial t_{im}} \frac{\partial
  t_{im}}{\partial y_i} \\
&= \sum_m  \frac{\mu}{\mu_m} \left(\sum_{k=1}^{J} t_{km} \right)^{\frac{\mu}{\mu_m}-1}\mu_m \alpha_{im}^{\frac{\mu_m}{\mu}} y_i^{\mu_m-1} \\
&= \mu \sum_m  \left(\sum_{k=1}^{J} t_{km} \right)^{\frac{\mu}{\mu_m}-1} \alpha_{im}^{\frac{\mu_m}{\mu}} y_i^{\mu_m-1}. 
\end{align*}
Moreover, if $i \neq j$,
\begin{align*}
G_{ij} = \frac{\partial G_i}{\partial y_j} &=  \sum_m  \frac{\partial G_i}{\partial t_{jm}} \frac{\partial
  t_{jm}}{\partial y_j} \\
 &=  \sum_m  \mu (\frac{\mu}{\mu_m}-1)
\left(\sum_{k=1}^{J} t_{km} \right)^{\frac{\mu}{\mu_m}-2}
\alpha_{im}^{\frac{\mu_m}{\mu}} y_i^{\mu_m-1} \mu_m
\alpha_{jm}^{\frac{\mu_m}{\mu}} y_j^{\mu_m-1} \\
 &= \mu  \sum_m  (\mu-\mu_m)
\left(\sum_{k=1}^{J} t_{km} \right)^{\frac{\mu}{\mu_m}-2}
\alpha_{im}^{\frac{\mu_m}{\mu}} y_i^{\mu_m-1} \alpha_{jm}^{\frac{\mu_m}{\mu}} y_j^{\mu_m-1}.
\end{align*}
Finally,
\begin{align*}
G_{ii} = \frac{d G_i}{d y_i} =&  \sum_m  \frac{\partial G_i}{\partial t_{im}} \frac{\partial
  t_{im}}{\partial y_i} + \frac{\partial G_i}{y_i} \\
=&\mu  \sum_m  (\mu-\mu_m)
\left(\sum_{k=1}^{J} t_{km} \right)^{\frac{\mu}{\mu_m}-2}
\alpha_{im}^{\frac{\mu_m}{\mu}} y_i^{\mu_m-1}
\alpha_{im}^{\frac{\mu_m}{\mu}} y_i^{\mu_m-1} \\
&+ (\mu_m-1)  \left(\sum_{k=1}^{J} t_{km} \right)^{\frac{\mu}{\mu_m}-1} \alpha_{im}^{\frac{\mu_m}{\mu}} y_i^{\mu_m-2}. 
\end{align*}

The derivatives of the CDF are calculated as
\begin{align*}
F_i = \frac{\partial F}{\partial \xi_i} &= \frac{\partial F}{\partial G}
\frac{\partial G}{\partial y_i} \frac{\partial y_i}{\partial \xi_i} \\
&= -e^{-G} G_i (-e^{-\xi_i}) \\
&= F G_i e^{-\xi_i},
\end{align*}
and, if $i\neq j$,
\begin{align*}
F_{ij} = \frac{\partial F_i}{\partial \xi_j} &= \frac{\partial F}{\partial \xi_j} G_i
e^{-\xi_i} + F \frac{\partial G_i}{\partial y_j} \frac{\partial
  y_j}{\partial \xi_j}e^{-\xi_i} \\
 &= F_j G_i e^{-\xi_i} - F G_{ij} e^{-\xi_i} e^{-\xi_j} \\
&= F G_j e^{-\xi_j} G_i e^{-\xi_i} - F G_{ij} e^{-\xi_i} e^{-\xi_j} \\
&= F e^{-\xi_i} e^{-\xi_j} (G_i G_j - G_{ij} ). \\
\end{align*}
Finally,
\[
F_{ii} = F e^{-\xi_i} e^{-\xi_i} (G_i G_i - G_{ii} ) - F G_i e^{-\xi_i}.
\]

\bibliographystyle{dcu}
\bibliography{../dca}





\end{document}





