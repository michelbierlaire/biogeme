\documentclass[12pt,a4paper]{article}

% Package to include code
\usepackage{listings}
\usepackage{color}
\usepackage{tikz}
\usepackage{pgfplots}
\lstset{language=Python}
\lstset{numbers=none, basicstyle=\footnotesize,
  numberstyle=\tiny,keywordstyle=\color{blue},stringstyle=\ttfamily,showstringspaces=false}
\lstset{backgroundcolor=\color[rgb]{0.95 0.95 0.95}}
\lstdefinestyle{numbers}{numbers=left, stepnumber=1,
  numberstyle=\tiny,basicstyle=\tiny, numbersep=10pt}
\lstdefinestyle{nonumbers}{numbers=none}


% Font selection: uncomment the next line to use the ``beton'' font
%\usepackage{beton}

% Font selection: uncomment the next line to use the ``times'' font
%\usepackage{times}

% Font for equations
\usepackage{euler}


%Package to define the headers and footers of the pages
\usepackage{fancyhdr}


%Package to include an index
\usepackage{index}

%Package to display boxes around texts. Used especially for the internal notes.
\usepackage{framed}

%PSTricks is a collection of PostScript-based TEX macros that is compatible
% with most TEX macro packages
\usepackage{pstricks}
\usepackage{pst-node}
\usepackage{pst-plot}
\usepackage{pst-tree}

%Package to display boxes around a minipage. Used especially to
%describe the biography of people.
\usepackage{boxedminipage}

%Package to include postscript figures
\usepackage{epsfig}

%Package for the bibliography
% \cite{XXX} produces Ben-Akiva et. al., 2010
% \citeasnoun{XXX} produces Ben-Akiva et al. (2010)
% \citeasnoun*{XXX} produces Ben-Akiva, Bierlaire, Bolduc and Walker (2010)
\usepackage[dcucite,abbr]{harvard}
\harvardparenthesis{none}\harvardyearparenthesis{round}

%Packages for advanced mathematics typesetting
\usepackage{amsmath,amsfonts,amssymb}

%Package to display trees easily
%\usepackage{xyling}

%Package to include smart references (on the next page, on the
%previous page, etc.)
%%

%% Remove as it is not working when the book will be procesed by the
%% publisher.
%\usepackage{varioref}

%Package to display the euro sign
\usepackage[right,official]{eurosym}

%Rotate material, especially large table (defines sidewaystable)
\usepackage[figuresright]{rotating}

%Defines the subfigure environment, to obtain refs like Figure 1(a)
%and Figure 1(b).
\usepackage{subfigure}

%Package for appendices. Allows subappendices, in particular
\usepackage{appendix}

%Package controling the fonts for the captions
\usepackage[font={small,sf}]{caption}

%Defines new types of columns for tabular ewnvironment
\usepackage{dcolumn}
\newcolumntype{d}{D{.}{.}{-1}}
\newcolumntype{P}[1]{>{#1\hspace{0pt}\arraybackslash}}
\newcolumntype{.}{D{.}{.}{9.3}}

%Allows multi-row cells in tables
\usepackage{multirow}

%Tables spaning more than one page
\usepackage{longtable}


%%
%%  Macros by Michel
%%

\newcommand{\PBIOGEME}{PythonBiogeme}
\newcommand{\PDBIOGEME}{PandasBiogeme}
\newcommand{\BIOGEME}{Biogeme}
\newcommand{\BBIOGEME}{BisonBiogeme}


%Internal notes
\newcommand{\note}[1]{
\begin{framed}{}%
\textbf{\underline{Internal note}:} #1
\end{framed}}

%Use this version to turn off the notes
%\newcommand{\note}[1]{}


%Include a postscript figure . Note that the label is prefixed with
%``fig:''. Remember it when you refer to it.
%Three arguments:
% #1 label
% #2 file (without extension)
% #3 Caption
\newcommand{\afigure}[3]{%
\begin{figure}[!tbp]%
\begin{center}%
\epsfig{figure=#2,width=0.8\textwidth}%
\end{center}
\caption{\label{fig:#1} #3}%
\end{figure}}






%Include two postscript figures side by side.
% #1 label of the first figure
% #2 file for the first figure
% #3 Caption for the first figure
% #4 label of the second figure
% #5 file for the second figure
% #6 Caption for the first figure
% #7 Caption for the set of two figures
\newcommand{\twofigures}[7]{%
\begin{figure}[htb]%
\begin{center}%
\subfigure[\label{fig:#1}#3]{\epsfig{figure=#2,width=0.45\textwidth}}%
\hfill
\subfigure[\label{fig:#4}#6]{\epsfig{figure=#5,width=0.45\textwidth}}%
\end{center}
\caption{#7}%
\end{figure}}

%Include a figure generated by gnuplot using the epslatex output. Note that the label is prefixed with
%``fig:''. Remember it when you refer to it.

%Three arguments:
% #1 label
% #2 file (without extension)
% #3 Caption
\newcommand{\agnuplotfigure}[3]{%
\begin{figure}[!tbp]%
\begin{center}%
\input{#2}%
\end{center}
\caption{\label{fig:#1} #3}%
\end{figure}}

%Three arguments:
% #1 label
% #2 file (without extension)
% #3 Caption
\newcommand{\asidewaysgnuplotfigure}[3]{%
\begin{sidewaysfigure}[!tbp]%
\begin{center}%
\input{#2}%
\end{center}
\caption{\label{fig:#1} #3}%
\end{sidewaysfigure}}


%Include two postscript figures side by side.
% #1 label of the first figure
% #2 file for the first figure
% #3 Caption for the first figure
% #4 label of the second figure
% #5 file for the second figure
% #6 Caption for the second figure
% #7 Caption for the set of two figures
% #8 label for the whole figure
\newcommand{\twognuplotfigures}[7]{%
\begin{figure}[htb]%
\begin{center}%
\subfigure[\label{fig:#1}#3]{\input{#2}}%
\hfill
\subfigure[\label{fig:#4}#6]{\input{#5}}%
\end{center}
\caption{#7}%
\end{figure}}



%Include the description of somebody. Four arguments:
% #1 label
% #2 Name
% #3 file (without extension)
% #4 description
\newcommand{\people}[4]{
\begin{figure}[tbf]
\begin{boxedminipage}{\textwidth}
\parbox{0.40\textwidth}{\epsfig{figure=#3,width = 0.39\textwidth}}%\hfill
\parbox{0.59\textwidth}{%
#4%
}%
\end{boxedminipage}
\caption{\label{fig:#1} #2}
\end{figure}
}

%Default command for a definition
% #1 label (prefix def:)
% #2 concept to be defined
% #3 definition
\newtheorem{definition}{Definition}
\newcommand{\mydef}[3]{%
\begin{definition}%
\index{#2|textbf}%
\label{def:#1}%
\textbf{#2} \slshape #3\end{definition}}

%Reference to a definitoin. Prefix 'def:' is assumed
\newcommand{\refdef}[1]{definition~\ref{def:#1}}


%Default command for a theorem, with proof
% #1: label (prefix thm:)
% #2: name of the theorem
% #3: statement
% #4: proof
\newtheorem{theorem}{Theorem}
\newcommand{\mytheorem}[4]{%
\begin{theorem}%
\index{#2|textbf}%
\index{Theorems!#2}%
\label{thm:#1}%
\textbf{#2} \sffamily \slshape #3
\end{theorem} \bpr #4 \epr \par}


%Default command for a theorem, without proof
% #1: label (prefix thm:)
% #2: name of the theorem
% #3: statement
\newcommand{\mytheoremsp}[3]{%
\begin{theorem}%
\index{#2|textbf}%
\index{Theorems!#2}%
\label{thm:#1}%
\textbf{#2} \sffamily \slshape #3
\end{theorem}}



%Put parentheses around the reference, as standard for equations
\newcommand{\req}[1]{(\ref{#1})}

%Short cut to make a column vector in math environment (centered)
\newcommand{\cvect}[1]{\left(\begin{array}{c} #1 \end{array} \right) }

%Short cut to make a column vector in math environment (right justified)
\newcommand{\rvect}[1]{\left(\begin{array}{r} #1 \end{array} \right) }

%A reference to a theorem. Prefix thm: is assumed for the label.
\newcommand{\refthm}[1]{Theorem~\ref{thm:#1}}

%Reference to a figure. Prefix fig: is assumed for the label.
\newcommand{\reffig}[1]{Figure~\ref{fig:#1}}

%Smart reference to a figure. Prefix fig: is assumed for the label.
%\newcommand{\vreffig}[1]{Figure~\vref{fig:#1}}

%C in mathcal font for the choice set
\newcommand{\C}{\mathcal{C}}

%R in bold font for the set of real numbers
\newcommand{\R}{\mathbb{R}}

%N in bold font for the set of natural numbers
\newcommand{\N}{\mathbb{N}}

%C in mathcal font for the log likelihood
\renewcommand{\L}{\mathcal{L}}

%S in mathcal font for the subset S
\renewcommand{\S}{\mathcal{S}}

%To write an half in math envionment
\newcommand{\half}{\frac{1}{2}}

%Probability
\newcommand{\prob}{\operatorname{Pr}}

%Expectation
\newcommand{\expect}{\operatorname{E}}

%Variance
\newcommand{\var}{\operatorname{Var}}

%Covariance
\newcommand{\cov}{\operatorname{Cov}}

%Correlation
\newcommand{\corr}{\operatorname{Corr}}

%Span
\newcommand{\myspan}{\operatorname{span}}

%plim
\newcommand{\plim}{\operatorname{plim}}

%Displays n in bold (for the normal distribution?)
\newcommand{\n}{{\bf n}}

%Includes footnote in a table environment. Warning: the footmark is
%always 1.
\newcommand{\tablefootnote}[1]{\begin{flushright}
\rule{5cm}{1pt}\\
\protect\footnotemark[1]{\footnotesize #1}
\end{flushright}
}
\renewcommand*{\thefootnote}{\alph{footnote}}

%Defines the ``th'' as in ``19th'' to be a superscript
\renewcommand{\th}{\textsuperscript{th}}

%Begin and end of a proof
\newcommand{\bpr}{{\bf Proof.} \hspace{1 em}}
\newcommand{\epr}{$\Box$}


\title{Aggregation, forecasting and calculation of  indicators with PandasBiogeme}
\author{Michel Bierlaire}
\date{October 31, 2021}


\begin{document}


\begin{titlepage}
\pagestyle{empty}

\maketitle
\vspace{2cm}

\begin{center}
\small Report TRANSP-OR 211031 \\ Transport and Mobility Laboratory \\ School of Architecture, Civil and Environmental Engineering \\ Ecole Polytechnique F\'ed\'erale de Lausanne \\ \verb+transp-or.epfl.ch+
\begin{center}
\textsc{Series on Biogeme}
\end{center}
\end{center}


\clearpage
\end{titlepage}

\emph{This document is an updated version of \citeasnoun{Bier18a},
  extended and adapted for Biogeme 3.9.}

The package Biogeme (\texttt{biogeme.epfl.ch}) is designed to estimate
the parameters of various models using maximum likelihood
estimation. It is particularly designed for discrete choice
models. But it can also apply an estimated model to calculate
aggregate quantities, calculate and forecast various indicators. In
this document, we describe how to calculate some indicators
particularly relevant in the context of discrete choice models: market
shares, revenues, elasticities, and willingness to pay. Clearly, the
use of the software is not restricted to these indicators, neither to
choice models. But these examples illustrate most of the capabilities.

We assume that the reader is already familiar with discrete choice
models, and has successfully installed \PDBIOGEME.\@  \PDBIOGEME\ is
a genuine Python package written in Python and C++, that relies on the
Pandas library for the management of the data.  This document has
been written using \PDBIOGEME\ 3.9.

\section*{Acknowledgement}
The author would like to thank the Division of Engineering at New York
University Abu Dhabi for hosting him during the preparation of this
document. 

\section{Mathematical expressions}

\begin{flushright}
See \lstinline$01expression.py$ in Section~\ref{sec:01expression}
\end{flushright}

The core of the Biogeme software is the representation of mathematical
expressions.
An important element of an expression is a parameter. It is defined as
followed:

\begin{lstlisting}[style=nonumbers]
b = Beta('b', 1, None, None, 0)
\end{lstlisting}
where the five arguments are
\begin{itemize}
\item the name of the parameter,
\item its default value,
\item the minimum value that it may take, or None if irrelevant,
\item the maximum value that it may take, or None if irrelevant,
  \item 0 if the parameter is free to vary, or 1 if it is fixed to its
    default value.
\end{itemize}
Note that the function \lstinline@Beta@ is available from the module
\lstinline@biogeme.expressions@. So the following statement must be in
the beginning of the file:
\begin{lstlisting}[style=nonumbers]
from biogeme.expressions import Beta
\end{lstlisting}

Then, each expression is a combination of elementary
expressions. The list of elementary expressions available in Biogeme
is available in Appendix~\ref{sec:expressions}. Here is an example of
an expression.
\begin{lstlisting}[style=nonumbers]
expression = exp(-b * b + 1)
\end{lstlisting}
Again, the function \lstinline@exp@ must be imported from \lstinline@biogeme.expressions@.
The value of the expression can be obtained using the function
\lstinline@getValue@ (available only for simple expressions) or
\lstinline@getValue_c@. The former uses a simple Python
implementation, and the latter uses an optimized C++ implementation.
\begin{lstlisting}[style=nonumbers]
expression.getValue_c()
\end{lstlisting}
would give the result 1.
It is also possible to obtain the first and second derivatives of the
expression with respect to the free parameters, as well as the
BHHH.\@ If $f$ is the function represented by the expression, than the
first derivative is a vector of dimension $K$
\[
g = \nabla f(\beta) = \cvect{\frac{\partial f}{\partial \beta_1}
  \\ \vdots \\ \frac{\partial f}{\partial \beta_K}}
\]
where $K$ is the number of free parameters. The second derivative is a
matrix
\[
h = \nabla^2 f(\beta) = \left(
\begin{array}{ccc}
\frac{\partial^2 f}{\partial \beta_1^2} & \cdots & \frac{\partial^2
  f}{\partial \beta_1 \partial \beta_K} \\
\frac{\partial^2 f}{\partial \beta_2 \beta_1 } & \cdots & \frac{\partial^2
  f}{\partial \beta_2 \partial \beta_K} \\
\vdots \\
\frac{\partial^2 f}{\partial \beta_K \beta_1 } & \cdots & \frac{\partial^2
  f}{\partial \beta^2_K} \\
\end{array}
\right).
\]
The BHHH matrix is a $K \times K$ matrix. It is the sample variance-covariance matrix of the
gradient, named after the authors of
\citeasnoun{BernHallHallHaus74}. For each entry in a database, the
contribution to the BHHH matrix is the outer product of the gradient
by itself:
\[
B = \nabla f(\beta) \nabla f(\beta)^T.
\]
In the case of this simple expression, the statement
\begin{lstlisting}[style=nonumbers]
f, g, h, b = expression.getValueAndDerivatives()
\end{lstlisting}
would return $f=1$, $g=-2$, $h=2$ and $b=4$. Indeed,
\[
g(\beta) = -2 \beta e^{1-\beta^2}
\]
and
\[
h(\beta) = 4 \beta^2 e^{1-\beta^2} - 2 e^{1-\beta^2}.
\]
Note that the user does not need to specify the expression for the
derivatives. Biogeme calculates them using automatic differentiation (\cite{Grie89}).
The function can also be used with an optimization algorithm, or for
plotting purposes, as illustrated in Figure~\ref{fig:fgh}. The code to
generate this figure is available in Appendix~\ref{sec:01expression}.

\begin{figure}
\begin{center}
  \input{simplePlot}
\end{center}
\caption{\label{fig:fgh}The simple function and its derivatives}
\end{figure}

\section{Estimation and simple simulation}

\begin{flushright}
See \lstinline$02estimation.py$ in Section~\ref{sec:02estimation}
\end{flushright}

We consider a case study involving a transportation mode choice model,
using revealed preference data collected in Switzerland in 2009 and
2010 (see~\cite{AtaGlerBier2012_DISP}).
The model is a nested logit model with 3 alternatives: \emph{public
  transportation}, \emph{car} and \emph{slow modes}. The utility functions are defined as:
\begin{lstlisting}[style=nonumbers,backgroundcolor=]
V_PT = BETA_TIME_FULLTIME * TimePT_scaled * fulltime +
       BETA_TIME_OTHER * TimePT_scaled * notfulltime +
       BETA_COST * MarginalCostPT_scaled
V_CAR = ASC_CAR +
        BETA_TIME_FULLTIME * TimeCar_scaled * fulltime +
        BETA_TIME_OTHER * TimeCar_scaled * notfulltime +
        BETA_COST * CostCarCHF_scaled
V_SM = ASC_SM +
       BETA_DIST_MALE * distance_km_scaled * male  +
       BETA_DIST_FEMALE * distance_km_scaled * female +
       BETA_DIST_UNREPORTED * distance_km_scaled * unreportedGender
\end{lstlisting}
where
\lstinline@ASC_CAR@,
\lstinline@ASC_SM@,
\lstinline@BETA_TIME_FULLTIME@,
\lstinline@BETA_TIME_OTHER@,
\lstinline@BETA_DIST_MALE@,
\lstinline@BETA_DIST_FEMALE@,
\lstinline@BETA_DIST_UNREPORTED@,
\lstinline@BETA_COST@,
are parameters to be estimated,
\lstinline@TimePT_scale@,
\lstinline@MarginalCostPT_scaled@,
\lstinline@TimeCar_scale@,
\lstinline@CostCarCHF_scale@,
\lstinline@distance_km_scale@
are attributes and
\lstinline@fulltime@,
\lstinline@notfulltime@,
\lstinline@male@,
\lstinline@female@,
\lstinline@unreportedGender@ are socio-economic characteristics.
The two alternatives ``public transportation'' and ``slow modes'' are
grouped into a nest.
The complete specification is available in the file
\lstinline$scenarios.py$ reported in
Section~\ref{sec:scenarios}. We refer the reader to
\citeasnoun{Bier18} for an introduction to the syntax.

The parameters are estimated using PandasBiogeme. Their values are
reported in Table~\ref{tab:estimatedParameters}. Note that it is
recommended to perform bootstrapping on the estimation results, in
order to be able to derive confidence intervals for simulated
quantities:
\begin{lstlisting}[style=nonumbers]
results = biogeme.estimate(bootstrap=100)
\end{lstlisting}

\begin{table}[htb]
  \begin{tabular}{l}
\begin{tabular}{rlr@{.}lr@{.}lr@{.}lr@{.}l}
         &                       &   \multicolumn{2}{l}{}    & \multicolumn{2}{l}{Robust}  &     \multicolumn{4}{l}{}   \\
Parameter &                       &   \multicolumn{2}{l}{Coeff.}      & \multicolumn{2}{l}{Asympt.}  &     \multicolumn{4}{l}{}   \\
number &  Description                     &   \multicolumn{2}{l}{estimate}      & \multicolumn{2}{l}{std. error}  &   \multicolumn{2}{l}{$t$-stat}  &   \multicolumn{2}{l}{$p$-value}   \\

\hline

1 & \lstinline@ASC_CAR@ & 0&261 & 0&100 & 2&61 & 0&01\\
2 & \lstinline@ASC_SM@ & 0&0591 & 0&217 & 0&273 & 0&785\\
3 & \lstinline@BETA_COST@ & -0&716 & 0&138 & -5&18 & 0&00\\
4 & \lstinline@BETA_DIST_FEMALE@ & -0&831 & 0&193 & -4&31 & 0&00\\
5 & \lstinline@BETA_DIST_MALE@ & -0&686 & 0&161 & -4&27 & 0&00\\
6 & \lstinline@BETA_DIST_UNREPORTED@ & -0&703 & 0&196 & -3&58 & 0&000344\\
7 & \lstinline@BETA_TIME_FULLTIME@ & -1&60 & 0&333 & -4&80 & 0&00\\
8 & \lstinline@BETA_TIME_OTHER@ & -0&577 & 0&296 & -1&95 & 0&0515\\
9 & \lstinline@NEST_NOCAR@ & 1&53 & 0&306 & 1&73\footnotemark[1] & 0&08\\

\hline
\end{tabular}
\\
\begin{tabular}{rcl}
\multicolumn{3}{l}{\bf Summary statistics}\\
\multicolumn{3}{l}{ Number of observations = $1906$} \\
\multicolumn{3}{l}{ Number of excluded observations = $359$} \\
\multicolumn{3}{l}{ Number of estimated  parameters = $9$} \\
 $\mathcal{L}(\beta_0)$ &=&  $-2093.955$ \\
 $\mathcal{L}(\hat{\beta})$ &=& $-1298.498 $  \\
 $-2[\mathcal{L}(\beta_0) -\mathcal{L}(\hat{\beta})]$ &=& $1590.913$ \\
AIC &=& $2614.997$\\
BIC &=& $2664.971$\\
\end{tabular}
  \end{tabular}
\tablefootnote{$t$-test against 1}
\caption{\protect\label{tab:estimatedParameters}Nested logit model: estimated parameters}
\end{table}

Once the model has been estimated, and the estimation results stored
in the variable \lstinline@results@, simulation can be performed. We
first use simulation to calculate the contribution of each entry of
the database to the log likelihood function. This is actually useful
to identify possible issues with the calculation of the log
likelihood, or to identify outliers, entries associated with low choice
probability.

\begin{lstlisting}[style=nonumbers]
simulated_choices = logprob.getValue_c(
    betas=results.getBetaValues(),
    database=database,
)
\end{lstlisting}
It returns a vector as long as the number of entries in the database,
containing the log of the choice probability for each entry.
The first three entries are $-0.65047242$,  $-1.43260796$,
$-0.13259356$.

The \lstinline@getValue_c@ function  also offers the option to aggregate the
results, and to return the sum. We can use this feature to verify that
it corresponds to the output of the estimation.

\begin{lstlisting}[style=nonumbers]
loglikelihood = logprob.getValue_c(
    betas=results.getBetaValues(),
    database=database,
    aggregation=True,
)
print(f'Final log likelihood:     {results.data.logLike}')
print(f'Simulated log likelihood: {loglikelihood}')
\end{lstlisting}
We obtain the following results:
\begin{lstlisting}[style=nonumbers]
Final log likelihood:     -1298.4982816177392
Simulated log likelihood: -1298.4982816177394
\end{lstlisting}

The complete code associated with this section 
is available in Appendix~\ref{sec:02estimation}.

\section{Simulation and aggregation}

\begin{flushright}
See \lstinline$03simulation.py$ in Section~\ref{sec:03simulation}
\end{flushright}

Clearly, any valid expression can be simulated on the database. When
several of them must be simulated, it may be useful to use the Biogeme
object to simulate them together. For example, suppose that we want to
calculate the values of the utility function and the choice
probability for each alternative, as well as the normalized weight,
for each entry in the database.

This can be done as introduced in the previous section, using the
\lstinline@getValue_c@ function of each expression. But Biogeme
provides a simulation feature that is more efficient, because it
recycles calculated quantities from one expression to the next. A
typical example is the denominator of the choice probability
formula. It is the same for each alternative. If the
\lstinline@getValue_c@ is used for each expression, the denominator
will be recalculated each time. Therefore, the use of the simulation
feature in Biogeme may save a significant amount of time.

The syntax to perform a simulation of several formulas is the
following:

\begin{lstlisting}[style=nonumbers]
simulate = {
    'weight': normalizedWeight,
    'Utility PT': V_PT,
    'Utility car': V_CAR,
    'Utility SM': V_SM,
    'Prob. PT': prob_PT,
    'Prob. car': prob_CAR,
    'Prob. SM': prob_SM,
}
biogeme = bio.BIOGEME(database, simulate)
biogeme_simulation = biogeme.simulate(results.getBetaValues())
\end{lstlisting}

If we perform a comparison of the calculation time with the
\lstinline@getValue_c@ function and with Biogeme, we obtain:
\begin{lstlisting}[style=nonumbers]
--- Execution time with getValue_c: 0.60 seconds ---
--- Execution time with Biogeme:    0.33 seconds ---
\end{lstlisting}
Note that these results vary from one run to the next. 

\section{Market shares and confidence intervals}

\begin{flushright}
See \lstinline$04market_shares.py$ in Section~\ref{sec:04market_shares}
\end{flushright}

Simulation is necessary to derive useful
indicators from an estimated model. We start by describing how to calculate market shares using
sample enumeration. It is necessary to have a sample of individuals
from the population. For each of them, the value of each of the
variables involved in the model must be known.  Note that it is possible to use the same sample
that was used for estimation, but only if it contains revealed
preferences data. Indeed, the calculation of indicators require real
values for the variables, not values that have been designed and engineered for the
sake of estimating parameters, like in stated preferences data. It is the procedure used in this document.

More formally, consider a choice model $P_n(i|x_n, \C_n)$ providing
the probability that individual $n$ chooses alternative $i$ within the
choice set $\C_n$, given the explanatory variables $x_n$.  In order to
calculate the market shares in the population of size $N$, a sample of
$N_s$ individuals is drawn. As it is rarely possible to draw from the
population with equal sampling probability, it is assumed that
stratified sampling has been used, and that each individual $n$ in the
sample is associated with a weight $w_n$ correcting for sampling
biases. The weights are normalized such that
\begin{equation}
  \label{eq:normalizingWeights}
N_s = \sum_{n=1}^{N_s} w_n.
\end{equation}
An estimator of the market share of alternative $i$ in the population is
\begin{equation}
  \label{eq:marketShare}
W_i = \frac{1}{N_s} \sum_{n=1}^{N_s} w_n P_n(i|x_n, \C_n).
\end{equation}
In practice, the size of the population is rarely known, and the above
quantity is used only in the context of price optimization. In this
case, the factor $N/N_s$ can be omitted.

We calculate \req{eq:marketShare} with
PandasBiogeme by simulating the choice probabilities of each
alternative for each entry in the database, as described in the
previous section.

It generates a Pandas data frame. Each row corresponds to an observation
in the Biogeme database, and each column corresponds to a quantity
requested above.

If bootstrapping has been requested during model estimation, 
it is possible to calculate confidence intervals on these
  quantities, using simulation. First, we obtain the results of the
  bootstrapping: 
\begin{lstlisting}
betas = biogeme.freeBetaNames
b = results.getBetasForSensitivityAnalysis(betas)
\end{lstlisting}
And we calculate 90\% confidence intervals:
\begin{lstlisting}
left, right = biogeme.confidenceIntervals(b, 0.9)
\end{lstlisting}
The two generated Pandas data frames have the same structure as the simulated
values, and contain the left and right bounds of the intervals,
respectively.

We can now calculate the market shares, and the confidence
  intervals. We do it below for the car alternative. See
  Appendix~\ref{sec:04market_shares} for the complete code for the
  other alternatives.

  First, we add a column to the data frames for the
  weighted probabilities
  involved in \req{eq:marketShare}:
\begin{lstlisting}
simulated_values['Weighted prob. car'] = (
    simulated_values['weight'] * simulated_values['Prob. car']
)
\end{lstlisting}
 The market shares as well as the confidence intervals,  are simply
 the mean of these new columns:
\begin{lstlisting}
marketShare_car = simulated_values['Weighted prob. car'].mean()
marketShare_car_left = left['Weighted prob. car'].mean()
marketShare_car_right = right['Weighted prob. car'].mean()
\end{lstlisting}

The output of the Python script is as follows:

\begin{lstlisting}
Market share for car: 65.3% [61.4%, 69.0%]
Market share for PT:  28.1% [24.1%, 32.2%]
Market share for SM:   6.6% [4.0%, 9.5%]
\end{lstlisting}

\section{Revenues}

If the alternative $i$ involves a price variable $p_{in}$, the expected revenue
generated by $i$ is
\begin{equation}
  \label{eq:revenues_pop}
\widehat{R}_i = \frac{N}{N_s} \sum_{n=1}^{N_s} w_n p_{in} P_n(i|x_n, p_{in}, \C_n).
\end{equation}
If $N$ is unknown, it is common to use the expected revenues
generated by the individuals in the sample:
\begin{equation}
  \label{eq:revenues_sample}
R_i = \frac{1}{N_s} \sum_{n=1}^{N_s} w_n p_{in} P_n(i|x_n, p_{in}, \C_n).
\end{equation}

In our example, this is calculated by simulating the following quantities:
\begin{lstlisting}[style=nonumbers]
    simulate = {
        'weight': normalizedWeight,
        'Revenue public transportation':
            prob_PT * MarginalCostScenario,
    }
\end{lstlisting}
Then, the total revenues is obtained using:
\begin{lstlisting}[style=nonumbers]
    revenues_pt = (
        simulated_values['Revenue public transportation']
        * simulated_values['weight']
    ).sum()
\end{lstlisting}

Confidence intervals are obtained in a similar way. The result for the
current price is:
\begin{lstlisting}[style=nonumbers, keywordstyle=\ttfamily]
Total revenues for public transportation (for the sample):
    3018.4 CHF [2458.1 CHF, 3664.3 CHF]
\end{lstlisting}
It is also possible to see how the revenues evolve with the price
factor, as illustrated in Figure~\ref{fig:revenues}. The maximum
revenue among the calculated scenarios is also identified:

\begin{lstlisting}[style=nonumbers]
Largest revenue: 3039.1 obtained with factor 1.2
\end{lstlisting}

\begin{figure}
\begin{center}
\input{revenues}
\end{center}
\caption{\label{fig:revenues}Revenues as a function of the factor}
\end{figure}

\section{Elasticities}\label{sec:elasticities}
Consider now one of the variables involved in the model, for instance
$x_{ink}$, the $k$th variable associated by individual $n$ with
alternative $i$. The
objective is to anticipate the impact of a change of the value of this
variable on the choice of individual $n$,  and subsequently on the market share of
alternative $i$.

\subsection{Point elasticities}


If the variable is continuous, we assume that the relative (infinitesimal) change of
the variable is the same for every individual in the population,  that
is
\begin{equation}
  \label{eq:uniformChange}
\frac{\partial x_{ink}}{x_{ink}} = \frac{\partial x_{ipk}}{x_{ipk}} =
\frac{\partial x_{ik}}{x_{ik}},
\end{equation}
where
\begin{equation}
  \label{eq:avgx}
x_{ik} = \frac{1}{N_s} \sum_{n=1}^{N_s}{x_{ink}}.
\end{equation}
The \emph{disaggregate direct point elasticity} of the model with respect to
the variable $x_{ink}$ is defined as
\begin{equation}
\label{eq:disagElasticity}
  E_{x_{ink}}^{P_n(i)} = \frac{\partial P_n(i|x_n, \C_n)}{\partial
  x_{ink}} \frac{x_{ink}}{P_n(i|x_n, \C_n)}.
\end{equation}
It is called
\begin{itemize}
\item disaggregate,  because it refers to the choice model related to a
  specific individual,
\item direct,  because it measures the impact of a change of an
    attribute of alternative $i$ on the choice probability of the
    same alternative,
\item point,  because we consider an infinitesimal change of the
  variable.
\end{itemize}
The \emph{aggregate direct point elasticity} of the model with
respect to the average value $x_{ik}$ is defined as
\begin{equation}
E_{x_{ik}}^{W_i} = \frac{\partial W_i}{\partial x_{ik}} \frac{x_{ik}}{W_i}.
\end{equation}
Using \req{eq:marketShare},  we obtain
\begin{equation}
E_{x_{ik}}^{W_i} = \frac{1}{N_s}  \sum_{n=1}^{N_s} w_n \frac{\partial
  P_n(i|x_n, \C_n)}{\partial x_{ik}} \frac{x_{ik}}{W_i}.
\end{equation}
From \req{eq:uniformChange},  we obtain
\begin{equation}
E_{x_{ik}}^{W_i} = \frac{1}{N_s}  \sum_{n=1}^{N_s} w_n \frac{\partial
  P_n(i|x_n, \C_n)}{\partial x_{ink}} \frac{x_{ink}}{W_i} =
\frac{1}{N_s}  \sum_{n=1}^{N_s} w_n E_{x_{ink}}^{P_n(i)}  \frac{P_n(i|x_n, \C_n)}{W_i},
\end{equation}
where the second equation is derived from \req{eq:disagElasticity}.
Using \req{eq:marketShare} again,  we obtain
\begin{equation}
\label{eq:aggDisagg}
  E_{x_{ik}}^{W_i} =  \sum_{n=1}^{N_s}
E_{x_{ink}}^{P_n(i)}  \frac{w_n P_n(i|x_n, \C_n)}{ \sum_{n=1}^{N_s} w_n P_n(i|x_n, \C_n)}.
\end{equation}
This equation shows that the calculation of aggregate elasticities
involves a weighted sum of disaggregate elasticities. However,  the
weight is not $w_n$ as for the market share,  but a normalized version
of $w_n P_n(i|x_n, \C_n)$.

The \emph{disaggregate cross point elasticity} of the model with respect to
the variable $x_{jnk}$ is defined as
\begin{equation}
\label{eq:disagCrossElasticity}
  E_{x_{jnk}}^{P_n(i)} = \frac{\partial P_n(i|x_n, \C_n)}{\partial
  x_{jnk}} \frac{x_{jnk}}{P_n(i|x_n, \C_n)}.
\end{equation}
It is called \emph{cross} elasticity because it measures the sensitivity
of the model for alternative $i$ with respect to a  modification of
the attribute of another alternative.



\subsection{Arc elasticities}

A similar derivation can be done for arc elasticities. In this case,
the relative change of the variable is not infinitesimal anymore. The
idea is to analyze a before/after scenario. The variable $x_{ink}$ in
the before scenario becomes $x_{ink} + \Delta x_{ink}$ in the after scenario.
As above,  we assume that the relative change of
the variable is the same for every individual in the population,  that
is
\begin{equation}
  \label{eq:uniformChangeArc}
\frac{\Delta x_{ink}}{x_{ink}} = \frac{\Delta x_{ipk}}{x_{ipk}} =
\frac{\Delta x_{ik}}{x_{ik}},
\end{equation}
where $x_{ik}$ is defined by \req{eq:avgx}.
The \emph{disaggregate direct arc elasticity} of the model with respect to
the variable $x_{ink}$ is defined as
\begin{equation}
\label{eq:disagElasticityArc}
  E_{x_{ink}}^{P_n(i)} = \frac{\Delta P_n(i|x_n, \C_n)}{\Delta
  x_{ink}} \frac{x_{ink}}{P_n(i|x_n, \C_n)}.
\end{equation}
The \emph{aggregate direct arc elasticity} of the model with
respect to the average value $x_{ik}$ is defined as
\begin{equation}
E_{x_{ik}}^{W_i} = \frac{\Delta W_i}{\Delta x_{ik}} \frac{x_{ik}}{W_i}.
\end{equation}
The two quantities are also related by \req{eq:aggDisagg},  following
the exact same derivation as for the point elasticity.

\subsection{Using PandasBiogeme for point elasticities}

\begin{flushright}
See \lstinline$06point_elasticities.py$ in Section~\ref{sec:06point_elasticities}
\end{flushright}

The calculation of \req{eq:disagElasticity} involves derivatives. For
simple models such as logit, the analytical formula of these
derivatives can easily be derived. However, their derivation for
advanced models can be tedious. It is common to make mistakes in the
derivation itself, and even more common to make mistakes in the
implementation. Therefore, PandasBiogeme provides an operator that
calculates the derivative of a formula. It is illustrated in the
file \lstinline$06point_elasticities.py$, reported in
Section~\ref{sec:06point_elasticities}. We describe here the
calculation of the elasticity of the demand for public transportation
with respect to the travel time of public transportation. Other
elasticities are calculated similarly.  The calculation of the
 disaggregate elasticities for each individual by PandasBiogeme are
 performed using the following statement:
\begin{lstlisting}
direct_elas_pt_time = \
  Derive(prob_pt,'TimePT') * TimePT / prob_pt
\end{lstlisting}
and adding the corresponding entry in the simulation dictionary:
\begin{lstlisting}
simulate = {'weight': normalizedWeight,
            'Prob. car': prob_car,
            'Prob. public transportation': prob_pt,
            'Prob. slow modes':prob_sm,
            'direct_elas_pt_time':direct_elas_pt_time,
            'direct_elas_pt_cost':direct_elas_pt_cost,
            'direct_elas_car_time':direct_elas_car_time,
            'direct_elas_car_cost':direct_elas_car_cost,
            'direct_elas_sm_dist':direct_elas_sm_dist}
\end{lstlisting}

The above syntax should be self-explanatory. But there is an important
aspect to take into account. In the context of the estimation of the
parameters of the model, the variables are often scaled in order to
improve the numerical properties of the likelihood function, using
statements like
\begin{lstlisting}
TimePT_scaled = DefineVariable('TimePT_scaled', TimePT / 200 )
\end{lstlisting}
or
\begin{lstlisting}
TimePT_scaled = TimePT / 200
\end{lstlisting}
The \lstinline$DefineVariable$ operator is designed to preprocess the
data file, and can be seen as a way to add another column in the data
file, defining a new variable. However, if it is used, the
relationship between the new variable and the original one is lost,
for the sake of computational speed.
Therefore, \lstinline+prob_pt+ depends on \lstinline+TimePT_scaled+,
but  not  on \lstinline+TimePT+. Therefore, the result of
\lstinline+Derive(prob_pt,'TimePT')+ is zero.

Consequently, when you need to calculate derivatives, you may want to
replace statements like
\begin{lstlisting}
TimePT_scaled = DefineVariable('TimePT_scaled', TimePT / 200 )
\end{lstlisting}
by
\begin{lstlisting}
TimePT_scaled  = TimePT / 200
\end{lstlisting}
in order to maintain the analytical structure of the formula to be derived.

The aggregate point elasticities can be obtained by aggregating the
disaggregate elasticities, using \req{eq:aggDisagg}. This requires the
calculation of the normalization factors
\begin{equation}
\sum_{n=1}^{N_s} w_n P_n(i|x_n, \C_n).
\end{equation}
This is performed with the following code, that first creates new
columns in the Pandas data frame, and then calculate their sum:
\begin{lstlisting}
simulated_values['Weighted prob. PT'] = (
    simulated_values['weight']
    * simulated_values['Prob. public transportation']
)
denominator_pt = simulated_values['Weighted prob. PT'].sum()
\end{lstlisting}

The calculation of the aggregate direct elasticity \req{eq:aggDisagg}
is performed as follows:
\begin{lstlisting}
direct_elas_term_pt_time = (
    simulated_values['Weighted prob. PT']
    * simulated_values['direct_elas_pt_time']
    / denominator_pt
).sum()
\end{lstlisting}
Note that, in this case, we did not explicitly create a new column
before calculating the sum. Looking at the formula, we have
\begin{itemize}
  \item the disaggregate elasticity: \lstinline+simulated_values['direct_elas_pt_time']+ $=E_{x_{ink}}^{P_n(i)}$,
  \item the numerator: \lstinline+simulated_values['Weighted prob. PT']+ $=w_n P_n(i|x_n,\C_n)$, calculated previously,
  \item the denominator \lstinline+denominator_pt+ $=
    \sum_{n=1}^{N_s} w_n P_n(i|x_n, \C_n)$, calculated previously.
\end{itemize}

The output of the Python script is as follows:
\begin{lstlisting}
Aggregate direct elasticity of car wrt time: -0.0441
Aggregate direct elasticity of car wrt cost: -0.0906
Aggregate direct elasticity of PT wrt time: -0.274
Aggregate direct elasticity of PT wrt cost: -0.32
Aggregate direct elasticity of SM wrt distance: -1.09
\end{lstlisting}

\subsection{Using PandasBiogeme for cross elasticities}

\begin{flushright}
See \lstinline$07cross_elasticities.py$ in Section~\ref{sec:07cross_elasticities}
\end{flushright}



The calculation of \req{eq:disagCrossElasticity} is performed in a
similar way as the direct elasticities \req{eq:disagElasticity}, using the following statements:
\begin{lstlisting}
cross_elas_pt_time = Derive(prob_pt,'TimeCar') * TimeCar / prob_pt
\end{lstlisting}

The output of the Python script is the following:
\begin{lstlisting}
Aggregate cross elasticity of car wrt time: 0.106
Aggregate cross elasticity of car wrt cost: 0.123
Aggregate cross elasticity of PT wrt car time: 0.0953
Aggregate cross elasticity of PT wrt car cost: 0.2
\end{lstlisting}
Note that these values are now positive. Indeed, when the travel time
or travel cost of a competing mode increases, the market share
increases.


\subsection{Using PandasBiogeme for arc elasticities}

\begin{flushright}
See \lstinline$08arc_elasticities.py$ in Section~\ref{sec:08arc_elasticities}
\end{flushright}


Arc elasticities require a before and after scenarios. In this case,
we calculate the sensitivity of the market share of the public transportation
alternative when there is an increase of 20\% of the marginal cost.

The disaggregate arc elasticity is calculated as
\begin{lstlisting}[style=nonumbers]
direct_elas_pt = (
    (prob_PT_after - prob_PT)
    * MarginalCostPT
    / (prob_PT * (MarginalCostPT_after - MarginalCostPT))
)
\end{lstlisting}

And the aggregate arc elasticity as follows:
\begin{lstlisting}[style=nonumbers]
simulated_values['Weighted prob. PT'] = (
    simulated_values['weight'] * simulated_values['Prob. PT']
)

denominator_pt = simulated_values['Weighted prob. PT'].sum()

direct_elas_pt = (
    simulated_values['Weighted prob. PT']
    * simulated_values['direct_elas_pt']
    / denominator_pt
).sum()
\end{lstlisting}

The output of the Python script is as follows:
\begin{lstlisting}
Aggregate direct arc elasticity of
  public transportation wrt cost: -0.299
\end{lstlisting}

\section{Willingness to pay}

\begin{flushright}
See \lstinline$09wtp.py$ in Section~\ref{sec:09wtp}
\end{flushright}


If the model contains a cost or price variable (like in this example),
it is possible to analyze the trade-off between any variable and
money.  This reflects the willingness of the decision maker to pay for
a modification of another variable of the model.  A typical example in
transportation is the \emph{value of time}, that is the amount of
money a traveler is willing to pay in order to decrease her travel
time.

Let $c_{in}$ be the cost of alternative $i$ for individual $n$.
Let $x_{ink}$ be the value of another variable of the model.
Let $V_{in}(c_{in},x_{ink})$ be the value of the utility function.
Consider a scenario where the variable of interest takes the value
$x_{ink} + \delta^x_{ink}$.
We denote by $\delta^c_{in}$ the additional  cost  that would achieve the same utility, that is
\begin{equation}
  \label{eq:wtpEquation}
V_{in}(c_{in}+\delta^c_{in},x_{ink}+\delta^x_{ink}) = V_{in}(c_{in},x_{ink}).
\end{equation}
The willingness to pay to increase the value of $x_{ink}$ is defined
as the additional cost per unit of $x$, that is
\begin{equation}
  \label{eq:wtpDiscrete}
  \delta^c_{in}/\delta^x_{ink},
\end{equation}
and is obtained by solving Equation~\req{eq:wtpEquation}.
If $x_{ink}$ and $c_{in}$ appear linearly in the utility function, that
is if
\begin{equation}
V_{in}(c_{in},x_{ink}) = \beta_c c_{in} + \beta_x x_{ink} + \cdots,
\end{equation}
and
\begin{equation}
V_{in}(c_{in}+\delta^c_{in},x_{ink}+\delta^x_{ink}) = \beta_c (c_{in}+\delta^c_{in}) + \beta_x (x_{ink}+\delta^x_{ink}) + \cdots.
\end{equation}
Therefore, \req{eq:wtpDiscrete} is
\begin{equation}
  \label{eq:wtpLinear}
  \delta^c_{in}/\delta^x_{ink} = -\beta_x / \beta_c.
\end{equation}
If $x_{ink}$ is a continuous variable, and if $V_{in}$ is
differentiable in $x_{ink}$ and $c_{in}$, we can invoke  Taylor's
theorem in \req{eq:wtpEquation}:
\begin{equation}
\begin{aligned}
V_{in}(c_{in},x_{ink})&= V_{in}(c_{in}+\delta^c_{in},x_{ink}+\delta^x_{ink})\\ &\approx V_{in}(c_{in},x_{ink}) + \delta^c_{in} \frac{\partial V_{in}}{\partial c_{in}}(c_{in},x_{ink})+ \delta^x_{ink} \frac{\partial V_{in}}{\partial x_{ink}}(c_{in},x_{ink})
\end{aligned}
\end{equation}
Therefore, the willingness to pay is equal to
\begin{equation}
  \label{eq:wtpContinuous}
\frac{\delta^c_{in}}{ \delta^x_{ink}} = - \frac{(\partial V_{in}/\partial x_{ink})(c_{in},x_{ink})}{(\partial V_{in}/\partial c_{in})(c_{in},x_{ink})}.
\end{equation}
Note that if $x_{ink}$ and $c_{in}$ appear linearly in the utility
function, \req{eq:wtpContinuous} is the same as \req{eq:wtpLinear}.
If we consider now a scenario where the variable under interest takes the value
$x_{ink} - \delta^x_{ink}$, the same derivation leads to the
willingness to pay to \emph{decrease} the value of $x_{ink}$:
\begin{equation}
  \label{eq:wtpContinuousDecrease}
\frac{\delta^c_{in}}{ \delta^x_{ink}} = \frac{(\partial V_{in}/\partial x_{ink})(c_{in},x_{ink})}{(\partial V_{in}/\partial c_{in})(c_{in},x_{ink})}.
\end{equation}
The calculation of the value of time corresponds to such a scenario:
\begin{equation}
\frac{\delta^c_{in}}{ \delta^t_{in}} =  \frac{(\partial V_{in}/\partial t_{in})(c_{in},t_{in})}{(\partial V_{in}/\partial c_{in})(c_{in},t_{in})} = \frac{\beta_t}{\beta_c},
\end{equation}
where the last equation assumes that $V$ is linear in these variables.
Note that, in this special case of linear utility functions, the value
of time is constant across individuals, and is also independent of
$\delta^t_{in}$. This is not true in general.

The calculation of \req{eq:wtpContinuousDecrease} involves the
calculation of derivatives. It is done in PandasBiogeme using the
following statements:
\begin{lstlisting}
WTP_PT_TIME = Derive(V_PT,'TimePT') / Derive(V_PT,'MarginalCostPT')
WTP_CAR_TIME = Derive(V_CAR,'TimeCar') / Derive(V_CAR,'CostCarCHF')
\end{lstlisting}
The full specification file can be found in
Section~\ref{sec:09wtp}.
The output of the Python script is as follows:
\begin{lstlisting}
Average WTP for car: 3.96 CI:[2.13, 7.53]
Unique values:       ['2.42', '6.69']
WTP car for workers: 6.69 CI:[4.29, 11.6]
WTP car for females: 3.17 CI:[1.51, 6.35]
WTP car for males  : 4.95 CI:[2.92, 9.04]
\end{lstlisting}


The average value of time for car is 3.96 CHF/hour (confidence
interval: [2.13, 7.53]). This value is abnormally low, which is a
sign of a potential poor specification of the model.
Note also that, with this specification, the value of time is the same for
car and public transportation, as the coefficients of the time and
cost variables are generic.

Finally, it is important to look at the distribution of the
willingness to pay in the population/sample. We have implemented a
Python function that calculates the average willingness to pay for a
subgroup of the population, defined by a filter.

\begin{lstlisting}
def wtpForSubgroup(theFilter):
    size = theFilter.sum()
    sim = simulated_values[theFilter]
    totalWeight = sim['weight'].sum()
    weight = sim['weight'] * size / totalWeight
    _wtpcar = (
        60 * sim['WTP CAR time'] * weight
    ).mean()
    _wtpcar_left = (
        60 * left[theFilter]['WTP CAR time'] * weight
    ).mean()
    _wtpcar_right = (
        60 * right[theFilter]['WTP CAR time'] * weight
    ).mean()
    return _wtpcar, _wtpcar_left, _wtpcar_right

\end{lstlisting}

We start by calculating the number of entries in the filter that are
True.
\begin{lstlisting}
size = theFilter.sum()
\end{lstlisting}
Then, we extract the simulated values corresponding to the filter:
\begin{lstlisting}
sim = simulated_values[theFilter]
\end{lstlisting}
We calculate the total weight of these observations:
\begin{lstlisting}
totalWeight = sim['weight'].sum()
\end{lstlisting}
We renormalize the weights in order to verify \req{eq:normalizingWeights}:
\begin{lstlisting}
weight = sim['weight'] * size / totalWeight
\end{lstlisting}
We are now ready to calculate the average quantities:
\begin{lstlisting}
_wtpcar = (
    60 * sim['WTP CAR time'] * weight
).mean()
_wtpcar_left = (
    60 * left[theFilter]['WTP CAR time'] * weight
).mean()
_wtpcar_right = (
    60 * right[theFilter]['WTP CAR time'] * weight
).mean()
\end{lstlisting}
They are returned as a Python tuple:
\begin{lstlisting}
return _wtpcar, _wtpcar_left, _wtpcar_right
\end{lstlisting}

For instance, in order to obtain the value for full time workers, we
use the following code:
\begin{lstlisting}
aFilter = database.data['OccupStat'] == 1
w, l, r = wtpForSubgroup(aFilter)
print(f'WTP car for workers: {w:.3g} CI:[{l:.3g}, {r:.3g}]')
\end{lstlisting}

This exploits the functionalities of Pandas. We have two Pandas data
frames involved here: \lstinline+database.data+ is the Biogeme data
file, and \lstinline+simulated_values+ is the output of the
simulation. The variable \lstinline+aFilter+ is a vector of boolean
variables of length 1906
(the total number of observations in the sample).

We can also plot the distribution of the willingness to pay in the
population (see Figure~\ref{fig:wtp}), using the following code:
\begin{lstlisting}
import matplotlib.pyplot as plt
plt.hist(
    60 * simulated_values['WTP CAR time'],
    weights=simulated_values['weight'],
)
plt.xlabel("WTP (CHF/hour)")
plt.ylabel("Individuals")
plt.show()
\end{lstlisting}
In this case, there are only two values: 2.42 CHF/hour and 6.69
CHF/hour. Unique values can be extracted in Pandas using the following
statement:
\begin{lstlisting}
60 * simulated_values['WTP CAR time'].unique()
\end{lstlisting}
where the constant 60 is designed to report the output in CHF/hours
instead of CHF/min.
\begin{figure}[htb]
\begin{center}
\input{wtp}
\end{center}
\caption{\label{fig:wtp}Distribution of the willingness to pay in the sample}
\end{figure}
\section{Conclusion}

PandasBiogeme is a flexible tool that allows to extract useful
indicators from complex models. In this document, we have presented
how some indicators relevant for discrete choice models  can be
generated. As the output of the simulation is a Pandas data frame, a
great deal of analysis can be performed exploiting the functionalities of
Python, Pandas and Biogeme.


\clearpage

\appendix

\section{Mathematical expressions}\label{sec:expressions}

\renewcommand{\arraystretch}{1.2}
\subsection{Simple expressions}
Simple expressions can be calculated both with the functions
\lstinline@getValue@ (implemented in Python) and the \lstinline@getValue_c@ (implemented in C++). They do not require a database.
\begin{center}
\begin{tabular}{llll}
  &   Input  & Syntax & Output \\
  \hline
  Addition & $x$, $y$ & \lstinline@x + y@ & $x+y$ \\
  Substraction & $x$, $y$ & \lstinline@x - y@ & $x-y$ \\
  Multiplication & $x$, $y$ & \lstinline@x * y@ & $xy$ \\
  Division & $x$, $y$ & \lstinline@x / y@ & $\frac{x}{y}$ \\
  Power & $x$, $y$ & \lstinline@x ** y@ & $x^y$ \\
  Exponential & $x$ & \lstinline@exp(x)@ & $e^x$ \\
  Logarithm & $x$ & \lstinline@log(x)@ & $\ln(x) = \log_e(x)$ \\
  Minimum & $x$, $y$ & \lstinline@bioMin(x, y)@ & $\min(x, y)$ \\
  Maximum & $x$, $y$ & \lstinline@bioMax(x, y)@ & $\max(x, y)$ \\
  And & $x$, $y$ & \lstinline@x & y@ & 1 if $(x\neq 0)$ and $(y \neq 0)$ \\
      &          &                   & 0 otherwise \\
  Or & $x$, $y$ & \lstinline@x | y@ & 1 if $(x\neq 0)$ or $(y \neq 0)$ \\
      &          &                   & 0 otherwise \\
\end{tabular}
\end{center}
\begin{center}
\begin{tabular}{llll}
  &   Input  & Syntax & Output \\
  \hline
  Equal & $x$, $y$ & \lstinline@x == y@ & 1 if $x=y$ \\
      &          &                   & 0 otherwise \\
  Not equal & $x$, $y$ & \lstinline@x != y@ & 1 if $x\neq y$ \\
      &          &                   & 0 otherwise \\
  Lesser or equal & $x$, $y$ & \lstinline@x <= y@ & 1 if $x\leq y$ \\
      &          &                   & 0 otherwise \\
  Greater or equal & $x$, $y$ & \lstinline@x >= y@ & 1 if $x\geq y$ \\
      &          &                   & 0 otherwise \\
  Lesser & $x$, $y$ & \lstinline@x < y@ & 1 if $x < y$ \\
      &          &                   & 0 otherwise \\
  Greater & $x$, $y$ & \lstinline@x > y@ & 1 if $x > y$ \\
  &          &                   & 0 otherwise \\
  Opposite & $x$ & \lstinline@-x@ & $-x$ \\
  Multiple sum & $(x_i)_{i=1}^K$ &
  \lstinline@bioMultSum(listOfExpressions)@ & $\sum_{i=1}^K x_i$ \\
  Element &  $k, d = \{k: x_k\}_{k=1}^K$  & \lstinline@Elem(my_dict, index)@ & $d[k]$ \\
\end{tabular}
\end{center}

\subsection{Complex expression}

An expressions is deemed complex in Biogeme when the
\lstinline@getValue@ function is not available. Only the
\lstinline@getValue_c@ function must be used. It calculates the
expressions using a C++ implementation of the expression.

\renewcommand{\arraystretch}{2}

\begin{center}
\begin{tabular}{lllp{0.5\textwidth}}
  &   Input  & Syntax & Output \\
  \hline
  Normal CDF & $x$ & \lstinline@bioNormalCdf(x)@ &
  $\displaystyle\frac{1}{\sqrt{2\pi}}\int_{t=-\infty}^x e^{\frac{1}{2}t^2}dt$ \\
  Derivative & $x$ & \lstinline@Derive(x, 'y')@ & $\displaystyle\frac{\partial x}{\partial
    y}$ where $y$ is a literal expression. \\
  Integral & $f(\omega)$, $\omega$ & \lstinline@Integrate(f, 'omega')@ &
  $\displaystyle\int_{-\infty}^{+\infty} f(\omega) d\omega.$
\end{tabular}
\end{center}

  \subsection{Expression requiring a database}

\begin{center}
\begin{tabular}{p{0.15\textwidth}p{0.20\textwidth}p{0.35\textwidth}p{0.30\textwidth}}
  &   Input  & Syntax & Output \\
  \hline
  Linear utility & $(\beta_i, x_i)_{i=1}^K$ & \lstinline@bioLinearUtility(listOfTerms)@
  & $\displaystyle\sum_{i=1}^K \beta_i x_i$. \\
  Monte-Carlo & $x(\xi)$, $(\xi_r)_{r=1}^R$ &
  \lstinline@MonteCarlo(x)@ & $\displaystyle\frac{1}{R} \sum_{r=1}^R x(\xi_r)$\\
  Panel trajectory & $x$ & \lstinline@PanelLikelihoodTrajectory(x)@ &
  $\displaystyle\prod_{t=1}^{T_n} x_t$ \\
     &    &   & $x_1, \ldots x_T$ are expressions calculated on the
  $T$ entries associated with the same
  individual $n$ in panel data. \\
  \end{tabular}
\end{center}

\section{Complete specification files}

We provide here the code of the specification files used in this
document.

\subsection{\lstinline$01expression.py$}\label{sec:01expression}
\begin{lstlisting}[style=numbers]
import numpy as np
import matplotlib.pyplot as plt
from biogeme.expressions import Beta, exp


# We create a simple expression
b = Beta('b', 1, None, None, 0)
expression = exp(-b * b + 1)

# We can calculate its value.
z = expression.getValue_c()
print(f'exp(-b * b + 1) = {z}')

# We can also calculate the value, the first derivative, the second
# derivative, and the BHHH, which in this case is the square of the
# first derivatives

f, g, h, bhhh = expression.getValueAndDerivatives()
print(f'f = {f}')
print(f'g = {g}')
print(f'h = {h}')
print(f'BHHH = {bhhh}')

# From the expression, we can create a Python function that takes as
# argument the value of the free parameters, and returns the function,
# the first and second derivatives.
fct = expression.createFunction()

# We can use the function for different values of the parameter
beta = 2
f, g, h = fct(beta)
print(f'f({beta}) = {f}')
print(f'g({beta}) = {g}')
print(f'h({beta}) = {h}')

beta = 3
f, g, h = fct(beta)
print(f'f({beta}) = {f}')
print(f'g({beta}) = {g}')
print(f'h({beta}) = {h}')


# We can also use it to plot the function and its derivatives
x = np.arange(-2, 2, 0.01)
f = [fct(xx)[0] for xx in x]
g = [float(fct([xx])[1]) for xx in x]
h = [float(fct([xx])[2]) for xx in x]

ax = plt.gca()
ax.plot(x, f, label="f(x)")
ax.plot(x, g, label="f'(x)")
ax.plot(x, h, label='f"(x)')
ax.legend()
plt.show()
\end{lstlisting}

\subsection{\lstinline$scenarios.py$}\label{sec:scenarios}
\begin{lstlisting}[style=numbers]
import pandas as pd
import biogeme.database as db
from biogeme.expressions import Beta, Variable

# Read the data
df = pd.read_csv('optima.dat', sep='\t')
database = db.Database('optima', df)

# Variables from the data
Choice = Variable('Choice')
TimePT = Variable('TimePT')
TimeCar = Variable('TimeCar')
MarginalCostPT = Variable('MarginalCostPT')
CostCarCHF = Variable('CostCarCHF')
distance_km = Variable('distance_km')
Gender = Variable('Gender')
OccupStat = Variable('OccupStat')
Weight = Variable('Weight')

# Exclude observations such that the chosen alternative is -1
database.remove(Choice == -1.0)

# Normalize the weights
sumWeight = database.data['Weight'].sum()
numberOfRows = database.data.shape[0]
normalizedWeight = Weight * numberOfRows / sumWeight

# List of parameters to be estimated
ASC_CAR = Beta('ASC_CAR', 0, None, None, 0)
ASC_PT = Beta('ASC_PT', 0, None, None, 1)
ASC_SM = Beta('ASC_SM', 0, None, None, 0)
BETA_TIME_FULLTIME = Beta('BETA_TIME_FULLTIME', 0, None, None, 0)
BETA_TIME_OTHER = Beta('BETA_TIME_OTHER', 0, None, None, 0)
BETA_DIST_MALE = Beta('BETA_DIST_MALE', 0, None, None, 0)
BETA_DIST_FEMALE = Beta('BETA_DIST_FEMALE', 0, None, None, 0)
BETA_DIST_UNREPORTED = Beta(
    'BETA_DIST_UNREPORTED', 0, None, None, 0
)
BETA_COST = Beta('BETA_COST', 0, None, None, 0)

# Definition of variables:
# For numerical reasons, it is good practice to scale the data to
# that the values of the parameters are around 1.0.

TimePT_scaled = TimePT / 200
TimeCar_scaled = TimeCar / 200
CostCarCHF_scaled = CostCarCHF / 10
distance_km_scaled = distance_km / 5
male = Gender == 1
female = Gender == 2
unreportedGender = Gender == -1

fulltime = OccupStat == 1
notfulltime = OccupStat != 1


def scenario(factor=1.0):
    """Provide the model specification for a scenario with the price of
        public transportation is multiplied by a factor

    :param factor: factor that multiples the price of public transportation.
    :type factor: float

    :return: a dict with the utility functions, the nesting structure,
        and the choice expression.

    :rtype: dict(int: biogeme.expression), tuple(biogeme.expression,
        list(int)), biogeme.expression
    """
    MarginalCostScenario = MarginalCostPT * factor
    MarginalCostPT_scaled = MarginalCostScenario / 10
    # Definition of utility functions:
    V_PT = (
        ASC_PT
        + BETA_TIME_FULLTIME * TimePT_scaled * fulltime
        + BETA_TIME_OTHER * TimePT_scaled * notfulltime
        + BETA_COST * MarginalCostPT_scaled
    )
    V_CAR = (
        ASC_CAR
        + BETA_TIME_FULLTIME * TimeCar_scaled * fulltime
        + BETA_TIME_OTHER * TimeCar_scaled * notfulltime
        + BETA_COST * CostCarCHF_scaled
    )
    V_SM = (
        ASC_SM
        + BETA_DIST_MALE * distance_km_scaled * male
        + BETA_DIST_FEMALE * distance_km_scaled * female
        + BETA_DIST_UNREPORTED
        * distance_km_scaled
        * unreportedGender
    )

    # Associate utility functions with the numbering of alternatives
    V = {0: V_PT, 1: V_CAR, 2: V_SM}

    # Definition of the nests:
    # 1: nests parameter
    # 2: list of alternatives
    MU_NOCAR = Beta('MU_NOCAR', 1.0, 1.0, None, 0)

    CAR_NEST = 1.0, [1]
    NO_CAR_NEST = MU_NOCAR, [0, 2]
    nests = CAR_NEST, NO_CAR_NEST
    return V, nests, Choice, MarginalCostScenario
\end{lstlisting}

\subsection{\lstinline$01expressions.py$}\label{sec:01expressions}
\begin{lstlisting}[style=numbers]
import numpy as np
import matplotlib.pyplot as plt
from biogeme.expressions import Beta, exp


# We create a simple expression
b = Beta('b', 1, None, None, 0)
expression = exp(-b * b + 1)

# We can calculate its value.
z = expression.getValue_c()
print(f'exp(-b * b + 1) = {z}')

# We can also calculate the value, the first derivative, the second
# derivative, and the BHHH, which in this case is the square of the
# first derivatives

f, g, h, bhhh = expression.getValueAndDerivatives()
print(f'f = {f}')
print(f'g = {g}')
print(f'h = {h}')
print(f'BHHH = {bhhh}')

# From the expression, we can create a Python function that takes as
# argument the value of the free parameters, and returns the function,
# the first and second derivatives.
fct = expression.createFunction()

# We can use the function for different values of the parameter
beta = 2
f, g, h = fct(beta)
print(f'f({beta}) = {f}')
print(f'g({beta}) = {g}')
print(f'h({beta}) = {h}')

beta = 3
f, g, h = fct(beta)
print(f'f({beta}) = {f}')
print(f'g({beta}) = {g}')
print(f'h({beta}) = {h}')


# We can also use it to plot the function and its derivatives
x = np.arange(-2, 2, 0.01)
f = [fct(xx)[0] for xx in x]
g = [float(fct([xx])[1]) for xx in x]
h = [float(fct([xx])[2]) for xx in x]

ax = plt.gca()
ax.plot(x, f, label="f(x)")
ax.plot(x, g, label="f'(x)")
ax.plot(x, h, label='f"(x)')
ax.legend()

plt.show()
\end{lstlisting}

\subsection{\lstinline$02estimation.py$}\label{sec:02estimation}
\begin{lstlisting}[style=numbers]
from biogeme import models
import biogeme.biogeme as bio
from scenarios import scenario, database

# Obtain the specification for the default scenario
V, nests, Choice, _ = scenario()

# The choice model is a nested logit, with availability conditions
# For estimation, we need the log of the probability
logprob = models.lognested(V, None, nests, Choice)

# Create the Biogeme object for estimation
biogeme = bio.BIOGEME(database, logprob)
biogeme.modelName = '02estimation'

print('Estimation...')
# Estimate the parameters. Perform bootstrapping.
results = biogeme.estimate(bootstrap=100)

# Get the results in a pandas table
pandasResults = results.getEstimatedParameters()
print(pandasResults)

print('Simulation...')

simulated_choices = logprob.getValue_c(
    betas=results.getBetaValues(), database=database
)

print(simulated_choices)

loglikelihood = logprob.getValue_c(
    betas=results.getBetaValues(),
    database=database,
    aggregation=True,
)

print(f'Final log likelihood:     {results.data.logLike}')
print(f'Simulated log likelihood: {loglikelihood}')
\end{lstlisting}

\subsection{\lstinline$03simulation.py$}\label{sec:03simulation}
\begin{lstlisting}[style=numbers]
import sys
import time
import pandas as pd
from biogeme import models
import biogeme.biogeme as bio
import biogeme.exceptions as excep
import biogeme.results as res
from scenarios import scenario, database, normalizedWeight

# Obtain the specification for the default scenario
V, nests, _, _ = scenario()

V_PT = V[0]
V_CAR = V[1]
V_SM = V[2]

# Obtain the expression for the choice probability of each alternative
prob_PT = models.nested(V, None, nests, 0)
prob_CAR = models.nested(V, None, nests, 1)
prob_SM = models.nested(V, None, nests, 2)

# Read the estimation results from the file
try:
    results = res.bioResults(pickleFile='02estimation.pickle')
except excep.biogemeError:
    sys.exit(
        'Run first the script 02simulation.py '
        'in order to generate the '
        'file 02estimation.pickle.'
    )


# We now simulate various expressions on the database, and store the
# results in a Pandas dataframe

start_time = time.time()
simulate_formulas = {
    'weight': normalizedWeight.getValue_c(
        betas=results.getBetaValues(), database=database
    ),
    'Utility PT': V_PT.getValue_c(
        betas=results.getBetaValues(), database=database
    ),
    'Utility car': V_CAR.getValue_c(
        betas=results.getBetaValues(), database=database
    ),
    'Utility SM': V_SM.getValue_c(
        betas=results.getBetaValues(), database=database
    ),
    'Prob. PT': prob_PT.getValue_c(
        betas=results.getBetaValues(), database=database
    ),
    'Prob. car': prob_CAR.getValue_c(
        betas=results.getBetaValues(), database=database
    ),
    'Prob. SM': prob_SM.getValue_c(
        betas=results.getBetaValues(), database=database
    ),
}

simulated_values = pd.DataFrame.from_dict(
    simulate_formulas,
)
print(
    f'--- Execution time with getValue_c: '
    f'{time.time() - start_time:.2f} seconds ---'
)

# We now perform the same simulation using Biogeme. The results are be
# identical, but the execution time is faster. Indeed, Biogeme
# recycles calculations performed for one expression for the other
# expressions.

# A dictionary with the requested expression must be provided to Biogeme

simulate = {
    'weight': normalizedWeight,
    'Utility PT': V_PT,
    'Utility car': V_CAR,
    'Utility SM': V_SM,
    'Prob. PT': prob_PT,
    'Prob. car': prob_CAR,
    'Prob. SM': prob_SM,
}

start_time = time.time()
biogeme = bio.BIOGEME(database, simulate)
biogeme_simulation = biogeme.simulate(results.getBetaValues())
print(
    f'--- Execution time with Biogeme:    '
    f'{time.time() - start_time:.2f} seconds ---'
)

# Let's print the two results, to show that they are identical

print('Results without Biogeme')
print(simulated_values)
print('Results with Biogeme')
print(biogeme_simulation)
\end{lstlisting}

\subsection{\lstinline$04market_shares.py$}\label{sec:04market_shares}
\begin{lstlisting}[style=numbers]
import sys
from biogeme import models
import biogeme.biogeme as bio
import biogeme.exceptions as excep
import biogeme.results as res
from scenarios import scenario, database, normalizedWeight

# Obtain the specification for the default scenario
V, nests, _, _ = scenario()

# Obtain the expression for the choice probability of each alternative
prob_PT = models.nested(V, None, nests, 0)
prob_CAR = models.nested(V, None, nests, 1)
prob_SM = models.nested(V, None, nests, 2)

# Read the estimation results from the file
try:
    results = res.bioResults(pickleFile='02estimation.pickle')
except excep.biogemeError:
    sys.exit(
        'Run first the script 02simulation.py '
        'in order to generate the '
        'file 02estimation.pickle.'
    )

# We now simulate the choice probabilities and the weight

simulate = {
    'weight': normalizedWeight,
    'Prob. PT': prob_PT,
    'Prob. car': prob_CAR,
    'Prob. SM': prob_SM,
}

biogeme = bio.BIOGEME(database, simulate)
simulated_values = biogeme.simulate(results.getBetaValues())

# We also calculate confidence intervals for the calculated quantities

betas = biogeme.freeBetaNames
b = results.getBetasForSensitivityAnalysis(betas)
left, right = biogeme.confidenceIntervals(b, 0.9)


# Market shares are calculated using the weighted mean of the
# individual probabilities

# Alternative car
simulated_values['Weighted prob. car'] = (
    simulated_values['weight'] * simulated_values['Prob. car']
)
left['Weighted prob. car'] = left['weight'] * left['Prob. car']
right['Weighted prob. car'] = (
    right['weight'] * right['Prob. car']
)

marketShare_car = simulated_values['Weighted prob. car'].mean()
marketShare_car_left = left['Weighted prob. car'].mean()
marketShare_car_right = right['Weighted prob. car'].mean()

# Alternative public transportation
simulated_values['Weighted prob. PT'] = (
    simulated_values['weight'] * simulated_values['Prob. PT']
)
left['Weighted prob. PT'] = left['weight'] * left['Prob. PT']
right['Weighted prob. PT'] = right['weight'] * right['Prob. PT']

marketShare_PT = simulated_values['Weighted prob. PT'].mean()
marketShare_PT_left = left['Weighted prob. PT'].mean()
marketShare_PT_right = right['Weighted prob. PT'].mean()

# Alternative slow modes
simulated_values['Weighted prob. SM'] = (
    simulated_values['weight'] * simulated_values['Prob. SM']
)
left['Weighted prob. SM'] = left['weight'] * left['Prob. SM']
right['Weighted prob. SM'] = right['weight'] * right['Prob. SM']

marketShare_SM = simulated_values['Weighted prob. SM'].mean()
marketShare_SM_left = left['Weighted prob. SM'].mean()
marketShare_SM_right = right['Weighted prob. SM'].mean()

# Reporting
print(
    f'Market share for car: {100*marketShare_car:.1f}% '
    f'[{100*marketShare_car_left:.1f}%, '
    f'{100*marketShare_car_right:.1f}%]'
)
print(
    f'Market share for PT:  {100*marketShare_PT:.1f}% '
    f'[{100*marketShare_PT_left:.1f}%, '
    f'{100*marketShare_PT_right:.1f}%]'
)
print(
    f'Market share for SM:   {100*marketShare_SM:.1f}% '
    f'[{100*marketShare_SM_left:.1f}%, '
    f'{100*marketShare_SM_right:.1f}%]'
)
\end{lstlisting}

\subsection{\lstinline$05revenues.py$}\label{sec:05revenues}
\begin{lstlisting}[style=numbers]
import sys
import numpy as np
from tqdm import tqdm
from biogeme import models
import biogeme.exceptions as excep
import matplotlib.pyplot as plt
import biogeme.biogeme as bio
import biogeme.results as res
from scenarios import scenario, database, normalizedWeight

# Read the estimation results from the file
try:
    results = res.bioResults(pickleFile='02estimation.pickle')
except excep.biogemeError:
    sys.exit(
        'Run first the script 02simulation.py '
        'in order to generate the '
        'file 02estimation.pickle.'
    )


def revenues(factor):
    """Calculate the total revenues generated by public transportation,
        when the price is multiplied by a factor.

    :param factor: factor that multiplies the current cost of public
        transportation
    :type factor: float

    :return: total revenues, followed by the lower and upper bound of
        the confidence interval.
    :rtype: tuple(float, float, float)

    """
    # Obtain the specification for the default scenario
    V, nests, _, MarginalCostScenario = scenario(factor=factor)

    # Obtain the expression for the choice probability of each alternative
    prob_PT = models.nested(V, None, nests, 0)

    # We now simulate the choice probabilities,the weight and the
    # price variable

    simulate = {
        'weight': normalizedWeight,
        'Revenue public transportation': prob_PT
        * MarginalCostScenario,
    }

    biogeme = bio.BIOGEME(database, simulate)
    simulated_values = biogeme.simulate(results.getBetaValues())

    # We also calculate confidence intervals for the calculated quantities

    betas = biogeme.freeBetaNames
    b = results.getBetasForSensitivityAnalysis(betas)
    left, right = biogeme.confidenceIntervals(b, 0.9)

    revenues_pt = (
        simulated_values['Revenue public transportation']
        * simulated_values['weight']
    ).sum()
    revenues_pt_left = (
        left['Revenue public transportation'] * left['weight']
    ).sum()
    revenues_pt_right = (
        right['Revenue public transportation'] * right['weight']
    ).sum()
    return revenues_pt, revenues_pt_left, revenues_pt_right


# Current revenues for public transportation

r, r_left, r_right = revenues(factor=1.0)
print(
    f'Total revenues for public transportation (for the sample): {r:.1f} CHF '
    f'[{r_left:.1f} CHF, '
    f'{r_right:.1f} CHF]'
)

# We now investigate how the revenues vary with the multiplicative factor

factors = np.arange(0.0, 5.0, 0.05)

plot_revenues = [revenues(s) for s in tqdm(factors)]
zipped = zip(*plot_revenues)
rev = next(zipped)
lower = next(zipped)
upper = next(zipped)

largest_revenue = max(rev)
max_index = rev.index(largest_revenue)
print(
    f'Largest revenue: {largest_revenue:.1f} obtained with '
    f'factor {factors[max_index]:.1f}'
)

# We plot the results
ax = plt.gca()
ax.plot(factors, rev, label="Revenues")
ax.plot(factors, lower, label="Lower bound of the CI")
ax.plot(factors, upper, label="Upper bound of the CI")
ax.legend()

plt.show()
\end{lstlisting}

\subsection{\lstinline$06point_elasticities.py$}\label{sec:06point_elasticities}
\begin{lstlisting}[style=numbers]
import sys
import biogeme.biogeme as bio
from biogeme import models
import biogeme.results as res
import biogeme.exceptions as excep
from biogeme.expressions import Derive
from scenarios import (
    scenario,
    database,
    normalizedWeight,
    TimePT,
    TimeCar,
    MarginalCostPT,
    CostCarCHF,
    distance_km,
)


# Obtain the specification for the default scenario
V, nests, _, _ = scenario()

# Obtain the expression for the choice probability of each alternative
prob_PT = models.nested(V, None, nests, 0)
prob_CAR = models.nested(V, None, nests, 1)
prob_SM = models.nested(V, None, nests, 2)

# Calculation of the direct elasticities.
# We use the 'Derive' operator to calculate the derivatives.

direct_elas_pt_time = (
    Derive(prob_PT, 'TimePT') * TimePT / prob_PT
)

direct_elas_pt_cost = (
    Derive(prob_PT, 'MarginalCostPT') * MarginalCostPT / prob_PT
)

direct_elas_car_time = (
    Derive(prob_CAR, 'TimeCar') * TimeCar / prob_CAR
)

direct_elas_car_cost = (
    Derive(prob_CAR, 'CostCarCHF') * CostCarCHF / prob_CAR
)

direct_elas_sm_dist = (
    Derive(prob_SM, 'distance_km') * distance_km / prob_SM
)

# Simulate the formulas
simulate = {
    'weight': normalizedWeight,
    'Prob. car': prob_CAR,
    'Prob. public transportation': prob_PT,
    'Prob. slow modes': prob_SM,
    'direct_elas_pt_time': direct_elas_pt_time,
    'direct_elas_pt_cost': direct_elas_pt_cost,
    'direct_elas_car_time': direct_elas_car_time,
    'direct_elas_car_cost': direct_elas_car_cost,
    'direct_elas_sm_dist': direct_elas_sm_dist,
}

biogeme = bio.BIOGEME(database, simulate)

# Read the estimation results from the file
try:
    results = res.bioResults(pickleFile='02estimation.pickle')
except excep.biogemeError:
    sys.exit(
        'Run first the script 02estimation.py in order to generate '
        'the file 02estimation.pickle.'
    )

# simulatedValues is a Panda dataframe with the same number of rows as
# the database, and as many columns as formulas to simulate.
simulated_values = biogeme.simulate(results.getBetaValues())

# We calculate the aggregate elasticities

# First, the weighted probabilities
simulated_values['Weighted prob. car'] = (
    simulated_values['weight'] * simulated_values['Prob. car']
)
simulated_values['Weighted prob. PT'] = (
    simulated_values['weight']
    * simulated_values['Prob. public transportation']
)
simulated_values['Weighted prob. SM'] = (
    simulated_values['weight']
    * simulated_values['Prob. slow modes']
)

# Then the denominator of the aggregate elasticity expression.
denominator_car = simulated_values['Weighted prob. car'].sum()
denominator_pt = simulated_values['Weighted prob. PT'].sum()
denominator_sm = simulated_values['Weighted prob. SM'].sum()

# And finally the aggregate elasticities themselves.
direct_elas_term_car_time = (
    simulated_values['Weighted prob. car']
    * simulated_values['direct_elas_car_time']
    / denominator_car
).sum()
print(
    f'Aggregate direct point elasticity of car wrt time: '
    f'{direct_elas_term_car_time:.3g}'
)

direct_elas_term_car_cost = (
    simulated_values['Weighted prob. car']
    * simulated_values['direct_elas_car_cost']
    / denominator_car
).sum()
print(
    f'Aggregate direct point elasticity of car wrt cost: '
    f'{direct_elas_term_car_cost:.3g}'
)

direct_elas_term_pt_time = (
    simulated_values['Weighted prob. PT']
    * simulated_values['direct_elas_pt_time']
    / denominator_pt
).sum()
print(
    f'Aggregate direct point elasticity of PT wrt time: '
    f'{direct_elas_term_pt_time:.3g}'
)

direct_elas_term_pt_cost = (
    simulated_values['Weighted prob. PT']
    * simulated_values['direct_elas_pt_cost']
    / denominator_pt
).sum()
print(
    f'Aggregate direct point elasticity of PT wrt cost: '
    f'{direct_elas_term_pt_cost:.3g}'
)

direct_elas_term_sm_dist = (
    simulated_values['Weighted prob. SM']
    * simulated_values['direct_elas_sm_dist']
    / denominator_sm
).sum()
print(
    f'Aggregate direct point elasticity of SM wrt distance: '
    f'{direct_elas_term_sm_dist:.3g}'
)
\end{lstlisting}

\subsection{\lstinline$07cross_elasticities.py$}\label{sec:07cross_elasticities}
\begin{lstlisting}[style=numbers]
import sys
import biogeme.biogeme as bio
from biogeme import models
import biogeme.exceptions as excep
import biogeme.results as res
from biogeme.expressions import Derive
from scenarios import (
    scenario,
    database,
    normalizedWeight,
    TimePT,
    TimeCar,
    MarginalCostPT,
    CostCarCHF,
)


# Obtain the specification for the default scenario
V, nests, _, _ = scenario()

# Obtain the expression for the choice probability of each alternative
prob_PT = models.nested(V, None, nests, 0)
prob_CAR = models.nested(V, None, nests, 1)
prob_SM = models.nested(V, None, nests, 2)

# The choice model is a nested logit
prob_PT = models.nested(V, None, nests, 0)
prob_CAR = models.nested(V, None, nests, 1)
prob_SM = models.nested(V, None, nests, 2)

# Calculation of the cross elasticities.
# We use the 'Derive' operator to calculate the derivatives.
cross_elas_pt_time = (
    Derive(prob_PT, 'TimeCar') * TimeCar / prob_PT
)
cross_elas_pt_cost = (
    Derive(prob_PT, 'CostCarCHF') * CostCarCHF / prob_PT
)
cross_elas_car_time = (
    Derive(prob_CAR, 'TimePT') * TimePT / prob_CAR
)
cross_elas_car_cost = (
    Derive(prob_CAR, 'MarginalCostPT')
    * MarginalCostPT
    / prob_CAR
)

simulate = {
    'weight': normalizedWeight,
    'Prob. car': prob_CAR,
    'Prob. public transportation': prob_PT,
    'Prob. slow modes': prob_SM,
    'cross_elas_pt_time': cross_elas_pt_time,
    'cross_elas_pt_cost': cross_elas_pt_cost,
    'cross_elas_car_time': cross_elas_car_time,
    'cross_elas_car_cost': cross_elas_car_cost,
}

biogeme = bio.BIOGEME(database, simulate)

# Read the estimation results from the file
try:
    results = res.bioResults(pickleFile='02estimation.pickle')
except excep.biogemeError:
    sys.exit(
        'Run first the script 02estimation.py in order to generate '
        'the file 02estimation.pickle.'
    )

# simulated_values is a Panda dataframe with the same number of rows as
# the database, and as many columns as formulas to simulate.
simulated_values = biogeme.simulate(results.getBetaValues())

# We calculate the elasticities
simulated_values['Weighted prob. car'] = (
    simulated_values['weight'] * simulated_values['Prob. car']
)
simulated_values['Weighted prob. PT'] = (
    simulated_values['weight']
    * simulated_values['Prob. public transportation']
)

denominator_car = simulated_values['Weighted prob. car'].sum()
denominator_pt = simulated_values['Weighted prob. PT'].sum()

cross_elas_term_car_time = (
    simulated_values['Weighted prob. car']
    * simulated_values['cross_elas_car_time']
    / denominator_car
).sum()
print(
    f'Aggregate cross elasticity of car wrt time: '
    f'{cross_elas_term_car_time:.3g}'
)

cross_elas_term_car_cost = (
    simulated_values['Weighted prob. car']
    * simulated_values['cross_elas_car_cost']
    / denominator_car
).sum()
print(
    f'Aggregate cross elasticity of car wrt cost: '
    f'{cross_elas_term_car_cost:.3g}'
)

cross_elas_term_pt_time = (
    simulated_values['Weighted prob. PT']
    * simulated_values['cross_elas_pt_time']
    / denominator_pt
).sum()
print(
    f'Aggregate cross elasticity of PT wrt car time: '
    f'{cross_elas_term_pt_time:.3g}'
)

cross_elas_term_pt_cost = (
    simulated_values['Weighted prob. PT']
    * simulated_values['cross_elas_pt_cost']
    / denominator_pt
).sum()
print(
    f'Aggregate cross direct elasticity of PT wrt car cost: '
    f'{cross_elas_term_pt_cost:.3g}'
)

\end{lstlisting}

\subsection{\lstinline$08arc_elasticities.py$}\label{sec:08arc_elasticities}
\begin{lstlisting}[style=numbers]
import sys
import biogeme.biogeme as bio
import biogeme.exceptions as excep
from biogeme import models
import biogeme.results as res
from scenarios import (
    scenario,
    database,
    normalizedWeight,
)

# Obtain the specification for the default scenario
V, nests, _, MarginalCostPT = scenario()

# Obtain the expression for the choice probability of each alternative
prob_PT = models.nested(V, None, nests, 0)

# We investigate a scenario where the price for public transportation
# increases by 20%
V_after, _, _, MarginalCostPT_after = scenario(factor=1.2)
prob_PT_after = models.nested(V_after, None, nests, 0)


# Disaggregate elasticities
direct_elas_pt = (
    (prob_PT_after - prob_PT)
    * MarginalCostPT
    / (prob_PT * (MarginalCostPT_after - MarginalCostPT))
)

simulate = {
    'weight': normalizedWeight,
    'Prob. PT': prob_PT,
    'direct_elas_pt': direct_elas_pt,
}

biogeme = bio.BIOGEME(database, simulate)

# Read the estimation results from the file
try:
    results = res.bioResults(pickleFile='02estimation.pickle')
except excep.biogemeError:
    sys.exit(
        'Run first the script 02estimation.py in order to generate '
        'the file 02estimation.pickle.'
    )

# simulated_values is a Panda dataframe with the same number of rows as
# the database, and as many columns as formulas to simulate.
simulated_values = biogeme.simulate(results.getBetaValues())

# We calculate the elasticities
simulated_values['Weighted prob. PT'] = (
    simulated_values['weight'] * simulated_values['Prob. PT']
)

denominator_pt = simulated_values['Weighted prob. PT'].sum()

direct_elas_pt = (
    simulated_values['Weighted prob. PT']
    * simulated_values['direct_elas_pt']
    / denominator_pt
).sum()

print(
    f'Aggregate direct arc elasticity of public transportation wrt cost: '
    f'{direct_elas_pt:.3g}'
)
\end{lstlisting}

\subsection{\lstinline$09wtp.py$}\label{sec:09wtp}
\begin{lstlisting}[style=numbers]
import sys
import matplotlib.pyplot as plt
import biogeme.biogeme as bio
import biogeme.exceptions as excep
import biogeme.results as res

from biogeme.expressions import Derive
from scenarios import (
    scenario,
    database,
    normalizedWeight,
)

V, _, _, _ = scenario()

V_PT = V[0]
V_CAR = V[1]

WTP_PT_TIME = Derive(V_PT, 'TimePT') / Derive(
    V_PT, 'MarginalCostPT'
)
WTP_CAR_TIME = Derive(V_CAR, 'TimeCar') / Derive(
    V_CAR, 'CostCarCHF'
)

simulate = {
    'weight': normalizedWeight,
    'WTP PT time': WTP_PT_TIME,
    'WTP CAR time': WTP_CAR_TIME,
}

biogeme = bio.BIOGEME(database, simulate)

# Read the estimation results from the file.
try:
    results = res.bioResults(pickleFile='02estimation.pickle')
except excep.biogemeError:
    sys.exit(
        'Run first the script 02estimation.py in order to generate '
        'the file 02estimation.pickle.'
    )

# simulated_values is a Panda dataframe with the same number of rows as
# the database, and as many columns as formulas to simulate.
simulated_values = biogeme.simulate(results.getBetaValues())

wtpcar = (
    60
    * simulated_values['WTP CAR time']
    * simulated_values['weight']
).mean()

# Calculate confidence intervals
b = results.getBetasForSensitivityAnalysis(biogeme.freeBetaNames)

# Returns data frame containing, for each simulated value, the left
# and right bounds of the confidence interval calculated by simulation.
left, right = biogeme.confidenceIntervals(b, 0.9)

wtpcar_left = (60 * left['WTP CAR time'] * left['weight']).mean()
wtpcar_right = (
    60 * right['WTP CAR time'] * right['weight']
).mean()
print(
    f'Average WTP for car: {wtpcar:.3g} '
    f'CI:[{wtpcar_left:.3g}, {wtpcar_right:.3g}]'
)


# In this specific case, there are only two distinct values in the
# population: for workers and non workers
print(
    'Unique values:      ',
    [
        f'{i:.3g}'
        for i in 60 * simulated_values['WTP CAR time'].unique()
    ],
)


def wtpForSubgroup(theFilter):
    """
    Check the value for groups of the population. Define a function that
    works for any filter to avoid repeating code.

    :param theFilter: pandas filter
    :type theFilter: numpy.Series(bool)

    :return: willingness-to-pay for car and confidence interval
    :rtype: tuple(float, float, float)
    """
    size = theFilter.sum()
    sim = simulated_values[theFilter]
    totalWeight = sim['weight'].sum()
    weight = sim['weight'] * size / totalWeight
    _wtpcar = (60 * sim['WTP CAR time'] * weight).mean()
    _wtpcar_left = (
        60 * left[theFilter]['WTP CAR time'] * weight
    ).mean()
    _wtpcar_right = (
        60 * right[theFilter]['WTP CAR time'] * weight
    ).mean()
    return _wtpcar, _wtpcar_left, _wtpcar_right


# full time workers.
aFilter = database.data['OccupStat'] == 1
w, l, r = wtpForSubgroup(aFilter)
print(f'WTP car for workers: {w:.3g} CI:[{l:.3g}, {r:.3g}]')

# females
aFilter = database.data['Gender'] == 2
w, l, r = wtpForSubgroup(aFilter)
print(f'WTP car for females: {w:.3g} CI:[{l:.3g}, {r:.3g}]')

# males
aFilter = database.data['Gender'] == 1
w, l, r = wtpForSubgroup(aFilter)
print(f'WTP car for males  : {w:.3g} CI:[{l:.3g}, {r:.3g}]')

# We plot the distribution of WTP in the population. In this case,
# there are only two values

plt.hist(
    60 * simulated_values['WTP CAR time'],
    weights=simulated_values['weight'],
)
plt.xlabel('WTP (CHF/hour)')
plt.ylabel('Individuals')
plt.show()
\end{lstlisting}



\clearpage

\bibliographystyle{dcu}
\bibliography{../dca}

\end{document}
