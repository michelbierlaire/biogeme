{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import biogeme.vns as vns\n",
    "import biogeme.database as db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pareto reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to display the information contained in the Pareto set generated by the algorithm, and saved in the pickle file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the following just to know the sample size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Choice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m database \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mDatabase(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mswissmetro\u001b[39m\u001b[38;5;124m'\u001b[39m, df)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mglobals\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(database\u001b[38;5;241m.\u001b[39mvariables)\n\u001b[0;32m----> 7\u001b[0m exclude \u001b[38;5;241m=\u001b[39m ((\u001b[43mChoice\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m (CostCarCHF \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m database\u001b[38;5;241m.\u001b[39mremove(exclude)\n\u001b[1;32m     10\u001b[0m sampleSize \u001b[38;5;241m=\u001b[39m database\u001b[38;5;241m.\u001b[39mgetSampleSize()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Choice' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('swissmetro.dat', sep='\\t')\n",
    "\n",
    "database = db.Database('swissmetro', df)\n",
    "\n",
    "globals().update(database.variables)\n",
    "\n",
    "exclude = ((Choice == -1) + (CostCarCHF < 0)) > 0\n",
    "database.remove(exclude)\n",
    "\n",
    "sampleSize = database.getSampleSize()\n",
    "sampleSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information about the approximation of the Pareto set is saved regularly by the algorithm in a pickle file. We first open it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickleFile = 'swissmetroPareto.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument is the largest size of neighborhood used by the algorithm. It is irrelevant when the algorithm is not executed. Here, we simply display the Pareto solutions. So it is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto = vns.paretoClass(_, archiveInputFile=pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of pareto solutions: {len(pareto.pareto)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the AIC and the BIC for each non dominated  model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AIC(k, LL):\n",
    "    \"\"\"Calculate the Aikaike Information Criterion for a model with k parameters and final log liklelihood LL\"\"\"\n",
    "    return 2 * k - 2 * LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIC(k, LL):\n",
    "    \"\"\"Calculate the Bayesian Information Criterion for a model with k parameters and final log liklelihood LL\"\"\"\n",
    "    return k * np.log(sampleSize) - 2 * LL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Pareto solutions: performance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model in the Pareto set, we display:\n",
    "\n",
    "- an id,\n",
    "- the negative log likelihood,\n",
    "- the number of parameters,\n",
    "- the Bayesian information criterion (BIC), \n",
    "- the Akaike informatoin criterion (AIC).\n",
    "\n",
    "The models with the best BIC (B*) and the best AIC (A*) are identified. If the same model has both the best BIC and AIC, it is labeled (**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestBIC = None\n",
    "bestAIC = None\n",
    "summary = []\n",
    "besti = -1\n",
    "bestj = -1\n",
    "for p in pareto.pareto:\n",
    "    res = ''\n",
    "    for t, r in zip(p.objectivesNames, p.objectives):\n",
    "        res += f'{t}: {r}\\t'\n",
    "    L = p.objectives[0]\n",
    "    k = p.objectives[1]\n",
    "    A = AIC(k, -L)\n",
    "    B = BIC(k, -L)\n",
    "    res += f'BIC = {B:.3f}\\tAIC = {A:.3f}'\n",
    "    summary.append(res)\n",
    "    if bestBIC is None or B < bestBIC:\n",
    "        bestBIC = B\n",
    "        besti = len(summary) - 1\n",
    "    if bestAIC is None or A < bestAIC:\n",
    "        bestAIC = A\n",
    "        bestj = len(summary) - 1\n",
    "for i in range(len(summary)):\n",
    "    if i == besti:\n",
    "        if i == bestj:\n",
    "            pre = f'{i+1:3} **'\n",
    "        else:\n",
    "            pre = f'{i+1:3} B*'\n",
    "    elif i == bestj:\n",
    "        pre = f'{i+1:3} A*'\n",
    "    else:\n",
    "        pre = f'{i+1:3}   '\n",
    "    print(f'{pre} {summary[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Pareto solutions: model specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model in the Pareto set, we provide a description of the model specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0 \n",
    "for p in pareto.pareto:\n",
    "    counter += 1\n",
    "    print(f'*************** Model {counter} ************************')\n",
    "    print(p)\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Pareto solutions: illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below illustrates all models considered by the algorithm. Each model corresponds to one point in the graph. \n",
    "\n",
    "- The x-coordinate corresponds to the negative log likelihood of the model, and the y-coordinate to the number of parameters. \n",
    "- The larger circles correspond to all models that are not dominated. They are in the Pareto set.\n",
    "- The crosses corresponds to model that happened to be non dominated at some point during the course of the algorithm, but have been removed from the Pareto set afterwards, as a dominating model has been identified.\n",
    "- Finally, the small dots corresponds to models that have been considered, but rejected because dominated by another model already in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectives = list(pareto.pareto)[0].objectivesNames\n",
    "objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_obj = [p.objectives for p in pareto.pareto]\n",
    "par_x, par_y = zip(*par_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_obj = [p.objectives for p in pareto.considered]\n",
    "con_x, con_y = zip(*con_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_obj = [p.objectives for p in pareto.removed]\n",
    "rem_x, rem_y = zip(*rem_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_buffer = 10\n",
    "y_buffer = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis([min(par_x)-x_buffer,\n",
    "          max(par_x)+x_buffer,\n",
    "          min(par_y)-y_buffer,\n",
    "          max(par_y)+y_buffer])\n",
    "plt.plot(par_x, par_y, 'o', label='Pareto')\n",
    "plt.plot(rem_x, rem_y, 'x', label='Removed')\n",
    "plt.plot(con_x, con_y, ',', label='Considered')\n",
    "plt.xlabel(objectives[0])\n",
    "plt.ylabel(objectives[1])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
